{
  "hash": "d7c5bc5a77b70a94b8d4960c6f019101",
  "result": {
    "markdown": "---\ntitle: \"Advanced Linux and CTL\"\nabstract: \"awk, grep, cut, etc.\"\n---\n\n\n\n\n# Q1\n\nI have many files inside a folder. I want to move them into to sub-folders named `f1` and `f2` (I made them using `mkdir -p f1 f2`). How to do that?\\\nThis is the way:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\ni=0\nfor file in $(ls *.fastq.gz | sort -V); do\n  if [ $i -le 400 ]; then\n    mv \"$file\" f1/\n  else\n    mv \"$file\" f2/\n  fi\n  i=$((i + 1))\ndone\n```\n:::\n\n\nHere, I am sending files having 0-400 in there name to `f1` folder and remaining ones to `f2`. You just need to use your file naming pattern, and you are all set.\n\n**How to execute/run this file now?**\\\nRun these:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nchmod +x move_files.sh\n./move_files.sh\n```\n:::\n\n\n`chmod +x` is making the file executable. Then we are running it using `./move_files.sh`.\n\n\n\n\n## HPC Cluster connection\nI am going to connect to Uni-Greifswald's Brain cluster. \n\n::: {.cell}\n\n```{.bash .cell-code}\nssh username@brain.uni-greifswald.de\n```\n:::\n\nYou need to give your real username and password. \nNow, let's get an interactive session to the gpu compute node (it is named \"vision\" for uni-greifswald's gpu node). \n\n::: {.cell}\n\n```{.bash .cell-code}\nsrun --pty --gres=gpu:1 --partition=vision --mem=16g -t 12:00:00 bash -i\n```\n:::\n\nLet's install conda for our environment management. \n\n::: {.cell}\n\n```{.bash .cell-code}\n# Download the Miniconda installer for Linux\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n\n# Run the installer, specifying the installation path\nbash Miniconda3-latest-Linux-x86_64.sh -b -p ~/miniconda_install/\n```\n:::\n\nNow, let's initialize conda:\n\n::: {.cell}\n\n```{.bash .cell-code}\n# Source the conda shell script to make the 'conda' command available\nsource ~/miniconda_install/bin/activate\n\n# Initialize conda for the current shell session.\nconda init bash\n```\n:::\n\nSince we're using an interactive session, we won't need to manually source anything after this. The `conda init` command makes it so we can use `conda` and `conda activate` as we normally would.\n\n**N.B. We could make some aliases for conda commands to write conda codes in shorter format, but we can do/see it later.**\n\nLet's make an environment and install our required tools there. Well, our goal is to make a system where we will use some images/characters and make short videos using them to teach Italian. We need to process photos, images, text, and sync lips (video) with audio/speech. We need MuseTalk for this. Let's configure our environment accordingly.  \n\n**Step 1 -- Create conda environment**\n\n::: {.cell}\n\n```{.bash .cell-code}\n# Create new environment for MuseTalk\nconda create -n musetalk python=3.10\nconda activate musetalk\n```\n:::\n\n**Step 2 -- Install Dependencies**\n\n::: {.cell}\n\n```{.bash .cell-code}\n# Create new environment for MuseTalk\n# Install PyTorch (adapt cuda version to your cluster, here CUDA 11.8 example)\npip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --extra-index-url https://download.pytorch.org/whl/cu118\n\n# HuggingFace core libs\npip install transformers accelerate diffusers safetensors\n\n# MuseTalk repo (clone)\ngit clone https://github.com/TMElyralab/MuseTalk.git\ncd MuseTalk\n\n# Install requirements\npip install -r requirements.txt\n\n# Install whisper encoder\npip install --editable ./musetalk/whisper\n\n# Extra: ffmpeg for video processing\nconda install -c conda-forge ffmpeg -y\n```\n:::\n\n**Step 3 -- Download MuseTalk Models**\nMuseTalk Hugging Face repo: https://huggingface.co/TMElyralab/MuseTalk  \nWe need:  \n- musetalk.pth (main model)\n- gfpgan (optional face enhancer)\n\nRun inside MuseTalk:\n\n::: {.cell}\n\n```{.bash .cell-code}\nmkdir checkpoints\ncd checkpoints\n\n# Download core model\nwget https://huggingface.co/TMElyralab/MuseTalk/resolve/main/musetalk.pth\n\n# Optional face enhancer\ngit clone https://github.com/TencentARC/GFPGAN.git\ncd ..\n\n```\n:::\n\n\n## Better way:\n\n::: {.cell}\n\n```{.bash .cell-code}\nexport CUDA_HOME=/usr/local/cuda-11.7\nexport PATH=$CUDA_HOME/bin:$PATH\nexport LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH\n```\n:::\n\nIn my HPC Cluster, it is cuda 11.7. I am configuring for that. We could add these lines in my ~/.bashrc file as well. We might see it later.  \n\nLet's load our cuda module first to get things done smoothly.\n\n::: {.cell}\n\n```{.bash .cell-code}\nmodule load cuda/12.0.0\n```\n:::\n\nNow, make the `yml` file to make the environment with all the tools required.\n\n::: {.cell}\n\n```{.bash .cell-code}\nname: musetalk3\nchannels:\n  - pytorch\n  - nvidia\n  - defaults\ndependencies:\n  - python=3.10\n  - pip\n  - ffmpeg\n  - pip:\n      # PyTorch + CUDA 11.8 compatible with 11.7 system\n      - torch==2.1.0+cu118\n      - torchvision==0.16.0+cu118\n      - torchaudio==2.1.0+cu118\n      - --extra-index-url https://download.pytorch.org/whl/cu118\n\n      # OpenMMLab dependencies\n      - mmcv==2.0.1 -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.1.0/index.html\n      - mmdet==3.1.0\n      - gradio\n      - opencv-python\n      - numpy\n      - scipy\n      - matplotlib\n      - tqdm\n      - pyyaml\n      - pillow\n      - soundfile\n      - librosa\n      - moviepy\n      - imageio\n```\n:::\n\nSave this as `musetalk3.yml` and run:\n\n::: {.cell}\n\n```{.bash .cell-code}\nconda env create -f musetalk3.yml\nconda activate musetalk3\n```\n:::\n\nNow, we have our environment ready to use. Let's use it.\n\n::: {.cell}\n\n```{.bash .cell-code}\nconda activate musetalk3\n```\n:::\n\nNow, install museetalk repo dependencies. \n\n::: {.cell}\n\n```{.bash .cell-code}\ngit clone https://github.com/TMElyralab/MuseTalk.git\ncd MuseTalk\npip install -r requirements.txt\n```\n:::\n\nLet's get the faces I want to use. I uploaded them to my GDrive.\n\n::: {.cell}\n\n```{.bash .cell-code}\npip install gdown\n\n# Get shareable link from Google Drive and copy file id\ngdown https://drive.google.com/uc?id=FILE_ID -O data/faces/alice.jpg\ngdown https://drive.google.com/uc?id=FILE_ID -O data/faces/bob.jpg\n```\n:::\n\nmodify the google drive links for the images and their destination name as you like it.\n\n# Real work\nNow, let's follow the author's guideline.\n\n::: {.cell}\n\n```{.bash .cell-code}\npip install --no-cache-dir -U openmim\nmim install mmengine\nmim install \"mmcv==2.0.1\"\nmim install \"mmdet==3.1.0\"\nmim install \"mmpose==1.1.0\"\n```\n:::\n\nLet's download the model's weight.\n\n::: {.cell}\n\n```{.bash .cell-code}\nsh ./download_weights.sh\n```\n:::\n\n::: {.cell}\n\n```{.bash .cell-code}\n# Check ffmpeg installation\nffmpeg -version\n```\n:::\n\nThe conversation\n\n::: {.cell}\n\n```{.bash .cell-code}\npip install TTS\n\n# Example: Alice speaking Italian\ntts --text \"Buon giorno, Bob! Come stai?\" \\\n    --model_name tts_models/it/mai_female/vits \\\n    --out_path alice_buongiorno.wav\n\n# Example: Bob replying\ntts --text \"Buon giorno, Alice! Va bene, grazie mille! Chi Ã¨ questo?\" \\\n    --model_name tts_models/it/mai_male/vits \\\n    --out_path bob_risposta.wav\n\n```\n:::\n",
    "supporting": [
      "advanced_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}