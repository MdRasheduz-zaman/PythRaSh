[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PythRaSh",
    "section": "",
    "text": "PythRaSh starts out as a step-by-step introduction to R, Python, Linux and Command-line Tools, Probability and Statistics, etc. while its later parts are more of a cookbook with analyses of typical examples in life sciences with focus on (Molecular) Biology, Breeding and Genetics, (Gen)-omics, Medical Sciences, Agriculture, Ecology and other related fields."
  },
  {
    "objectID": "index.html#workshops",
    "href": "index.html#workshops",
    "title": "PythRaSh",
    "section": "Workshops",
    "text": "Workshops\nMoreover, the chapters published here serve as the basis for my YouTube teaching. Also, these are the primers for my upcoming workshops. Email me at md.rasheduzzaman.ugoe[at]gmail.com if you are interested in one of my upcoming workshops. Provide me with your details, what you want to learn, motivation behind that, what you are expert in, etc.\nData Analysis with R course is done. I taught basic R, git and GitHub. These are the first step of taking you towards your (Gen)-omics journey. The next part, Statistics, modeling, -omics, RNAseq, and command-line bioinformatics will resume in October.\nConsider emailing if you would like to take Python course with me."
  },
  {
    "objectID": "ch/prob_stat/stat.html",
    "href": "ch/prob_stat/stat.html",
    "title": "Statistics Basics",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.width = 10, fig.height = 6)\nlibrary(tidyverse)\nlibrary(car)\nlibrary(emmeans)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(patchwork)\nlibrary(corrplot)\n\n# Helper function for separators\nsep_line &lt;- function(char = \"=\", n = 50) {\n  cat(paste(rep(char, n), collapse = \"\"), \"\\n\")\n}"
  },
  {
    "objectID": "ch/linux-and-ctl/advanced.html",
    "href": "ch/linux-and-ctl/advanced.html",
    "title": "Advanced Linux and Command Line Tools",
    "section": "",
    "text": "Learn a lot from Harvard Chan Bioinformatics Core"
  },
  {
    "objectID": "ch/rbasics/solutions.html",
    "href": "ch/rbasics/solutions.html",
    "title": "HW solutions",
    "section": "",
    "text": "We measured the concentration (in µg/µL) of three proteins (P1, P2, P3) in four samples (S1–S4):\n\n\n# Making Protein Matrix\nProteinMatrix &lt;- matrix(\n  c(5, 3, 2,\n    7, 6, 4),\n  nrow = 2, byrow = TRUE\n)\nrownames(ProteinMatrix) = c(\"Sample1\", \"Sample2\")\ncolnames(ProteinMatrix) = c(\"ProteinX\", \"ProteinY\", \"ProteinZ\")\nProteinMatrix\n\n        ProteinX ProteinY ProteinZ\nSample1        5        3        2\nSample2        7        6        4\n\n\nNow goes the weight matrix\n\n# Making weight matrix\nWeightVector &lt;- matrix(\n  c(0.5, 1.0, 1.5),\n  nrow=3, byrow = TRUE\n)\nrownames(WeightVector) = c(\"ProteinX\", \"ProteinY\", \"ProteinZ\")\ncolnames(WeightVector) = c(\"Weight\")\nWeightVector\n\n         Weight\nProteinX    0.5\nProteinY    1.0\nProteinZ    1.5\n\n\nNow, multiply them.\n\n# Multiplying Matrices\nTotalConc = ProteinMatrix %*% WeightVector\ncolnames(TotalConc) &lt;- \"Total_Protein_Conc\"\nprint(TotalConc)\n\n        Total_Protein_Conc\nSample1                8.5\nSample2               15.5\n\n\n\n\nProteinMatTranspose = t(ProteinMatrix)\nProteinMatTranspose\n\n         Sample1 Sample2\nProteinX       5       7\nProteinY       3       6\nProteinZ       2       4\n\n\n\n\nI &lt;- diag(3)\nIdentitycheck = ProteinMatrix %*% I\ncolnames(Identitycheck) &lt;- c(\"ProteinX\", \"ProteinY\", \"ProteinZ\")\nIdentitycheck\n\n        ProteinX ProteinY ProteinZ\nSample1        5        3        2\nSample2        7        6        4\n\n\n\n\nrowSums(ProteinMatrix)\n\nSample1 Sample2 \n     10      17 \n\n\n\n\ncolSums(ProteinMatrix)\n\nProteinX ProteinY ProteinZ \n      12        9        6 \n\n\n\n\nheatmap(ProteinMatrix, scale = \"none\", col = heat.colors(10))\n\n\n\n\n\n\nMultiplying the protein levels by the weight vector shows how much each protein contributes in a sample. The result shows total protein concentration per sample.\nThe result shows that sample S2 has the highest protein burden.\nThe identity matrix represents no protein interactions or measurement biases. It is a simple matrix calculation.\nNew calculation:\n\n\n# changing the weight of ProteinZ to 3.0\nnewweightvector = matrix(\n  c(0.5, 1.0, 3.0),\n  nrow=3, byrow = TRUE\n)\nrownames(WeightVector) = c(\"ProteinX\", \"ProteinY\", \"ProteinZ\")\ncolnames(WeightVector) = c(\"Weight\")\nnewTotalconc = ProteinMatrix %*% newweightvector\ncolnames(newTotalconc) &lt;- \"Total_Protein_Conc\"\nnewTotalconc\n\n        Total_Protein_Conc\nSample1               11.5\nSample2               21.5\n\n\nStill, S2 has more protein burden.\nBonus:\n\nHeatmap reveals PX is most abundant across all samples.\n\n\n# making  Gene Expression matrix\nGeneExpression &lt;- matrix(\n  c(10, 8, 5,\n    15, 12, 10),\n  nrow = 2, byrow = TRUE\n)\nrownames(GeneExpression) &lt;- c(\"Sample1\", \"Sample2\")\ncolnames(GeneExpression) &lt;- c(\"GeneA\", \"GeneB\", \"GeneC\")\nGeneExpression\n\n        GeneA GeneB GeneC\nSample1    10     8     5\nSample2    15    12    10\n\n\nTranslation efficiency:\n\n# making Translation Matrix\nTranslationMatrix &lt;- matrix(\n  c(1.5, 0 , 0,\n  0, 1.2, 0,\n  0, 0, 1.8),\nnrow = 3, byrow = TRUE\n)\n\nrownames(TranslationMatrix) &lt;- c(\"GeneA\", \"GeneB\", \"GeneC\")\ncolnames(TranslationMatrix) &lt;- c(\"protA\", \"protB\", \"protC\")\nTranslationMatrix\n\n      protA protB protC\nGeneA   1.5   0.0   0.0\nGeneB   0.0   1.2   0.0\nGeneC   0.0   0.0   1.8\n\n\n\n\n# computing Protein matrix\nProtein_matrix &lt;- GeneExpression %*% TranslationMatrix\ncolnames(Protein_matrix) &lt;- c(\"total_protA\", \"total_protB\", \"total_protC\")\nprint(Protein_matrix)\n\n        total_protA total_protB total_protC\nSample1        15.0         9.6           9\nSample2        22.5        14.4          18\n\n\n\n\n# Transpose of GeneExpression matrix\nGeneExpression_Transpose &lt;- t(GeneExpression)\nGeneExpression_Transpose\n\n      Sample1 Sample2\nGeneA      10      15\nGeneB       8      12\nGeneC       5      10\n\n\nThe new matrix represnts a matrix where the rows and columns of GeneExpression matrix have been interchanged.\n\n\n# Creating Identity matrix\nI &lt;- diag(3)\nI\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n\n\nNow, multiply:\n\nProduct_matrix = TranslationMatrix %*%  I \nProduct_matrix\n\n      [,1] [,2] [,3]\nGeneA  1.5  0.0  0.0\nGeneB  0.0  1.2  0.0\nGeneC  0.0  0.0  1.8\n\n\nThe product is identical to TranslationMatrix\n\n\n# making submatrix A\nA = matrix(\n  c(10, 8,\n    15, 12), nrow=2, byrow = TRUE\n)\nrownames(A) = c(\"sample1\", \"sample2\")\ncolnames(A) = c(\"GeneA\", \"GeneB\")\n\nA\n\n        GeneA GeneB\nsample1    10     8\nsample2    15    12\n\n# finding inverse of A\n#inv_A &lt;- solve(A)\n#inv_A\n\nThe inverse matrix could not be calculated since A is a singular matrix. So, A * A^-1 is also not possible.\n\n\n\nMARplot\n\n\n\n# generating MARplot-style scatter plot\nplot(GeneExpression, Protein_matrix, type=\"p\", main=\"Protein level vs. Gene Expression level\")\nlabels &lt;- \"Sample-Gene\"\ntext(GeneExpression, Protein_matrix, labels = labels, pos=3)\n\n\n\n# generating a heatmap\nheatmap(Protein_matrix, main= \"Heatmap of Protein Level\", Rowv = TRUE, Colv = TRUE, labRow= rownames(Protein_matrix), labCol= c(\"ProteinA\", \"ProteinB\", \"ProteinC\"), col=topo.colors(256) )\n\n\n\n\n\n\nHeatmap of Expression:\n\n\n\nheatmap(GeneExpression, col = terrain.colors(10), scale = \"column\")\n\n\n\n\n\n\nMatrix multiplication allows each gene in both samples to be multiplied to their respective translation efficiency. So, the product shows how successfully each gene is translated”)\nThe diagonal TranslationMatrix make sense biologically because they show translation efficiency of each gene and there is no other interaction between them. Although there could be interaction in real-world scenarios.\nIf Sample2 has higher protein levels even with similar gene expression, it means that more mRNAs are translated to proteins compared to Sample1”\nThe upward trend in MARplot may indicate an increase in translation efficacy and downward trend may indicate a decline in translation efficacy”\nClustering in the heatmap may suggest which samples are most similar to each other based on their prot.\n\n\n\n# Define Bull EBVs\nBullEBVs &lt;- matrix(c(\n  400, 1.2, 0.8,\n  500, 1.5, 0.6\n), nrow = 2, byrow = TRUE)\n\nrownames(BullEBVs) &lt;- c(\"Bull1\", \"Bull2\")\ncolnames(BullEBVs) &lt;- c(\"Milk_yield\", \"Growth_rate\", \"Fertility\")\nBullEBVs\n\n      Milk_yield Growth_rate Fertility\nBull1        400         1.2       0.8\nBull2        500         1.5       0.6\n\n# Define Economic Weights\nEconomicWeights &lt;- matrix(c(0.002, 50, 100), ncol = 1)\nrownames(EconomicWeights) &lt;- colnames(BullEBVs)\ncolnames(EconomicWeights) &lt;- c(\"Weight\")\nEconomicWeights\n\n            Weight\nMilk_yield   2e-03\nGrowth_rate  5e+01\nFertility    1e+02\n\n\n\n\nTotalValue &lt;- BullEBVs %*% EconomicWeights\ncolnames(TotalValue) &lt;- c(\"Merit\")\nTotalValue\n\n      Merit\nBull1 140.8\nBull2 136.0\n\n\nInterpretation\nBull1: (400 × 0.002) + (1.2 × 50) + (0.8 × 100) = 140.8\nBull2: (500 × 0.002) + (1.5 × 50) + (0.6 × 100) = 136.0\nBull1 is more valuable economically.\nBiological Interpretation\nEconomic weights convert genetic merit (EBVs, Estimated Breeding Values) into economic merit. Traits with higher financial importance have a larger impact, regardless of absolute EBV values.\n\n\nI3 &lt;- diag(3)\nrownames(I3) &lt;- colnames(BullEBVs)\ncolnames(I3) &lt;- colnames(BullEBVs)\nI3\n\n            Milk_yield Growth_rate Fertility\nMilk_yield           1           0         0\nGrowth_rate          0           1         0\nFertility            0           0         1\n\nBullEBVs_identity &lt;- BullEBVs %*% I3\nBullEBVs_identity\n\n      Milk_yield Growth_rate Fertility\nBull1        400         1.2       0.8\nBull2        500         1.5       0.6\n\n\nInterpretation\nMultiplying by identity matrix returns the original matrix. It confirms that EBV structure is preserved.\n\n\nBullEBVs_noMilk &lt;- BullEBVs[, -1]\nEconomicWeights_noMilk &lt;- EconomicWeights[2:3, , drop = FALSE]\n\nTotalValue_noMilk &lt;- BullEBVs_noMilk %*% EconomicWeights_noMilk\ncolnames(TotalValue_noMilk) &lt;- c(\"New_Merit\")\nTotalValue_noMilk\n\n      New_Merit\nBull1       140\nBull2       135\n\n\nInterpretation\nBull1: (1.2 × 50) + (0.8 × 100) = 140\nBull2: (1.5 × 50) + (0.6 × 100) = 135\nBull1 still ranks higher, but by a smaller margin.\n\n\nbarplot(\n  TotalValue,\n  beside = TRUE,\n  names.arg = rownames(BullEBVs),\n  col = c(\"skyblue\", \"orange\"),\n  main = \"Total Economic Value of Bulls\",\n  ylab = \"Total Value\"\n)\n\n\n\n\n\n\nheatmap(\n  BullEBVs,\n  Rowv = NA,\n  Colv = NA,\n  scale = \"none\",\n  col = heat.colors(256),\n  main = \"Heatmap of Bull EBVs\"\n)\n\n\n\n\n\n\nHow do economic weights affect trait importance?\n\nTraits with higher weights contribute more to the total economic value. This makes them more influential in ranking and selection.\n\nWhy might you ignore milk yield?\n\nMilk yield may be excluded in systems focusing on fertility, growth, or when it is no longer a limiting factor. Environmental or economic contexts may also shift trait priorities.\n\nWhat is the value of heatmaps?\n\nHeatmaps visually compare EBVs across bulls and traits.They help detect patterns, outliers, and clusters easily in multivariate data.\n\nCan this method be extended to more bulls and traits?\n\nYes. This method scales to any number of bulls or traits. Just ensure the EBVs matrix and economic weights are dimensionally compatible.\n\n\nParent trait values (normalized 1–10)\n\nParentTraits &lt;- matrix(c(\n  7, 5, 3,\n  6, 8, 4,\n  5, 6, 6\n), nrow = 3, byrow = TRUE)\n\nrownames(ParentTraits) &lt;- c(\"P1\", \"P2\", \"P3\")\ncolnames(ParentTraits) &lt;- c(\"Drought_resistance\", \"Yield\", \"Maturation_time\")\nParentTraits\n\n   Drought_resistance Yield Maturation_time\nP1                  7     5               3\nP2                  6     8               4\nP3                  5     6               6\n\n# Define Hybrid Weights\nHybridWeights &lt;- matrix(c(0.5, 0.3, 0.2), nrow = 1)\ncolnames(HybridWeights) &lt;- colnames(ParentTraits)\nrownames(HybridWeights) &lt;- c(\"Weight\")\nHybridWeights\n\n       Drought_resistance Yield Maturation_time\nWeight                0.5   0.3             0.2\n\n\n\n\nHybridTraits &lt;- HybridWeights %*% ParentTraits\nrownames(HybridTraits) &lt;- c(\"Contribution\")\ncolnames(HybridTraits) &lt;- rownames(ParentTraits)\nHybridTraits\n\n              P1  P2  P3\nContribution 6.3 6.1 3.9\n\n\nInterpretation\nHybridTraits = (0.5 × P1) + (0.3 × P2) + (0.2 × P3)\nDrought_resistance = (0.5 × 7) + (0.3 × 6) + (0.2 × 5) = 6.3\nYield = (0.5 × 5) + (0.3 × 8) + (0.2 × 6) = 6.1\nMaturation_time = (0.5 × 3) + (0.3 × 4) + (0.2 × 6) = 3.9\nThe hybrid is moderately strong in drought resistance and yield, and has a relatively shorter maturation time.\nBiological Meaning of Unequal Contribution\nWhen one parent contributes more to a trait, it suggests that the trait’s heritable strength comes disproportionately from that parent. Breeders can use this knowledge to amplify desirable traits using the best parent.\n\n\nI3 &lt;- diag(3)\nParentTraits_identity &lt;- ParentTraits %*% I3\ncolnames(ParentTraits_identity) &lt;- colnames(ParentTraits)\nParentTraits_identity\n\n   Drought_resistance Yield Maturation_time\nP1                  7     5               3\nP2                  6     8               4\nP3                  5     6               6\n\n\nInterpretation\nMultiplying by identity matrix returns the original matrix. This operation verifies structural consistency and dimensionality.\n\n\nParentTraits_T1T2 &lt;- ParentTraits[, 1:2]\nParentTraits_T1T2\n\n   Drought_resistance Yield\nP1                  7     5\nP2                  6     8\nP3                  5     6\n\nHybridTraits_T1T2 &lt;- HybridWeights %*% ParentTraits_T1T2\nHybridTraits_T1T2\n\n       Drought_resistance Yield\nWeight                6.3   6.1\n\n\nInterpretation\nDrought_resistance = (0.5 × 7) + (0.3 × 6) + (0.2 × 5) = 6.3\nYield = (0.5 × 5) + (0.3 × 8) + (0.2 × 6) = 6.1\nRemoving a trait (T3) changes the trait profile. Hybrid selection may now favor traits that remain.\n\n\nheatmap(\n  ParentTraits,\n  Rowv = NA,\n  Colv = NA,\n  scale = \"none\",\n  col = heat.colors(256),\n  main = \"Heatmap of Parent Traits\"\n)\n\n\n\n\n\n\nbarplot(\n  HybridTraits,\n  beside = TRUE,\n  names.arg = colnames(ParentTraits),\n  col = c(\"#66c2a5\", \"#fc8d62\", \"#8da0cb\"),\n  main = \"Hybrid Trait Profile\",\n  ylab = \"Trait Value\"\n)\n\n\n\n\n\n\n\nHow does the weighting of parents affect the hybrid’s performance?\nStronger weights mean more genetic contribution. Traits from highly weighted parents dominate the hybrid profile.\n\n\nWhat does the identity matrix represent here?\nIt represents a neutral transformation. It confirms data integrity when used in matrix multiplication.\n\n\nIf you used equal weights (⅓ for each), how would the hybrid traits change?\nTraits would reflect an even mix, potentially leading to balanced but less specialized performance.\n\n\nWhat real-world limitations does this simplified model ignore?\n\nNon-additive genetic effects (dominance, epistasis)\nEnvironmental interactions\nTrait heritability and correlations\nBreeding feasibility and cost\n\n\n\nNow that we’ve explored trait-based decisions using matrices, it’s time to organize our work using R’s list structure. Lists help bundle related objects like matrices and weight vectors, keeping the analysis modular and scalable.\n\n\n# Assuming previous matrices and weights are already defined:\n\n# Making a MasterList\nbioList = list(\n  ProteinConc = list(matrix = ProteinMatrix, weights = WeightVector),\n  ProteinMap  = list(matrix = GeneExpression, weights = TranslationMatrix),\n  Animal = list(matrix = BullEBVs, weights = EconomicWeights),\n  Plant = list(matrix = ParentTraits, weights = HybridWeights)\n)\n\nprint(bioList)\n\n$ProteinConc\n$ProteinConc$matrix\n        ProteinX ProteinY ProteinZ\nSample1        5        3        2\nSample2        7        6        4\n\n$ProteinConc$weights\n         Weight\nProteinX    0.5\nProteinY    1.0\nProteinZ    1.5\n\n\n$ProteinMap\n$ProteinMap$matrix\n        GeneA GeneB GeneC\nSample1    10     8     5\nSample2    15    12    10\n\n$ProteinMap$weights\n      protA protB protC\nGeneA   1.5   0.0   0.0\nGeneB   0.0   1.2   0.0\nGeneC   0.0   0.0   1.8\n\n\n$Animal\n$Animal$matrix\n      Milk_yield Growth_rate Fertility\nBull1        400         1.2       0.8\nBull2        500         1.5       0.6\n\n$Animal$weights\n            Weight\nMilk_yield   2e-03\nGrowth_rate  5e+01\nFertility    1e+02\n\n\n$Plant\n$Plant$matrix\n   Drought_resistance Yield Maturation_time\nP1                  7     5               3\nP2                  6     8               4\nP3                  5     6               6\n\n$Plant$weights\n       Drought_resistance Yield Maturation_time\nWeight                0.5   0.3             0.2\n\n\n\n\nnames(bioList)               # Top-level list names\n\n[1] \"ProteinConc\" \"ProteinMap\"  \"Animal\"      \"Plant\"      \n\nlengths(bioList)             # Number of components in each sublist\n\nProteinConc  ProteinMap      Animal       Plant \n          2           2           2           2 \n\n\nInterpretation\nEach top-level entry (e.g., ProteinConc, Plant) contains two components:\n\nA matrix (e.g., ProteinMatrix)\nA corresponding weight vector or matrix\n\n\n# Access the trait matrix for Plant\nbioList$Plant[[1]]\n\n   Drought_resistance Yield Maturation_time\nP1                  7     5               3\nP2                  6     8               4\nP3                  5     6               6\n\n#or\nbioList$Plant$matrix\n\n   Drought_resistance Yield Maturation_time\nP1                  7     5               3\nP2                  6     8               4\nP3                  5     6               6\n\n# Access the weight vector for ProteinConc\nbioList$ProteinConc[[2]]\n\n         Weight\nProteinX    0.5\nProteinY    1.0\nProteinZ    1.5\n\n#or\nbioList$ProteinConc$weights\n\n         Weight\nProteinX    0.5\nProteinY    1.0\nProteinZ    1.5\n\n\nInterpretation\nUse double brackets [[ ]] to extract unnamed list elements by position. But we named our list, so they are easily extractable using the $ notation.\n\n\n# Protein concentration score\nbioList$ProteinConc$matrix %*% bioList$ProteinConc$weights\n\n        Weight\nSample1    8.5\nSample2   15.5\n\n# Gene → Protein contribution\nbioList$ProteinMap$matrix %*% bioList$ProteinMap$weights\n\n        protA protB protC\nSample1  15.0   9.6     9\nSample2  22.5  14.4    18\n\n# Bull economic value\nbioList$Animal$matrix %*% bioList$Animal$weights\n\n      Weight\nBull1  140.8\nBull2  136.0\n\n# Hybrid trait value\nbioList$Plant$weights %*% bioList$Plant$matrix\n\n       Drought_resistance Yield Maturation_time\nWeight                6.3   6.1             3.9\n\n\n\n\n# Remove last trait from ParentTraits\nParentSubset &lt;- bioList$Plant$matrix[, 1:2]\nNewWeights &lt;- matrix(c(0.6, 0.4), nrow = 2)\n\n# Recalculated hybrid score\nSubsetHybridScore &lt;- ParentSubset %*% NewWeights\nSubsetHybridScore\n\n   [,1]\nP1  6.2\nP2  6.8\nP3  5.4\n\n\nInterpretation\nDropping a trait and reweighting highlights its influence in trait aggregation and selection.\n\n\n\nheatmap(\n  bioList$ProteinMap$matrix,\n  scale = \"none\",\n  col = heat.colors(256),\n  main = \"Gene Expression Heatmap\",\n  xlab = \"Proteins\",\n  ylab = \"Genes\"\n)\n\n\n\n\n\n\n\nbarplot(\n  bioList$Plant$weights %*% bioList$Plant$matrix,\n  beside = TRUE,\n  main = \"Hybrid Trait Contributions\",\n  col = \"#66c2a5\",\n  ylab = \"Score\"\n)\n\n\n\n\n\n\nbarplot(\n  bioList$Animal$matrix %*% bioList$Animal$weights,\n  beside = TRUE,\n  main = \"Bull EBVs (Economic Values)\",\n  col = \"#fc8d62\",\n  ylab = \"Score\"\n)\n\n\n\n\n\n\nWhy use a list structure?\n\nKeeps each dataset and its weights together. Facilitates automated workflows and reuse.\n\nWhat’s tricky about [[ ]] access?\n\nYou must remember the order ([[1]] = matrix, [[2]] = weights). No names means you can’t use $matrix, only positional access.\nLoop across all list entries\nWeighted scores for all entries lapply(bioList, function(x) x[[2]] %*% x[[1]])\n\nHow does this help in large-scale pipelines?\n\nYou can use this format with lapply(), purrr::map(), or in targets pipelines for reproducibility and modular processing.\n\n\nCharacter vs Factor A character vector simply holds string values, but a factor is a categorical variable with fixed levels, used especially in modeling.\nFor mutation_status, a factor ensures consistent categories (e.g., \"Yes\" or \"No\") and helps control level order and statistical reference groups.\nFactor Levels\n\n\nspecies &lt;- c(\"Lactobacillus\", \"Bacteroides\", \"Escherichia\", \"Bacteroides\", \"Lactobacillus\")\nspecies_factor &lt;- factor(species, levels = c(\"Bacteroides\", \"Escherichia\", \"Lactobacillus\"))\nlevels(species_factor)\n\n[1] \"Bacteroides\"   \"Escherichia\"   \"Lactobacillus\"\n\n\nBecause we defined the level order explicitly, R maintains that order regardless of data input.\n\nOrdered Factor Comparison\n\n\ndisease_severity &lt;- factor(c(\"Mild\", \"Severe\", \"Moderate\"), levels = c(\"Mild\", \"Moderate\", \"Severe\", \"Critical\"), ordered = TRUE)\ndisease_severity[1] &lt; disease_severity[2]\n\n[1] TRUE\n\n# TRUE\n\n“Mild” is less severe than “Severe” based on the defined order.\n\nProportion Extraction\n\n\nprop &lt;- prop.table(table(species_factor))\nprop[\"Escherichia\"]\n\nEscherichia \n        0.2 \n\n\nprop$Escherichia won’t work — named numeric vectors require bracket-based access.\n\nSubsetting by Conditions\n\n\ngene_df &lt;- data.frame(\n  gene_id = c(\"BRCA1\", \"TP53\", \"MYC\", \"EGFR\", \"GAPDH\"),\n  expression = c(8.2, 6.1, 9.5, 7.0, 10.0),\n  mutation = factor(c(\"Yes\", \"No\", \"Yes\", \"No\", \"No\")),\n  pathway = c(\"DNA Repair\", \"Apoptosis\", \"Cell Cycle\", \"Signaling\", \"Metabolism\")\n)\nrownames(gene_df) &lt;- gene_df$gene_id #name the rows by the gene IDs\ngene_df &lt;- gene_df[, -1] #remove the first column which is not needed anymore\n#gene_df\ngene_df[gene_df$expression &gt; 7 & gene_df$mutation == \"No\", ]\n\n      expression mutation    pathway\nGAPDH         10       No Metabolism\n\n\nReturns genes with high expression (&gt;7) and no mutation — potentially highly active but wild-type genes.\n\nGroup-wise Expression Summary\n\nThe given vectors are:\n\nsamples &lt;- c(\"WT\", \"KO\", \"WT\", \"KO\", \"WT\")\nexpression &lt;- c(5.2, 8.1, 4.3, 9.0, 5.7)\n\nThe solution would be:\n\ngroup_factor &lt;- factor(samples)\n\n# Mean expression\ntapply(expression, group_factor, mean) ## KO: 8.55, WT: 5.07\n\n      KO       WT \n8.550000 5.066667 \n\n# Plot\nbarplot(tapply(expression, group_factor, mean), \n        col = c(\"skyblue\", \"salmon\"),\n        ylab = \"Mean Expression\",\n        main = \"Group-wise Expression\")\n\n\n\n\n\nGene Subsetting\n\n\ngene_df[gene_df$expression &gt; 8 & \n        gene_df$pathway %in% c(\"Cell Cycle\", \"Signaling\"), ]\n\n    expression mutation    pathway\nMYC        9.5      Yes Cell Cycle\n\n\nIt filters for genes highly expressed and involved in key biological pathways.\n\nDisease Stage Visualization\n\n\nstages &lt;- c(\"Stage I\", \"Stage III\", \"Stage II\", \"Stage IV\", \"Stage I\")\ndisease_stage &lt;- factor(stages, \n                        levels = c(\"Stage I\", \"Stage II\", \"Stage III\", \"Stage IV\"), \n                        ordered = TRUE)\n\nbarplot(table(disease_stage), \n        col = \"lightgreen\", \n        main = \"Patient Count by Disease Stage\",\n        ylab = \"Count\")\n\n\n\n\nLet’s do the severity order check:\n\n# Comparison\ndisease_stage[2] &gt; disease_stage[1]  # TRUE\n\n[1] TRUE\n\n\nSo, “Stage III” is more sever than “Stage I”.\n\nOncogene Subsetting and Releveling\n\n\n# Define a small gene dataset\ngene_data &lt;- data.frame(\n  gene = c(\"TP53\", \"BRCA1\", \"MYC\", \"GAPDH\", \"EGFR\"),\n  expression = c(9.1, 7.3, 10.5, 5.2, 8.6),\n  type = factor(c(\"Tumor Suppressor\", \"Oncogene\", \"Oncogene\", \"Housekeeping\", \"Oncogene\"))\n)\n\n# Subset: Oncogene rows with expression &gt; 8\ngene_data[gene_data$type == \"Oncogene\" & gene_data$expression &gt; 8, ]\n\n  gene expression     type\n3  MYC       10.5 Oncogene\n5 EGFR        8.6 Oncogene\n\n\nLet’s relevel now, “Housekeeping” is the reference:\n\n# Relevel: make \"Housekeeping\" the reference level\ngene_data$type &lt;- relevel(gene_data$type, ref = \"Housekeeping\")\n\n# Check the new levels\nlevels(gene_data$type)\n\n[1] \"Housekeeping\"     \"Oncogene\"         \"Tumor Suppressor\"\n\n\nBut the “Housekeeping” was already a reference by default (using alphabetic ordering by R). Making something else the reference would make more sense. Our code above work, but nothing new is done.\n\nSimulated Expression by Tissue\n\n\nset.seed(42)\ngene_expr &lt;- rnorm(45, mean = 8, sd = 2)\ntissue &lt;- rep(c(\"brain\", \"liver\", \"kidney\"), each = 15)\ntissue_factor &lt;- factor(tissue, levels = c(\"liver\", \"brain\", \"kidney\"))\n\nboxplot(gene_expr ~ tissue_factor, \n        col = c(\"orange\", \"skyblue\", \"lightgreen\"), \n        main = \"Expression by Tissue\",\n        ylab = \"Expression Level\")\n\n\n\n\nLet’s calculate variability per tissue type now:\n\n# Variability\ntapply(gene_expr, tissue_factor, sd)\n\n   liver    brain   kidney \n2.713940 2.050487 1.993668 \n\n# Returns standard deviation per tissue group\n\n\n\n# Load necessary package\nlibrary(ggplot2)\n\n# Use the built-in iris dataset\ndata(iris)\n\n# Create the plot\nggplot(iris, aes(x = Petal.Length, y = Petal.Width, color = Species)) +\n  geom_point(size = 2) +  # scatter plot points\n  geom_smooth(method = \"lm\", se = FALSE) +  # linear regression lines\n  labs(\n    title = \"Relationship between Petal Length and Width by Species\",\n    x = \"Petal Length (cm)\",\n    y = \"Petal Width (cm)\",\n    color = \"Iris Species\"\n  ) +\n  theme_minimal()  # apply a clean minimal theme\n\n\n\n\n\nlibrary(ggplot2)\n\nggplot(iris, aes(x = Petal.Length, y = Petal.Width)) +\n  geom_point(aes(color = Species), size = 2) +  # color points by Species\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\") +  # single regression line\n  labs(\n    title = \"Overall Regression: Petal Length vs Width (Iris Dataset)\",\n    x = \"Petal Length (cm)\",\n    y = \"Petal Width (cm)\",\n    color = \"Iris Species\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "ch/rbasics/solutions.html#task-1-protein-quantification-in-biological-samples",
    "href": "ch/rbasics/solutions.html#task-1-protein-quantification-in-biological-samples",
    "title": "HW solutions",
    "section": "",
    "text": "We measured the concentration (in µg/µL) of three proteins (P1, P2, P3) in four samples (S1–S4):\n\n\n# Making Protein Matrix\nProteinMatrix &lt;- matrix(\n  c(5, 3, 2,\n    7, 6, 4),\n  nrow = 2, byrow = TRUE\n)\nrownames(ProteinMatrix) = c(\"Sample1\", \"Sample2\")\ncolnames(ProteinMatrix) = c(\"ProteinX\", \"ProteinY\", \"ProteinZ\")\nProteinMatrix\n\n        ProteinX ProteinY ProteinZ\nSample1        5        3        2\nSample2        7        6        4\n\n\nNow goes the weight matrix\n\n# Making weight matrix\nWeightVector &lt;- matrix(\n  c(0.5, 1.0, 1.5),\n  nrow=3, byrow = TRUE\n)\nrownames(WeightVector) = c(\"ProteinX\", \"ProteinY\", \"ProteinZ\")\ncolnames(WeightVector) = c(\"Weight\")\nWeightVector\n\n         Weight\nProteinX    0.5\nProteinY    1.0\nProteinZ    1.5\n\n\nNow, multiply them.\n\n# Multiplying Matrices\nTotalConc = ProteinMatrix %*% WeightVector\ncolnames(TotalConc) &lt;- \"Total_Protein_Conc\"\nprint(TotalConc)\n\n        Total_Protein_Conc\nSample1                8.5\nSample2               15.5\n\n\n\n\nProteinMatTranspose = t(ProteinMatrix)\nProteinMatTranspose\n\n         Sample1 Sample2\nProteinX       5       7\nProteinY       3       6\nProteinZ       2       4\n\n\n\n\nI &lt;- diag(3)\nIdentitycheck = ProteinMatrix %*% I\ncolnames(Identitycheck) &lt;- c(\"ProteinX\", \"ProteinY\", \"ProteinZ\")\nIdentitycheck\n\n        ProteinX ProteinY ProteinZ\nSample1        5        3        2\nSample2        7        6        4\n\n\n\n\nrowSums(ProteinMatrix)\n\nSample1 Sample2 \n     10      17 \n\n\n\n\ncolSums(ProteinMatrix)\n\nProteinX ProteinY ProteinZ \n      12        9        6 \n\n\n\n\nheatmap(ProteinMatrix, scale = \"none\", col = heat.colors(10))\n\n\n\n\n\n\nMultiplying the protein levels by the weight vector shows how much each protein contributes in a sample. The result shows total protein concentration per sample.\nThe result shows that sample S2 has the highest protein burden.\nThe identity matrix represents no protein interactions or measurement biases. It is a simple matrix calculation.\nNew calculation:\n\n\n# changing the weight of ProteinZ to 3.0\nnewweightvector = matrix(\n  c(0.5, 1.0, 3.0),\n  nrow=3, byrow = TRUE\n)\nrownames(WeightVector) = c(\"ProteinX\", \"ProteinY\", \"ProteinZ\")\ncolnames(WeightVector) = c(\"Weight\")\nnewTotalconc = ProteinMatrix %*% newweightvector\ncolnames(newTotalconc) &lt;- \"Total_Protein_Conc\"\nnewTotalconc\n\n        Total_Protein_Conc\nSample1               11.5\nSample2               21.5\n\n\nStill, S2 has more protein burden.\nBonus:\n\nHeatmap reveals PX is most abundant across all samples."
  },
  {
    "objectID": "ch/rbasics/solutions.html#task-2-gene-to-protein-translation",
    "href": "ch/rbasics/solutions.html#task-2-gene-to-protein-translation",
    "title": "HW solutions",
    "section": "",
    "text": "# making  Gene Expression matrix\nGeneExpression &lt;- matrix(\n  c(10, 8, 5,\n    15, 12, 10),\n  nrow = 2, byrow = TRUE\n)\nrownames(GeneExpression) &lt;- c(\"Sample1\", \"Sample2\")\ncolnames(GeneExpression) &lt;- c(\"GeneA\", \"GeneB\", \"GeneC\")\nGeneExpression\n\n        GeneA GeneB GeneC\nSample1    10     8     5\nSample2    15    12    10\n\n\nTranslation efficiency:\n\n# making Translation Matrix\nTranslationMatrix &lt;- matrix(\n  c(1.5, 0 , 0,\n  0, 1.2, 0,\n  0, 0, 1.8),\nnrow = 3, byrow = TRUE\n)\n\nrownames(TranslationMatrix) &lt;- c(\"GeneA\", \"GeneB\", \"GeneC\")\ncolnames(TranslationMatrix) &lt;- c(\"protA\", \"protB\", \"protC\")\nTranslationMatrix\n\n      protA protB protC\nGeneA   1.5   0.0   0.0\nGeneB   0.0   1.2   0.0\nGeneC   0.0   0.0   1.8\n\n\n\n\n# computing Protein matrix\nProtein_matrix &lt;- GeneExpression %*% TranslationMatrix\ncolnames(Protein_matrix) &lt;- c(\"total_protA\", \"total_protB\", \"total_protC\")\nprint(Protein_matrix)\n\n        total_protA total_protB total_protC\nSample1        15.0         9.6           9\nSample2        22.5        14.4          18\n\n\n\n\n# Transpose of GeneExpression matrix\nGeneExpression_Transpose &lt;- t(GeneExpression)\nGeneExpression_Transpose\n\n      Sample1 Sample2\nGeneA      10      15\nGeneB       8      12\nGeneC       5      10\n\n\nThe new matrix represnts a matrix where the rows and columns of GeneExpression matrix have been interchanged.\n\n\n# Creating Identity matrix\nI &lt;- diag(3)\nI\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n\n\nNow, multiply:\n\nProduct_matrix = TranslationMatrix %*%  I \nProduct_matrix\n\n      [,1] [,2] [,3]\nGeneA  1.5  0.0  0.0\nGeneB  0.0  1.2  0.0\nGeneC  0.0  0.0  1.8\n\n\nThe product is identical to TranslationMatrix\n\n\n# making submatrix A\nA = matrix(\n  c(10, 8,\n    15, 12), nrow=2, byrow = TRUE\n)\nrownames(A) = c(\"sample1\", \"sample2\")\ncolnames(A) = c(\"GeneA\", \"GeneB\")\n\nA\n\n        GeneA GeneB\nsample1    10     8\nsample2    15    12\n\n# finding inverse of A\n#inv_A &lt;- solve(A)\n#inv_A\n\nThe inverse matrix could not be calculated since A is a singular matrix. So, A * A^-1 is also not possible.\n\n\n\nMARplot\n\n\n\n# generating MARplot-style scatter plot\nplot(GeneExpression, Protein_matrix, type=\"p\", main=\"Protein level vs. Gene Expression level\")\nlabels &lt;- \"Sample-Gene\"\ntext(GeneExpression, Protein_matrix, labels = labels, pos=3)\n\n\n\n# generating a heatmap\nheatmap(Protein_matrix, main= \"Heatmap of Protein Level\", Rowv = TRUE, Colv = TRUE, labRow= rownames(Protein_matrix), labCol= c(\"ProteinA\", \"ProteinB\", \"ProteinC\"), col=topo.colors(256) )\n\n\n\n\n\n\nHeatmap of Expression:\n\n\n\nheatmap(GeneExpression, col = terrain.colors(10), scale = \"column\")\n\n\n\n\n\n\nMatrix multiplication allows each gene in both samples to be multiplied to their respective translation efficiency. So, the product shows how successfully each gene is translated”)\nThe diagonal TranslationMatrix make sense biologically because they show translation efficiency of each gene and there is no other interaction between them. Although there could be interaction in real-world scenarios.\nIf Sample2 has higher protein levels even with similar gene expression, it means that more mRNAs are translated to proteins compared to Sample1”\nThe upward trend in MARplot may indicate an increase in translation efficacy and downward trend may indicate a decline in translation efficacy”\nClustering in the heatmap may suggest which samples are most similar to each other based on their prot."
  },
  {
    "objectID": "ch/rbasics/solutions.html#task-3-animal-breeding-bull-ranking-by-economic-traits",
    "href": "ch/rbasics/solutions.html#task-3-animal-breeding-bull-ranking-by-economic-traits",
    "title": "HW solutions",
    "section": "",
    "text": "# Define Bull EBVs\nBullEBVs &lt;- matrix(c(\n  400, 1.2, 0.8,\n  500, 1.5, 0.6\n), nrow = 2, byrow = TRUE)\n\nrownames(BullEBVs) &lt;- c(\"Bull1\", \"Bull2\")\ncolnames(BullEBVs) &lt;- c(\"Milk_yield\", \"Growth_rate\", \"Fertility\")\nBullEBVs\n\n      Milk_yield Growth_rate Fertility\nBull1        400         1.2       0.8\nBull2        500         1.5       0.6\n\n# Define Economic Weights\nEconomicWeights &lt;- matrix(c(0.002, 50, 100), ncol = 1)\nrownames(EconomicWeights) &lt;- colnames(BullEBVs)\ncolnames(EconomicWeights) &lt;- c(\"Weight\")\nEconomicWeights\n\n            Weight\nMilk_yield   2e-03\nGrowth_rate  5e+01\nFertility    1e+02\n\n\n\n\nTotalValue &lt;- BullEBVs %*% EconomicWeights\ncolnames(TotalValue) &lt;- c(\"Merit\")\nTotalValue\n\n      Merit\nBull1 140.8\nBull2 136.0\n\n\nInterpretation\nBull1: (400 × 0.002) + (1.2 × 50) + (0.8 × 100) = 140.8\nBull2: (500 × 0.002) + (1.5 × 50) + (0.6 × 100) = 136.0\nBull1 is more valuable economically.\nBiological Interpretation\nEconomic weights convert genetic merit (EBVs, Estimated Breeding Values) into economic merit. Traits with higher financial importance have a larger impact, regardless of absolute EBV values.\n\n\nI3 &lt;- diag(3)\nrownames(I3) &lt;- colnames(BullEBVs)\ncolnames(I3) &lt;- colnames(BullEBVs)\nI3\n\n            Milk_yield Growth_rate Fertility\nMilk_yield           1           0         0\nGrowth_rate          0           1         0\nFertility            0           0         1\n\nBullEBVs_identity &lt;- BullEBVs %*% I3\nBullEBVs_identity\n\n      Milk_yield Growth_rate Fertility\nBull1        400         1.2       0.8\nBull2        500         1.5       0.6\n\n\nInterpretation\nMultiplying by identity matrix returns the original matrix. It confirms that EBV structure is preserved.\n\n\nBullEBVs_noMilk &lt;- BullEBVs[, -1]\nEconomicWeights_noMilk &lt;- EconomicWeights[2:3, , drop = FALSE]\n\nTotalValue_noMilk &lt;- BullEBVs_noMilk %*% EconomicWeights_noMilk\ncolnames(TotalValue_noMilk) &lt;- c(\"New_Merit\")\nTotalValue_noMilk\n\n      New_Merit\nBull1       140\nBull2       135\n\n\nInterpretation\nBull1: (1.2 × 50) + (0.8 × 100) = 140\nBull2: (1.5 × 50) + (0.6 × 100) = 135\nBull1 still ranks higher, but by a smaller margin.\n\n\nbarplot(\n  TotalValue,\n  beside = TRUE,\n  names.arg = rownames(BullEBVs),\n  col = c(\"skyblue\", \"orange\"),\n  main = \"Total Economic Value of Bulls\",\n  ylab = \"Total Value\"\n)\n\n\n\n\n\n\nheatmap(\n  BullEBVs,\n  Rowv = NA,\n  Colv = NA,\n  scale = \"none\",\n  col = heat.colors(256),\n  main = \"Heatmap of Bull EBVs\"\n)\n\n\n\n\n\n\nHow do economic weights affect trait importance?\n\nTraits with higher weights contribute more to the total economic value. This makes them more influential in ranking and selection.\n\nWhy might you ignore milk yield?\n\nMilk yield may be excluded in systems focusing on fertility, growth, or when it is no longer a limiting factor. Environmental or economic contexts may also shift trait priorities.\n\nWhat is the value of heatmaps?\n\nHeatmaps visually compare EBVs across bulls and traits.They help detect patterns, outliers, and clusters easily in multivariate data.\n\nCan this method be extended to more bulls and traits?\n\nYes. This method scales to any number of bulls or traits. Just ensure the EBVs matrix and economic weights are dimensionally compatible."
  },
  {
    "objectID": "ch/rbasics/solutions.html#task-4-plant-breeding-trait-contributions-from-parental-lines",
    "href": "ch/rbasics/solutions.html#task-4-plant-breeding-trait-contributions-from-parental-lines",
    "title": "HW solutions",
    "section": "",
    "text": "Parent trait values (normalized 1–10)\n\nParentTraits &lt;- matrix(c(\n  7, 5, 3,\n  6, 8, 4,\n  5, 6, 6\n), nrow = 3, byrow = TRUE)\n\nrownames(ParentTraits) &lt;- c(\"P1\", \"P2\", \"P3\")\ncolnames(ParentTraits) &lt;- c(\"Drought_resistance\", \"Yield\", \"Maturation_time\")\nParentTraits\n\n   Drought_resistance Yield Maturation_time\nP1                  7     5               3\nP2                  6     8               4\nP3                  5     6               6\n\n# Define Hybrid Weights\nHybridWeights &lt;- matrix(c(0.5, 0.3, 0.2), nrow = 1)\ncolnames(HybridWeights) &lt;- colnames(ParentTraits)\nrownames(HybridWeights) &lt;- c(\"Weight\")\nHybridWeights\n\n       Drought_resistance Yield Maturation_time\nWeight                0.5   0.3             0.2\n\n\n\n\nHybridTraits &lt;- HybridWeights %*% ParentTraits\nrownames(HybridTraits) &lt;- c(\"Contribution\")\ncolnames(HybridTraits) &lt;- rownames(ParentTraits)\nHybridTraits\n\n              P1  P2  P3\nContribution 6.3 6.1 3.9\n\n\nInterpretation\nHybridTraits = (0.5 × P1) + (0.3 × P2) + (0.2 × P3)\nDrought_resistance = (0.5 × 7) + (0.3 × 6) + (0.2 × 5) = 6.3\nYield = (0.5 × 5) + (0.3 × 8) + (0.2 × 6) = 6.1\nMaturation_time = (0.5 × 3) + (0.3 × 4) + (0.2 × 6) = 3.9\nThe hybrid is moderately strong in drought resistance and yield, and has a relatively shorter maturation time.\nBiological Meaning of Unequal Contribution\nWhen one parent contributes more to a trait, it suggests that the trait’s heritable strength comes disproportionately from that parent. Breeders can use this knowledge to amplify desirable traits using the best parent.\n\n\nI3 &lt;- diag(3)\nParentTraits_identity &lt;- ParentTraits %*% I3\ncolnames(ParentTraits_identity) &lt;- colnames(ParentTraits)\nParentTraits_identity\n\n   Drought_resistance Yield Maturation_time\nP1                  7     5               3\nP2                  6     8               4\nP3                  5     6               6\n\n\nInterpretation\nMultiplying by identity matrix returns the original matrix. This operation verifies structural consistency and dimensionality.\n\n\nParentTraits_T1T2 &lt;- ParentTraits[, 1:2]\nParentTraits_T1T2\n\n   Drought_resistance Yield\nP1                  7     5\nP2                  6     8\nP3                  5     6\n\nHybridTraits_T1T2 &lt;- HybridWeights %*% ParentTraits_T1T2\nHybridTraits_T1T2\n\n       Drought_resistance Yield\nWeight                6.3   6.1\n\n\nInterpretation\nDrought_resistance = (0.5 × 7) + (0.3 × 6) + (0.2 × 5) = 6.3\nYield = (0.5 × 5) + (0.3 × 8) + (0.2 × 6) = 6.1\nRemoving a trait (T3) changes the trait profile. Hybrid selection may now favor traits that remain.\n\n\nheatmap(\n  ParentTraits,\n  Rowv = NA,\n  Colv = NA,\n  scale = \"none\",\n  col = heat.colors(256),\n  main = \"Heatmap of Parent Traits\"\n)\n\n\n\n\n\n\nbarplot(\n  HybridTraits,\n  beside = TRUE,\n  names.arg = colnames(ParentTraits),\n  col = c(\"#66c2a5\", \"#fc8d62\", \"#8da0cb\"),\n  main = \"Hybrid Trait Profile\",\n  ylab = \"Trait Value\"\n)\n\n\n\n\n\n\n\nHow does the weighting of parents affect the hybrid’s performance?\nStronger weights mean more genetic contribution. Traits from highly weighted parents dominate the hybrid profile.\n\n\nWhat does the identity matrix represent here?\nIt represents a neutral transformation. It confirms data integrity when used in matrix multiplication.\n\n\nIf you used equal weights (⅓ for each), how would the hybrid traits change?\nTraits would reflect an even mix, potentially leading to balanced but less specialized performance.\n\n\nWhat real-world limitations does this simplified model ignore?\n\nNon-additive genetic effects (dominance, epistasis)\nEnvironmental interactions\nTrait heritability and correlations\nBreeding feasibility and cost"
  },
  {
    "objectID": "ch/rbasics/solutions.html#task-5-managing-matrices-and-weight-vectors-using-lists-in-r",
    "href": "ch/rbasics/solutions.html#task-5-managing-matrices-and-weight-vectors-using-lists-in-r",
    "title": "HW solutions",
    "section": "",
    "text": "Now that we’ve explored trait-based decisions using matrices, it’s time to organize our work using R’s list structure. Lists help bundle related objects like matrices and weight vectors, keeping the analysis modular and scalable.\n\n\n# Assuming previous matrices and weights are already defined:\n\n# Making a MasterList\nbioList = list(\n  ProteinConc = list(matrix = ProteinMatrix, weights = WeightVector),\n  ProteinMap  = list(matrix = GeneExpression, weights = TranslationMatrix),\n  Animal = list(matrix = BullEBVs, weights = EconomicWeights),\n  Plant = list(matrix = ParentTraits, weights = HybridWeights)\n)\n\nprint(bioList)\n\n$ProteinConc\n$ProteinConc$matrix\n        ProteinX ProteinY ProteinZ\nSample1        5        3        2\nSample2        7        6        4\n\n$ProteinConc$weights\n         Weight\nProteinX    0.5\nProteinY    1.0\nProteinZ    1.5\n\n\n$ProteinMap\n$ProteinMap$matrix\n        GeneA GeneB GeneC\nSample1    10     8     5\nSample2    15    12    10\n\n$ProteinMap$weights\n      protA protB protC\nGeneA   1.5   0.0   0.0\nGeneB   0.0   1.2   0.0\nGeneC   0.0   0.0   1.8\n\n\n$Animal\n$Animal$matrix\n      Milk_yield Growth_rate Fertility\nBull1        400         1.2       0.8\nBull2        500         1.5       0.6\n\n$Animal$weights\n            Weight\nMilk_yield   2e-03\nGrowth_rate  5e+01\nFertility    1e+02\n\n\n$Plant\n$Plant$matrix\n   Drought_resistance Yield Maturation_time\nP1                  7     5               3\nP2                  6     8               4\nP3                  5     6               6\n\n$Plant$weights\n       Drought_resistance Yield Maturation_time\nWeight                0.5   0.3             0.2\n\n\n\n\nnames(bioList)               # Top-level list names\n\n[1] \"ProteinConc\" \"ProteinMap\"  \"Animal\"      \"Plant\"      \n\nlengths(bioList)             # Number of components in each sublist\n\nProteinConc  ProteinMap      Animal       Plant \n          2           2           2           2 \n\n\nInterpretation\nEach top-level entry (e.g., ProteinConc, Plant) contains two components:\n\nA matrix (e.g., ProteinMatrix)\nA corresponding weight vector or matrix\n\n\n# Access the trait matrix for Plant\nbioList$Plant[[1]]\n\n   Drought_resistance Yield Maturation_time\nP1                  7     5               3\nP2                  6     8               4\nP3                  5     6               6\n\n#or\nbioList$Plant$matrix\n\n   Drought_resistance Yield Maturation_time\nP1                  7     5               3\nP2                  6     8               4\nP3                  5     6               6\n\n# Access the weight vector for ProteinConc\nbioList$ProteinConc[[2]]\n\n         Weight\nProteinX    0.5\nProteinY    1.0\nProteinZ    1.5\n\n#or\nbioList$ProteinConc$weights\n\n         Weight\nProteinX    0.5\nProteinY    1.0\nProteinZ    1.5\n\n\nInterpretation\nUse double brackets [[ ]] to extract unnamed list elements by position. But we named our list, so they are easily extractable using the $ notation.\n\n\n# Protein concentration score\nbioList$ProteinConc$matrix %*% bioList$ProteinConc$weights\n\n        Weight\nSample1    8.5\nSample2   15.5\n\n# Gene → Protein contribution\nbioList$ProteinMap$matrix %*% bioList$ProteinMap$weights\n\n        protA protB protC\nSample1  15.0   9.6     9\nSample2  22.5  14.4    18\n\n# Bull economic value\nbioList$Animal$matrix %*% bioList$Animal$weights\n\n      Weight\nBull1  140.8\nBull2  136.0\n\n# Hybrid trait value\nbioList$Plant$weights %*% bioList$Plant$matrix\n\n       Drought_resistance Yield Maturation_time\nWeight                6.3   6.1             3.9\n\n\n\n\n# Remove last trait from ParentTraits\nParentSubset &lt;- bioList$Plant$matrix[, 1:2]\nNewWeights &lt;- matrix(c(0.6, 0.4), nrow = 2)\n\n# Recalculated hybrid score\nSubsetHybridScore &lt;- ParentSubset %*% NewWeights\nSubsetHybridScore\n\n   [,1]\nP1  6.2\nP2  6.8\nP3  5.4\n\n\nInterpretation\nDropping a trait and reweighting highlights its influence in trait aggregation and selection.\n\n\n\nheatmap(\n  bioList$ProteinMap$matrix,\n  scale = \"none\",\n  col = heat.colors(256),\n  main = \"Gene Expression Heatmap\",\n  xlab = \"Proteins\",\n  ylab = \"Genes\"\n)\n\n\n\n\n\n\n\nbarplot(\n  bioList$Plant$weights %*% bioList$Plant$matrix,\n  beside = TRUE,\n  main = \"Hybrid Trait Contributions\",\n  col = \"#66c2a5\",\n  ylab = \"Score\"\n)\n\n\n\n\n\n\nbarplot(\n  bioList$Animal$matrix %*% bioList$Animal$weights,\n  beside = TRUE,\n  main = \"Bull EBVs (Economic Values)\",\n  col = \"#fc8d62\",\n  ylab = \"Score\"\n)\n\n\n\n\n\n\nWhy use a list structure?\n\nKeeps each dataset and its weights together. Facilitates automated workflows and reuse.\n\nWhat’s tricky about [[ ]] access?\n\nYou must remember the order ([[1]] = matrix, [[2]] = weights). No names means you can’t use $matrix, only positional access.\nLoop across all list entries\nWeighted scores for all entries lapply(bioList, function(x) x[[2]] %*% x[[1]])\n\nHow does this help in large-scale pipelines?\n\nYou can use this format with lapply(), purrr::map(), or in targets pipelines for reproducibility and modular processing."
  },
  {
    "objectID": "ch/rbasics/solutions.html#homework-solutions-factors-subsetting-and-biological-insight",
    "href": "ch/rbasics/solutions.html#homework-solutions-factors-subsetting-and-biological-insight",
    "title": "HW solutions",
    "section": "",
    "text": "Character vs Factor A character vector simply holds string values, but a factor is a categorical variable with fixed levels, used especially in modeling.\nFor mutation_status, a factor ensures consistent categories (e.g., \"Yes\" or \"No\") and helps control level order and statistical reference groups.\nFactor Levels\n\n\nspecies &lt;- c(\"Lactobacillus\", \"Bacteroides\", \"Escherichia\", \"Bacteroides\", \"Lactobacillus\")\nspecies_factor &lt;- factor(species, levels = c(\"Bacteroides\", \"Escherichia\", \"Lactobacillus\"))\nlevels(species_factor)\n\n[1] \"Bacteroides\"   \"Escherichia\"   \"Lactobacillus\"\n\n\nBecause we defined the level order explicitly, R maintains that order regardless of data input.\n\nOrdered Factor Comparison\n\n\ndisease_severity &lt;- factor(c(\"Mild\", \"Severe\", \"Moderate\"), levels = c(\"Mild\", \"Moderate\", \"Severe\", \"Critical\"), ordered = TRUE)\ndisease_severity[1] &lt; disease_severity[2]\n\n[1] TRUE\n\n# TRUE\n\n“Mild” is less severe than “Severe” based on the defined order.\n\nProportion Extraction\n\n\nprop &lt;- prop.table(table(species_factor))\nprop[\"Escherichia\"]\n\nEscherichia \n        0.2 \n\n\nprop$Escherichia won’t work — named numeric vectors require bracket-based access.\n\nSubsetting by Conditions\n\n\ngene_df &lt;- data.frame(\n  gene_id = c(\"BRCA1\", \"TP53\", \"MYC\", \"EGFR\", \"GAPDH\"),\n  expression = c(8.2, 6.1, 9.5, 7.0, 10.0),\n  mutation = factor(c(\"Yes\", \"No\", \"Yes\", \"No\", \"No\")),\n  pathway = c(\"DNA Repair\", \"Apoptosis\", \"Cell Cycle\", \"Signaling\", \"Metabolism\")\n)\nrownames(gene_df) &lt;- gene_df$gene_id #name the rows by the gene IDs\ngene_df &lt;- gene_df[, -1] #remove the first column which is not needed anymore\n#gene_df\ngene_df[gene_df$expression &gt; 7 & gene_df$mutation == \"No\", ]\n\n      expression mutation    pathway\nGAPDH         10       No Metabolism\n\n\nReturns genes with high expression (&gt;7) and no mutation — potentially highly active but wild-type genes.\n\nGroup-wise Expression Summary\n\nThe given vectors are:\n\nsamples &lt;- c(\"WT\", \"KO\", \"WT\", \"KO\", \"WT\")\nexpression &lt;- c(5.2, 8.1, 4.3, 9.0, 5.7)\n\nThe solution would be:\n\ngroup_factor &lt;- factor(samples)\n\n# Mean expression\ntapply(expression, group_factor, mean) ## KO: 8.55, WT: 5.07\n\n      KO       WT \n8.550000 5.066667 \n\n# Plot\nbarplot(tapply(expression, group_factor, mean), \n        col = c(\"skyblue\", \"salmon\"),\n        ylab = \"Mean Expression\",\n        main = \"Group-wise Expression\")\n\n\n\n\n\nGene Subsetting\n\n\ngene_df[gene_df$expression &gt; 8 & \n        gene_df$pathway %in% c(\"Cell Cycle\", \"Signaling\"), ]\n\n    expression mutation    pathway\nMYC        9.5      Yes Cell Cycle\n\n\nIt filters for genes highly expressed and involved in key biological pathways.\n\nDisease Stage Visualization\n\n\nstages &lt;- c(\"Stage I\", \"Stage III\", \"Stage II\", \"Stage IV\", \"Stage I\")\ndisease_stage &lt;- factor(stages, \n                        levels = c(\"Stage I\", \"Stage II\", \"Stage III\", \"Stage IV\"), \n                        ordered = TRUE)\n\nbarplot(table(disease_stage), \n        col = \"lightgreen\", \n        main = \"Patient Count by Disease Stage\",\n        ylab = \"Count\")\n\n\n\n\nLet’s do the severity order check:\n\n# Comparison\ndisease_stage[2] &gt; disease_stage[1]  # TRUE\n\n[1] TRUE\n\n\nSo, “Stage III” is more sever than “Stage I”.\n\nOncogene Subsetting and Releveling\n\n\n# Define a small gene dataset\ngene_data &lt;- data.frame(\n  gene = c(\"TP53\", \"BRCA1\", \"MYC\", \"GAPDH\", \"EGFR\"),\n  expression = c(9.1, 7.3, 10.5, 5.2, 8.6),\n  type = factor(c(\"Tumor Suppressor\", \"Oncogene\", \"Oncogene\", \"Housekeeping\", \"Oncogene\"))\n)\n\n# Subset: Oncogene rows with expression &gt; 8\ngene_data[gene_data$type == \"Oncogene\" & gene_data$expression &gt; 8, ]\n\n  gene expression     type\n3  MYC       10.5 Oncogene\n5 EGFR        8.6 Oncogene\n\n\nLet’s relevel now, “Housekeeping” is the reference:\n\n# Relevel: make \"Housekeeping\" the reference level\ngene_data$type &lt;- relevel(gene_data$type, ref = \"Housekeeping\")\n\n# Check the new levels\nlevels(gene_data$type)\n\n[1] \"Housekeeping\"     \"Oncogene\"         \"Tumor Suppressor\"\n\n\nBut the “Housekeeping” was already a reference by default (using alphabetic ordering by R). Making something else the reference would make more sense. Our code above work, but nothing new is done.\n\nSimulated Expression by Tissue\n\n\nset.seed(42)\ngene_expr &lt;- rnorm(45, mean = 8, sd = 2)\ntissue &lt;- rep(c(\"brain\", \"liver\", \"kidney\"), each = 15)\ntissue_factor &lt;- factor(tissue, levels = c(\"liver\", \"brain\", \"kidney\"))\n\nboxplot(gene_expr ~ tissue_factor, \n        col = c(\"orange\", \"skyblue\", \"lightgreen\"), \n        main = \"Expression by Tissue\",\n        ylab = \"Expression Level\")\n\n\n\n\nLet’s calculate variability per tissue type now:\n\n# Variability\ntapply(gene_expr, tissue_factor, sd)\n\n   liver    brain   kidney \n2.713940 2.050487 1.993668 \n\n# Returns standard deviation per tissue group"
  },
  {
    "objectID": "ch/rbasics/baddata.html",
    "href": "ch/rbasics/baddata.html",
    "title": "Bad data & Outliers",
    "section": "",
    "text": "CitationBibTeX citation:@online{rasheduzzaman2025,\n  author = {Md Rasheduzzaman},\n  title = {Bad Data \\& {Outliers}},\n  date = {2025-08-03},\n  langid = {en},\n  abstract = {Cleaning data, dealing with missing data and comparing\n    results for correlation and regression before vs. after removing an\n    outlier from the data.}\n}\nFor attribution, please cite this work as:\nMd Rasheduzzaman. 2025. “Bad Data & Outliers.” August\n3, 2025."
  },
  {
    "objectID": "ch/rbasics/correlation_regression.html",
    "href": "ch/rbasics/correlation_regression.html",
    "title": "Correlation & Regression",
    "section": "",
    "text": "CitationBibTeX citation:@online{rasheduzzaman2025,\n  author = {Md Rasheduzzaman},\n  title = {Correlation \\& {Regression}},\n  date = {2025-08-03},\n  langid = {en},\n  abstract = {Correlation and simple linear regression (with and without\n    intercept).}\n}\nFor attribution, please cite this work as:\nMd Rasheduzzaman. 2025. “Correlation & Regression.”\nAugust 3, 2025."
  },
  {
    "objectID": "ch/python/basics.html",
    "href": "ch/python/basics.html",
    "title": "Python Basics",
    "section": "",
    "text": "print(\"Hello, world!\")\n\nHello, world!"
  },
  {
    "objectID": "ch/python/basics.html#intro",
    "href": "ch/python/basics.html#intro",
    "title": "Python Basics",
    "section": "",
    "text": "print(\"Hello, world!\")\n\nHello, world!"
  },
  {
    "objectID": "ch/python/basics.html#np-array",
    "href": "ch/python/basics.html#np-array",
    "title": "Python Basics",
    "section": "np array",
    "text": "np array\n\n# Your Python code\n#py_install(\"numpy\"\")\nimport numpy as np\n\narr = np.array([1, 2, 3, 4, 5])\nprint(arr * 2)\n\n[ 2  4  6  8 10]"
  },
  {
    "objectID": "ch/python/MLA.html",
    "href": "ch/python/MLA.html",
    "title": "Machine Learning",
    "section": "",
    "text": "CitationBibTeX citation:@online{rasheduzzaman2025,\n  author = {Md Rasheduzzaman},\n  title = {Machine {Learning}},\n  date = {2025-04-11},\n  langid = {en},\n  abstract = {Supervised and Unsupervised ML}\n}\nFor attribution, please cite this work as:\nMd Rasheduzzaman. 2025. “Machine Learning.” April 11, 2025."
  },
  {
    "objectID": "ch/genomics/basics.html",
    "href": "ch/genomics/basics.html",
    "title": "Basic Genomics",
    "section": "",
    "text": "CitationBibTeX citation:@online{rasheduzzaman2024,\n  author = {Md Rasheduzzaman},\n  title = {Basic {Genomics}},\n  date = {2024-08-14},\n  langid = {en},\n  abstract = {Genome, -omics, data formats, platforms}\n}\nFor attribution, please cite this work as:\nMd Rasheduzzaman. 2024. “Basic Genomics.” August 14, 2024."
  },
  {
    "objectID": "ch/blogs/en/example.html",
    "href": "ch/blogs/en/example.html",
    "title": "Place Holder",
    "section": "",
    "text": "bla bla bla…….\n← Back to all English Blogs\n\n\n\n\n\nCitationBibTeX citation:@online{rasheduzzaman2025,\n  author = {Md Rasheduzzaman},\n  title = {Place {Holder}},\n  date = {2025-08-03},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMd Rasheduzzaman. 2025. “Place Holder.” August 3, 2025."
  },
  {
    "objectID": "ch/blogs/bn/gurdian.html",
    "href": "ch/blogs/bn/gurdian.html",
    "title": "জীবজগতে অভিভাবকত্ব",
    "section": "",
    "text": "আমাদের পোষা গৃহপালিত প্রাণিদের মধ্যে পাখিরা ডিম থেকে বাচ্চা দেয়। যেমন, হাঁস, মুরগি, কবুতর, কোয়েল, টার্কি, টিয়া, ময়না, মুনিয়া ইত্যাদি। এরা বেশ কয়েক সপ্তাহ ধরে ডিমে তা’ দিয়ে বাচ্চা ফুটায়। বন্য পরিবেশে বাস করা পাখিরাও ঠিক একইভাবে ডিম থেকেই বাচ্চা ফুটায়। এরকম একটি পাখি হল এম্পেরর পেঙ্গুইন (emperor penguin)। না, এরা গৃহপালিত নয়। এদের একমাত্র বাসস্থান একেবারে পৃথিবীর এক মেরুতে—এন্টার্কটিকায়।\nপৃথিবীতে যত প্রজাতির পেঙ্গুইন আছে এরা আকারে তাদের সবার চেয়ে বড়, ওজনে সবচেয়ে ভারী। এরা মানুষের মত খাড়াভাবে হাঁটে। দূর থেকে দেখলে মনে হবে যেন কোটপরুয়া কোন ভদ্রলোক হেঁটে চলেছেন। তার ছোট ডানাজোড়ার একটা যদি একটু পাশ দিয়ে প্রসারিত করে রাখে তাহলে মনে হবে লোকটার হাতে একটা ব্রিফ কেসও আছে। কোন শীতের দেশে সে বরফ ডিঙ্গিয়ে অফিসে চলেছে। হ্যাঁ, তার অফিসের নাম দক্ষিণ মহাসাগর। সেখানে গিয়ে সে অফিসের কাজকর্ম সারে, মানে আহার খুঁজে। ক্রিল, স্কুইড, ছোট বড় বিভিন্ন মাছ খেয়ে সে পেট ভরায়। তারপর পরিবারের জন্য খাবার নিয়ে ফিরে আসে তার বাসস্থানে। কিন্তু আমরা আপাতদৃষ্টিতে হয়তো দেখব না যে সে খাবার নিয়ে ফিরেছে। আসলে খাবারটা সে নিয়ে আসে তার পাকস্থলিতে করে।\nযেসব প্রাণী সমুদ্রের গভীরে খাবার খুঁজতে যায় তাদের প্রথম চ্যালেঞ্জ হল চাপ সহ্য করা। কেননা প্রতি ১০.০৬ মিটার বা ৩৩ ফুট গভীরতায় চাপ বেড়ে দ্বিগুণ হয়ে যায়। তাই তাদের হাড়ের গঠন হয় আলাদা রকম। এদের হাড়ের সংযোগস্থলে তরুনাস্থি থাকে যা চাপ সহ্য করতে সাহায্য করে। এদের হিমোগ্লোবিনের গঠনও একটু আলাদা; যেন এতো গভীরতায় অক্সিজেন পরিবহণে কোন বাধা-বিপত্তি না ঘটে। আর এদের পালকগুলো এমন যেন পানি লাগলেই তা শরীরের সাথে লেপটে যায়।\nআমাদের গৃহপালিত পাখিগুলো হয় স্ত্রী-পুরুষ পালা করে ডিমে তা দেয়, কিংবা শুধু স্ত্রী পাখিই এ কাজ করে। সন্তান উৎপাদনের জন্য এদের সবচেয়ে বড় চ্যালেঞ্জ হল ডিমে তা’ দেয়া। স্ত্রী এম্পেরর পেঙ্গুইন একটিমাত্র ডিম পাড়ে একেবারে শীতকালে। ডিম পাড়ার পর তাতে তা’ দিয়ে ফুটানোর মত আর কোন শক্তি তার দেহে অবশিষ্ট থাকে না। তাই সে সাগরে পাড়ি জমায় খাবার খেতে। এসময় পরিবেশের তাপমাত্রা থাকে -২০ ডিগ্রি সেলসিয়াস। তাই এই ডিম থেকে বাচ্চা ফুটানো একটা বিশাল চ্যালেঞ্জ। তাহলে কি পেঙ্গুইনের বাচ্চা হবেনা? তার ধারা শেষ হয়ে যাবে? না, তখন বাকি কাজ করে পুরুষ এম্পেরর পেঙ্গুইন। ডিমকে দুই পায়ের পাতায় রেখে পেটের একটা থলিতে করে আগলে রাখে সে। তাই এসময় তারা সাধারণত যেভাবে হাঁটে সেভাবে হাঁটতেও পারে না। কিন্তু কয়দিন পরেই পরিবেশের তাপমাত্রা কমে -৬০ ডিগ্রি সেলসিয়াসে নেমে যায়। তখন কি তার শরীরের তাপমাত্রা পারবে ডিমকে ফুটানোর মত তাপ যোগাতে? আসলে কোন এম্পেরর পেঙ্গুইন বাবাই একা একা এত কম তাপমাত্রা ও প্রচন্ড বাতাসের সাথে লড়াই করে পারত না। তাই অন্য সব পুরুষ পেঙ্গুইনেরা মিলে একটা কলোনি তৈরি করে। একজন আরেকজনের সাথে গায়ে গা ঘেঁষে দাড়ায়, যেন তাপ না হারায়। আর যে পেঙ্গুইনেরা একেবারে বাহিরে পরিধির দিকে থাকে তারা শত মেইল বেগে ধেয়ে আসা ঠান্ডা বাতাস সহ্য করে। তাহলে তাদের তাপের সুষম সমাধান কিভাবে হল? আসলে তারা সবসময় গায়ে গা ঘেঁষে ঘুরতে থাকে। যারা ভিতরের দিকে ছিল তারা পরিধির দিকে আসে, আর যারা পরিধির দিকে ছিল তারা ভিতরে চলে যায়। এভাবে তারা তাপ বণ্টনের সুষ্ঠু সমাধান করে ফেলে। যেন কারো ডিম তাপের অভাবে নষ্ট না হয়। এই দীর্ঘ এক মাস তারা পৃথিবীর সবচেয়ে অন্ধকারাচ্ছন্ন ঠান্ডা আবহাওয়াকে হারিয়ে দেয় দলগত প্রচেষ্টায়।\nতারপর সূর্যের দেখা মিলবার সময় হয়। আর প্রকৃতির সাথে তাল মিলিয়ে এই সময় এদের ডিম ফুটে বাচ্চা বেরিয়ে আসে। বাচ্চাটা যেহেতু অতো বেশি ঠান্ডা সহ্য করতে পারবে না তাই এই যথোপযুক্ত সময়টা খুবই গুরুত্বপূর্ণ। আর ডিমকে তা’ দিতে শুরু করার দিন থেকেই পিতা পেঙ্গুইন একবেলার মাত্র আহার নিজের পাকস্থলিতে জমা করে রাখে অনাগত শিশুটির জন্য। বাচ্চা হওয়ার পর পরম আদরে সেটা বাচ্চার মুখে তুলে দেয়। এরপর শুধুই অপেক্ষা। মা ফিরে আসার অপেক্ষা। মা ফিরে আসে তার সঙ্গীর উদ্দেশ্যে ডাক ছেড়ে। পরস্পরের ডাক শুনে এভাবেই তারা একে অপরকে চিনে নেয়। বাবা তখন তার আদরের ছানাকে মায়ের দায়িত্বে দিয়ে দেয়। ৪ মাসের দীর্ঘ লড়াইয়ের পর বাচ্চার মুখ দেখা স্নেহবাৎসল বাবা অতোটা সহজে সন্তানকে ছাড়তে চায়না। তবু, ক্ষুধার চেয়ে কঠিন কিছু মনে হয় নেই! মাকে সন্তান লালন-পালনের দায়িত্ব দিয়ে বাবা তখন সাগরে যায় খাবার খেতে। কেননা সে দীর্ঘ ৪ মাস ধরে কোন খাবারই খায়নি! আর মা যদি ফিরে না আসে, যদি শিকারী প্রাণীর হাতে ধরা পড়ে, কিংবা যদি খাড়া ঢাল থেকে পড়ে মারা যায় তাহলে ভবিষ্যত অনিশ্চিত। বাবাকে বেঁচে থাকার জন্য যেতেই হবে সাগরে। বাচ্চাটা তখন অন্য সব মায়েদের কাছে যায় খাবারের জন্য। কিন্তু তাদের তো পরিবার আছে, তারা আর কাউকে সহ্য করেনা। আর মা যদি এসে দেখে যে তার ডিমটি ফু্টে নি কিংবা ছোট্ট ছানাটি মরে গেছে তাহলে মায়ের আর কষ্ট দেখে কে! যেসব বাচ্চার মা নাই তাদেরকে দত্তক নেয়ার জন্য তারা উঠেপড়ে লাগে, অন্য সন্তানহারা মায়েদের সাথে প্রতিযোগিতা শুরু করে দেয় তারা। এতে করে অনেক সময় বাচ্চা প্রাণ হারায় ওইসব প্রতিযোগী মায়েদের প্রচণ্ড মাতৃত্ববোধের চাপে।\nজীবজগতে সব প্রাণিদেরই বাবা-মা যেমন আছে, ঠিক তেমনি আছে সন্তানের প্রতি মা-বাবার স্নেহ-ভালবাসা, আদর-যত্ন এবং মঙ্গলচিন্তা। সন্তানেরও আত্মচিন্তার উর্ধ্বে ভালবাসা ও মঙ্গলাকাঙ্ক্ষা আছে মা-বাবা কিংবা পরিবার নিয়ে, তবে সেটা আসবে অনেক পরে। শৈশবে পিতা-মাতার প্রতি সন্তানের ভালবাসা মনে হয় সবচেয়ে আত্মকেন্দ্রিক–বেঁচে থাকার তাগিদে। আর মা-বাবা তাদের উপর সন্তানের এই নির্ভরশীলতার মধ্য দিয়ে বুনতে থাকেন সন্তানের মধ্য দিয়ে নিজেকে অমর করে রেখে যাবার সুপ্তবাসনা। শুধু যে মানুষের মধ্যেই এসব ব্যাপার আছে তা কিন্তু নয়। আবার অন্যান্য জীবে এই ব্যাপারগুলো আছে বলে মানুষের এই দিকগুলো কোনভাবে খাটো হয়েছে এমনটাও নয়। ঠিক তেমনিভাবে তাদের অভিভাবকত্ব বা সন্তান বাৎসল্যকেও ছোট করে দেখবার কোন উপায় নেই। সন্তান তো সন্তানই, মা-বাবা তো মা-বাবাই। এর কোন জাতিভেদ চলে না। এ যেন একে অপরকে জানতে পারার, বুঝতে পারার জন্য বিধাতার এক অন্তর্নিহিত নিদর্শন। আমাদের বুঝতে হবে, খুঁজে নিতে হবে সেগুলো।।\n← Back to all Bangla Blogs\n\n\n\n\n\nCitationBibTeX citation:@online{rasheduzzaman2025,\n  author = {Md Rasheduzzaman},\n  title = {জীবজগতে অভিভাবকত্ব},\n  date = {2025-08-03},\n  langid = {bn}\n}\nFor attribution, please cite this work as:\nMd Rasheduzzaman. 2025. “জীবজগতে অভিভাবকত্ব.” August 3,\n2025."
  },
  {
    "objectID": "ch/blogs/bn/index.html",
    "href": "ch/blogs/bn/index.html",
    "title": "বাংলা ব্লগ",
    "section": "",
    "text": "স্বাগতম বাংলা ব্লগ আর্কাইভে! ✍️📚\n\n\n\n\n\n\n\n\n\n\n\n\nএক বিধবার গল্প\n\n\n\n\n\n\n\n\n\n\n\n\nAug 3, 2025\n\n\nMd Rasheduzzaman\n\n\n\n\n\n\n\n\nজীবজগতে অভিভাবকত্ব\n\n\n\n\n\n\n\n\n\n\n\n\nAug 3, 2025\n\n\nMd Rasheduzzaman\n\n\n\n\n\n\nNo matching items\n\nCitationBibTeX citation:@online{rasheduzzaman2025,\n  author = {Md Rasheduzzaman},\n  title = {বাংলা {ব্লগ}},\n  date = {2025-09-15},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMd Rasheduzzaman. 2025. “বাংলা ব্লগ.” September 15, 2025."
  },
  {
    "objectID": "ch/blogs/bn/roshun.html",
    "href": "ch/blogs/bn/roshun.html",
    "title": "এক বিধবার গল্প",
    "section": "",
    "text": "নাম তার Alium sativum। খালাত বোন Alium cepa-র সাথে তার অনেক মিল। তাই তারা একই ফ্যামিলিতে আছে। অবশ্য অমিলও আছে অনেক। Alium sativum এর গায়ে সেই আজন্ম বাল্যবৈধব্যের সাজ, আর Alium cepa-র লাল টুকটুকে বিয়ের সাজ আজও ঘোচে নি।\nআজ থেকে প্রায় ২-৩ বছর আগে, একদিন দেখি আমার ছোট বোন এক আঁটি ফুল (flower stalk) নিয়ে আসছে। মনে হল পিয়াজের। কিন্তু কাছে আসতে মনে হল— না, ওগুলো রসুনের হবে। রসুন রসুন গন্ধ বেরুচ্ছে। তবু জিজ্ঞাসা করলাম, “ওগুলো কি?” সে বলল, “রসুনের ঢেঁপ।” হাতে নিয়ে অবাক হয়ে দেখতে লাগলাম। জিবনের এতোগুলো বছর পাড়ি দিলাম, গ্রামেই বাস, ফসলের মাঠের সাথে লুটোপুটি খেতে খেতে বড় হলাম, অথচ রসুনের ফুলই আমি দেখি নি এর আগে!!! দেখলাম কিছু পুষ্পদন্ড ( scape বা ঢেঁপ) কিছুটা প্যাঁচানো, আর কিছু সোজা। আসলে কিছুটা প্যাঁচানোগুলো ছিল বেশি কচি, আর সোজাগুলো ছিল একটু ম্যাচিউর। যখন আরো ভাল করে দেখতে লাগলাম আমার তো চোখ চড়ক গাছ হবার পালা। একি! এর umbel বা ক্যাপসুলের ভিতরে দেখি ছোট ছোট রসুনের কোয়া!!! বাহ্, মাটির নিচেও রসুন, বাতাসেও রসুন, ভাল না?? Umbel এর ভিতরের এই কোয়াগুলোকে বলে bulbils। আর মাটির নিচেরগুলোকে বলে cloves। Clove-গুলো মিলে গঠন করে bulb (বাতি)। আসলে দেখতে বাতির মত বলেই এর নাম হয়েছে bulb. এই bulb আর bulbils দিয়ে রসুনের অঙ্গজ জনন ঘটে। রসুনের umbel এর ভিতরে আবার অনেক ফুলও আছে। আসলে রসুনের inflorescence (পুষ্পবিন্যাস) এর নিচের দিকটা তৈরি করে bulbils, আর উপরের দিকটা হয়ে যায় ফুল। যাই হোক, আমি এতোদিন রসুনের এই রূপের কিছুই দেখি নি। শুধু মায়ের হাতের রান্নায় রসুনের ব্যবহার দেখেছি। জ্বর হলে ওষুধের সাথে সাথে চাল ভাজা, মরিচ, আদা, রসুনের ঝাল চাটনি খেয়ে ফোঁপাতে ফোঁপাতে জ্বর সারার আশা করেছি। রসুনের গন্ধযুক্ত নিঃশ্বাস (garlic breath) ছেড়েছি—- হরেক রকম ফুলও আমি দেখেছি, অস্বীকার করছি না— কিন্তু রসুনের ফুল আমি এর আগে দেখি নি।\nআমি যেমন রসুনের ফুলের ব্যাপারে ছিলাম লাওয়াকিফ, ঠিক তেমনি আগে ভাবা হত এর ফুল sterile (বন্ধ্যা)। কিন্তু এগুলো আসলে বন্ধ্যা নয়। ১৮৭৫ সালে Eduard Regel সর্বপ্রথম রসুনের এই অনন্য ফুলধারণ পদ্ধতির বর্ণনা দেন।\nএখন তাহলে আমরা বলতেই পারি রসুনের প্রজননের মাধ্যম কয়টি। বলুন তো কি কি? হ্যাঁ, cloves, bulbils আর বীজ। তাহলে শুধু cloves দিয়ে আমাদের দেশের কৃষকেরা চাষ করে কেন এই প্রশ্নটা আপনাদের মনে জাগতে পারে। আমারও জেগেছিল। এর সহজ উত্তর হচ্ছে বীজ ও bulbils এর কন্ট্রোলিং, আবাদ করা বেশ ঝক্কির ব্যাপার।তাছাড়া এগুলো থেকে রসুন পেতে লেগে যাবে অনেক বেশি সময়, শ্রমও লাগবে বেশি। অর্থ উপার্জন, দেশভর্তি মানুষের চাহিদা মেটানো যেখানে কৃষকদের প্রধান ও প্রথম লক্ষ্য সেখানে এমন সময় ও শ্রমসাপেক্ষ কাজ করা নিশ্চই কৃষকদের কাজ নয়। তবুও প্রশ্ন থেকেই যায় bulbils বা বীজ দিয়ে কী আবাদ করার দরকার পড়বে কখনো?\nহ্যাঁ, এর উত্তরটা অনেক important. রসুন প্রজাতীর স্বকীয়তা রক্ষা করা, জাত উন্নয়ন, বংশনাশের হাত থেকে রক্ষা করা সবকিছুর সমাধান দেবে এই উত্তরটিই।\nযদিও রসুন স্বাভাবিকভাবেই অযৌন জনন প্রক্রিয়ায় (bulb বা bulbils দিয়ে) বংশ বৃদ্ধি করে, বীজ উৎপাদন হার ও অঙ্কুরোদগম হার অনেক কম তবুও আমরা যদি বীজ পাবার জন্য চাষ করতে শুরু করি ধীরে ধীরে বীজ উৎপাদন হার বাড়তে থাকবে। অযৌন বা অঙ্গজ জনন পদ্ধতি মাতৃ উদ্ভিদের হুবহু কপি (clone) তৈরি করে। অর্থাৎ, কোনো গাছের যা বৈশিষ্ট্য, অঙ্গজ প্রজননের মাধ্যমে পাওয়া গাছেরও ঠিক একই বৈশিষ্ট্য হবে। কাজেই যদি মিউটেশন বা ভাইরাল ইনফেকশনের কারণে কোনো ক্ষতিকর বৈশিষ্ট্যের আগমন ঘটে তাহলে সেটাও যতবার এই পদ্ধতিতে আবাদ করবেন ততবারই বাহিত হবে। এতে হয়তো একটা নির্দিষ্ট এলাকায় (পরিসর অনেক বড়ও হতে পারে) রসুনের চাষই হয়তো বন্ধ হয়ে যাবে। অবশ্য umbel এর ভিতরে সুরক্ষিত থাকায় bulbils এ এই ধরণের পরিবর্তন খুব কমই হয়। তবু চান্স তো থেকেই যায়। কিন্তু বীজ দিয়ে চাষ করলে এই সম্ভাবনা আমরা এড়াতে পারি। কেননা দেহকোষের মিউটেশনের প্রভাব বীজে বাহিত হয় না। কাজেই আমরা যদি হুমকির হাত থেকে রসুনকে বাঁচাতে চাই, সহজেই জাত উন্নয়ন করতে চাই (hybridization বা crossing এর মাধ্যমে), কিংবা যদি নিদেনপক্ষে এর চমৎকার ফুলধারণ দেখে মন জুড়াতে চাই, তাহলে আসুন সেটা কীভাবে করা যায় দেখে নিই। বাগানের একখন্ড জমিতে রসুনের cloves লাগিয়ে ফেলুন। জায়গার অভাব থাকলে টবে কিংবা ছাদে মাটি দিয়ে কাজটা করে ফেলুন।\nজাত উন্নয়ন বা হাইব্রিডাইজেশনের জন্য প্যারেন্ট উদ্ভিতের অন্তত একটা নির্দিষ্ট এলাকায় খুব ভালভাবে বেঁচে থাকার সক্ষমতা (adaptability) থাকতে হবে, বীজ দিয়ে বংশবৃদ্ধি করা গাছগুলোর বেঁচে থাকতে হবে। কিন্তু রসুনের তা খুবই কম। তাই আগে রসুনের এই adaptability নিয়ে আসার জন্য বেশ কিছু প্রজন্ম ধরে বীজ দিয়ে চাষ করতে থাকতে হবে।\nরসুনের ফুল protandrous ধরণের, অর্থাৎ এর stigma (গর্ভমুন্ড) পরাগগ্রাহী (receptive) হবার আগেই পরাগরেণু (pollen grain) ম্যাচিউর হয়ে যায়। তাই এদের স্বনিষেক (self-fertilization) ঘটে না। তবে একই umbel এর অন্যান্য ফুলকে নিষিক্ত করতে পারে। মৌমাছির মত অন্যান্য কীটপতঙ্গেরা এতে সাহায্য করে ফুলে ফুলে ঘুরে মধু আহরণের চারিত্রিক বৈশিষ্ট্য দিয়ে। রসুনের গর্ভাশয়ে থাকে তিনটি seed chamber. প্রতি চ্যাম্বারে আছে দুইটি করে ডিম্বক (ovule— যা বীজে পরিণত হয়।). সুতরাং, রসুনের একটি ফুল থেকেই আমরা পেয়ে যাব ছয়টি বীজ। প্রতি umbel থেকে কয়েক শ’।\nএখন সমস্যা হচ্ছে ফুল ও bulbils পুষ্টি পাবার লড়াইয়ে নামে। প্রাকৃতিকভাবে bulbils-ই জেতে। তাই বীজ পেতে চাইলে আগে এদেরকে ছাড়ানো দরকার। ভ্যারিটিভেদে bulbils এর সংখ্যা ও প্রকৃতির তারতম্য আছে। সংখ্যা কম ও আকারে বড় হলে ছাড়ানোটা সহজ হয়। তবে সংখ্যায় বেশি ও আকারে ছোট হলে প্রথমে বাইরের দিকেরগুলো ছাড়িয়ে নিয়ে কয়েকদিন পর ভিতরের দিকেরগুলো ছাড়াতে হবে। নজর রাখতে হবে যেন ফুলের কোনো ক্ষতি না হয়। আমাদের দেশে যেগুলো চাষ হয় সেগুলোতে bulbils এর সংখ্যা কম আর আকারেও বড়। কাজেই কাজটা সহজই হবে। পানি, সার ও পরিচর্যা অব্যাহত রাখতে হবে। পরাগায়ন হবার ৪৫-৬০ দিনের মধ্যেই বীজ পরিপক্ব হয়ে যায়। বীজ পরিপক্ব হবার আগেই যদি গাছ বুড়িয়ে যায় তাহলে scape টা কেটে নিয়ে পানিতে ডুবিয়ে রাখতে হবে— পাতাবাহারের ডালকে যেভাবে বাঁচিয়ে রাখা হয়, ঠিক সেভাবেই।তাহলেই আপনি বীজ পেয়ে যাবেন। এরপর এভাবে বীজ উৎপাদনের মাধ্যমে উপর্যুপরি আবাদ করতে থাকলে একটা অভিযোজিত (well adapted) ভ্যারিটি পেয়ে যাব আমরা। এরপর ক্রসিং বা হাইব্রিডাইজেশনের বিভিন্ন টেকনিক কাজে লাগিয়ে আমরা পেয়ে যাব অনেক উন্নত জাত। আর এই উন্নত জাতগুলোরও তো bulb, bulbils থাকবেই, এদের দিয়ে আমরা প্রতিটি নতুন জাত বা ভ্যারিটিকে সংরক্ষণ করতে পারি। যৌন জননের পাশাপাশি অঙ্গজ বা অযৌন জনন থাকার এটাই এক বিশাল সুবিধা। কাজেই আসুন আমরা নতুনভাবে প্রকৃতিটাকে দেখতে শুরু করি। একবীজপত্রী ও দ্বিবীজপত্রী উদ্ভিদের অযৌন ও যৌন জনন পদ্ধতিগুলো নিয়ে ভাবতে থাকি।পদ্ধতিগুলোর পার্থক্যগুলোও ভাবতে থাকি। পথ চলতে চলতে যে গাছগুলো চোখে পড়ে সেগুলোর কান্ড, পাতা, ফুল, ফলের মিল-অমিলগুলো নিয়ে ভাবতে থাকি। তাহলেই উদ্ভিদবিদ্যা জানার উৎসাহ-উদ্দীপনা, আনন্দ দ্বিগুণ হয়ে যাবে বলে আমি মন করি।\n← Back to all Bangla Blogs\n\n\n\n\n\nCitationBibTeX citation:@online{rasheduzzaman2025,\n  author = {Md Rasheduzzaman},\n  title = {এক বিধবার গল্প},\n  date = {2025-08-03},\n  langid = {bn}\n}\nFor attribution, please cite this work as:\nMd Rasheduzzaman. 2025. “এক বিধবার গল্প.” August 3, 2025."
  },
  {
    "objectID": "ch/blogs/en/index.html",
    "href": "ch/blogs/en/index.html",
    "title": "English Blogs",
    "section": "",
    "text": "Welcome to the English blog archive! ✍️📚\n\n\n\n\n\n\n\n\n\n\n\n\nPlace Holder\n\n\n\n\n\n\n\n\n\n\n\n\nAug 3, 2025\n\n\nMd Rasheduzzaman\n\n\n\n\n\n\nNo matching items\n\nCitationBibTeX citation:@online{rasheduzzaman2025,\n  author = {Md Rasheduzzaman},\n  title = {English {Blogs}},\n  date = {2025-08-03},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMd Rasheduzzaman. 2025. “English Blogs.” August 3, 2025."
  },
  {
    "objectID": "ch/genomics/advanced.html",
    "href": "ch/genomics/advanced.html",
    "title": "Advanced Genomics",
    "section": "",
    "text": "CitationBibTeX citation:@online{rasheduzzaman2024,\n  author = {Md Rasheduzzaman},\n  title = {Advanced {Genomics}},\n  date = {2024-08-14},\n  langid = {en},\n  abstract = {conda environment, HPC cluster, quality control, trimming\n    and filtering, Genome assembly, mapping, alignment, classification,\n    contamination removal, pangenomics, Illumina, ONT, PacBio}\n}\nFor attribution, please cite this work as:\nMd Rasheduzzaman. 2024. “Advanced Genomics.” August 14,\n2024."
  },
  {
    "objectID": "ch/genomics/pipeline.html",
    "href": "ch/genomics/pipeline.html",
    "title": "Bioinformatics Pipelines Using Snakemake",
    "section": "",
    "text": "Here is a nice repo to learn snakemake workflows. Just try some to learn.\n\n\n\n\n\nCitationBibTeX citation:@online{rasheduzzaman2025,\n  author = {Md Rasheduzzaman},\n  title = {Bioinformatics {Pipelines} {Using} {Snakemake}},\n  date = {2025-04-11},\n  langid = {en},\n  abstract = {Automating pipelines, snakemake, HPC cluster, etc.}\n}\nFor attribution, please cite this work as:\nMd Rasheduzzaman. 2025. “Bioinformatics Pipelines Using\nSnakemake.” April 11, 2025."
  },
  {
    "objectID": "ch/python/panda.html",
    "href": "ch/python/panda.html",
    "title": "Panda Dataframe, scypy, numpy, data typing, etc.",
    "section": "",
    "text": "Data typing in python: It is dynamic in python. But we can put some hints to help users with their input. But still, it can be problematic. See below:\n\ndef insert_patient_data(name: str, age: int):\n  print(name)\n  print(age)\n  print(\"inserted into the DB\")\n\ninsert_patient_data(\"Rashed\", \"thirty\")\n\nRashed\nthirty\ninserted into the DB\n\n\nYou see, nobody is stopping the user to put age as a string. A better way would be to keep a check on the data type using loop. If the data type doesn’t match, we will raise an error.\n\ndef insert_patient_data(name: str, age: int):\n  if type(name)==str and type(age)==int:\n    print(name)\n    print(age)\n    print(\"inserted into the DB\")\n  else:\n    raise TypeError(\"Incorrect data type\")\n\ninsert_patient_data(\"Rashed\", 30)\n\nRashed\n30\ninserted into the DB\n\n\n\ninsert_patient_data(\"Rashed\", \"thirty\")\n\nTypeError: Incorrect data type\n\n\nWe see a data type error here, so our system works to catch it.\nBut this way is not scalable. Let’s work around it.\n\ndef insert_patient_data(name: str, age: int):\n  if type(name)==str and type(age)==int:\n    print(name)\n    print(age)\n    print(\"inserted into the DB\")\n  else:\n    raise TypeError(\"Incorrect data type\")\n\ndef update_patient_data(name: str, age: int):\n  if type(name)==str and type(age)==int:\n    print(name)\n    print(age)\n    print(\"Updated\")\n  else:\n    raise TypeError(\"Incorrect data type\")\n\ninsert_patient_data(\"Rashed\", 30)\n\nRashed\n30\ninserted into the DB\n\nupdate_patient_data(\"Rashed\", 29)\n\nRashed\n29\nUpdated\n\n\nYou see the issue with scalability? How many times will we do it if we have more functions using these variables? Data validation is also very important for us for better control. In the above example, we could put -10 as age, it would pass the data type check, there is no stopping. But is it meaningful? So, we could say age can not be less than 0. How to do it?\n\ndef insert_patient_data(name: str, age: int):\n  if type(name)==str and type(age)==int:\n    if age &lt; 0:\n      raise ValueError(\"Age cannot be less than 0\")\n    else:\n      print(name)\n      print(age)\n      print(\"Inserted into the DB\")\n  else:\n    raise TypeError(\"Incorrect data type\")\n\nNow, let’s check.\n\ninsert_patient_data(\"Rashed\", 10)\n\nRashed\n10\nInserted into the DB\n\ninsert_patient_data(\"Rashed\", -10)\n\nValueError: Age cannot be less than 0\n\ninsert_patient_data(\"Rashed\", \"10\")\n\nTypeError: Incorrect data type\n\n\nHere comes Pydantic to help us checking for\n\nData type, and\nData validation\n\nAnd it does so in 3 steps:\n\n\n\nDefine a Pydantic model (class) representing the ideal schema. This includes the expected fields, their data types and any validation constraint (e.g. lt=0 for negative numbers)\n\n\n\n\n\nInstantiate the model with raw input data or make a Pydantic object (usually a dictionary or JSON-like structure)\n\n\nPydantic will automatically validate the data and coerce it into the correct Python types (if possible)\n\nIf the data doesn’t meet the model’s criteria, Pydantic raise a ValidationError.\n\n\n\n\n\nPass the validated model object to functions or use it throughout your codebase.\n\n\nThis ensures that every part of your program works with clean, type-safe, and logically valid data.\n\n\n\nLet’s use it now. But let’s make the example more realistic. We will make a dataframe with the required fields using pandas first. Then we will insert a patient info into that dataframe if the patient is new. If not, we will update information for that patient.\n\nfrom pydantic import BaseModel, ValidationError\nimport pandas as pd\n\n# ---------------------\n# 1. Define the model\n# ---------------------\nclass Patient(BaseModel):\n    name: str\n    age: int\n    weight: float\n\n# ---------------------\n# 2. In-memory database\n# ---------------------\n# Create a DataFrame to store patient records\ndb = pd.DataFrame({\n    'name': pd.Series(dtype='str'),\n    'age': pd.Series(dtype='int'),\n    'weight': pd.Series(dtype='float')\n})\n\n# ---------------------\n# 3. Insert function\n# ---------------------\ndef insert_patient_data(patient: Patient):\n    global db\n    # Check if patient already exists by name\n    if db['name'].eq(patient.name).any():\n        print(f\"Patient '{patient.name}' already exists. Use update instead.\")\n        return\n    \n    # Append new patient\n    db = pd.concat([db, pd.DataFrame([patient.model_dump()])], ignore_index=True)\n    print(f\"Inserted patient: {patient.name}\")\n\n# ---------------------\n# 4. Update function\n# ---------------------\ndef update_patient_data(patient: Patient):\n    global db\n    # Find index of the patient by name\n    idx = db.index[db['name'] == patient.name].tolist()\n    if not idx:\n        print(f\"Patient '{patient.name}' not found. Use insert instead.\")\n        return\n    \n    # Update the record\n    db.loc[idx[0], ['age', 'weight']] = patient.age, patient.weight\n    print(f\"Updated patient: {patient.name}\")\n\n# ---------------------\n# 5. Test the system\n# ---------------------\n# Initial insert\npatient_info = {'name': 'Rashed', 'age': 29, 'weight': '55'}\ntry:\n    patient1 = Patient(**patient_info) #unpacking using 2 star signs\n    insert_patient_data(patient1)\nexcept ValidationError as e:\n    print(\"Validation Error:\", e)\n\nInserted patient: Rashed\n\n# Try to insert again (should warn)\ninsert_patient_data(patient1)\n\nPatient 'Rashed' already exists. Use update instead.\n\n# Update patient\nupdated_info = {'name': 'Rashed', 'age': 30, 'weight': 57.5}\ntry:\n    patient1_updated = Patient(**updated_info)\n    update_patient_data(patient1_updated)\nexcept ValidationError as e:\n    print(\"Validation Error:\", e)\n\nUpdated patient: Rashed\n\n# Show database\nprint(\"\\nCurrent Database:\")\n\n\nCurrent Database:\n\nprint(db)\n\n     name  age  weight\n0  Rashed   30    57.5\n\n\nDid you notice something? We put 'weight': '55' and PyDantic coerced it to float smartly.\nBut we have another practical issue remaining. Names are not reliable identifier, multiple patients could have the same name. So, we need to handle it correctly using a patient id.\n\nfrom pydantic import BaseModel, ValidationError\nimport pandas as pd\n\n# ---------------------\n# 1. Patient model with manual ID\n# ---------------------\nclass Patient(BaseModel):\n    patient_id: str\n    name: str\n    age: int\n    weight: float\n\n# ---------------------\n# 2. In-memory DB\n# ---------------------\ndb = pd.DataFrame({\n    'patient_id': pd.Series(dtype='str'),\n    'name': pd.Series(dtype='str'),\n    'age': pd.Series(dtype='int'),\n    'weight': pd.Series(dtype='float')\n})\n\n# ---------------------\n# 3. Insert function\n# ---------------------\ndef insert_patient_data(patient: Patient):\n    global db\n    if db['patient_id'].eq(patient.patient_id).any():\n        print(f\"Patient ID '{patient.patient_id}' already exists. Use update instead.\")\n        return\n    new_row = pd.DataFrame([patient.model_dump()])\n    db = pd.concat([db, new_row], ignore_index=True)\n    print(f\"Inserted patient: {patient.name} with ID: {patient.patient_id}\")\n\n# ---------------------\n# 4. Update function\n# ---------------------\ndef update_patient_data(patient: Patient):\n    global db\n    idx = db.index[db['patient_id'] == patient.patient_id].tolist()\n    if not idx:\n        print(f\"Patient ID '{patient.patient_id}' not found. Use insert instead.\")\n        return\n    db.loc[idx[0], ['name', 'age', 'weight']] = patient.name, patient.age, patient.weight\n    print(f\"Updated patient: {patient.name} with ID: {patient.patient_id}\")\n\n# ---------------------\n# 5. Test it\n# ---------------------\ntry:\n    # Add 2 patients manually\n    patient1 = Patient(patient_id='P001', name='Rashed', age=29, weight=55)\n    patient2 = Patient(patient_id='P002', name='Rashed', age=40, weight=70)\n\n    insert_patient_data(patient1)\n    insert_patient_data(patient2)\n\n    # Attempt duplicate insert\n    insert_patient_data(patient1)\n\n    # Update patient1\n    patient1_updated = Patient(patient_id='P001', name='Rashed', age=30, weight=56.5)\n    update_patient_data(patient1_updated)\n\nexcept ValidationError as e:\n    print(\"Validation Error:\", e)\n\nInserted patient: Rashed with ID: P001\nInserted patient: Rashed with ID: P002\nPatient ID 'P001' already exists. Use update instead.\nUpdated patient: Rashed with ID: P001\n\n# ---------------------\n# 6. Show DB\n# ---------------------\nprint(\"\\nCurrent Database:\")\n\n\nCurrent Database:\n\nprint(db)\n\n  patient_id    name  age  weight\n0       P001  Rashed   30    56.5\n1       P002  Rashed   40    70.0\n\n\nLet’s make a bit more complex model. We are going to add more fields having more than one entry. So, pandas dataframe is not a good choice. We will use json data format instead.\n\nfrom pydantic import BaseModel, ValidationError\nfrom typing import List, Dict\nimport json\n\n# ---------------------\n# 1. Patient model\n# ---------------------\nclass Patient(BaseModel):\n    patient_id: str\n    name: str\n    age: int\n    weight: float\n    married: bool\n    allergies: List[str]\n    contact_info: Dict[str, str]\n\n# ---------------------\n# 2. In-memory \"DB\"\n# ---------------------\ndb: List[Patient] = []\n\n# ---------------------\n# 3. Insert function\n# ---------------------\ndef insert_patient_data(patient: Patient):\n    global db\n    if any(p.patient_id == patient.patient_id for p in db):\n        print(f\"Patient ID '{patient.patient_id}' already exists. Use update instead.\")\n        return\n    db.append(patient)\n    print(f\"Inserted patient: {patient.name} with ID: {patient.patient_id}\")\n\n# ---------------------\n# 4. Update function\n# ---------------------\ndef update_patient_data(patient: Patient):\n    global db\n    for idx, p in enumerate(db):\n        if p.patient_id == patient.patient_id:\n            db[idx] = patient\n            print(f\"Updated patient: {patient.name} with ID: {patient.patient_id}\")\n            return\n    print(f\"Patient ID '{patient.patient_id}' not found. Use insert instead.\")\n\n# ---------------------\n# 5. Save/Load to/from JSON\n# ---------------------\ndef save_db_to_json(filepath=\"patients.json\"):\n    with open(filepath, 'w') as f:\n        json.dump([p.model_dump() for p in db], f, indent=2)\n    print(\"Database saved to JSON.\")\n\ndef load_db_from_json(filepath=\"patients.json\"):\n    global db\n    try:\n        with open(filepath, 'r') as f:\n            data = json.load(f)\n            db = [Patient(**p) for p in data]\n        print(\"Database loaded from JSON.\")\n    except FileNotFoundError:\n        print(\"No existing database found.\")\n    except ValidationError as e:\n        print(\"Validation error while loading:\", e)\n\n# ---------------------\n# 6. Test it\n# ---------------------\ntry:\n    load_db_from_json()\n\n    patient1 = Patient(\n        patient_id='P001',\n        name='Rashed',\n        age=29,\n        weight=55,\n        married=True,\n        allergies=['Dust', 'Pollen'],\n        contact_info={'phone': '+492648973', 'email': 'abcrashed@gmail.com'}\n    )\n\n    patient2 = Patient(\n        patient_id='P002',\n        name='Rashed',\n        age=40,\n        weight=70,\n        married=True,\n        allergies=['Pollen'],\n        contact_info={'phone': '+49663882', 'email': 'rashed@gmail.com'}\n    )\n\n    insert_patient_data(patient1)\n    insert_patient_data(patient2)\n    insert_patient_data(patient1)  # Duplicate test\n\n    # Update\n    patient1_updated = Patient(\n        patient_id='P001',\n        name='Rashed',\n        age=30,\n        weight=56.7,\n        married=True,\n        allergies=['Dust', 'Pollen'],\n        contact_info={'phone': '+492648973', 'email': 'abcrashed@gmail.com'}\n    )\n    update_patient_data(patient1_updated)\n\n    save_db_to_json()\n\nexcept ValidationError as e:\n    print(\"Validation Error:\", e)\n\nDatabase loaded from JSON.\nPatient ID 'P001' already exists. Use update instead.\nPatient ID 'P002' already exists. Use update instead.\nPatient ID 'P001' already exists. Use update instead.\nUpdated patient: Rashed with ID: P001\nDatabase saved to JSON.\n\n# ---------------------\n# 7. Show database\n# ---------------------\nprint(\"\\nCurrent Database (in-memory):\")\n\n\nCurrent Database (in-memory):\n\nfor patient in db:\n    print(patient.model_dump())\n\n{'patient_id': 'P001', 'name': 'Rashed', 'age': 30, 'weight': 56.7, 'married': True, 'allergies': ['Dust', 'Pollen'], 'contact_info': {'phone': '+492648973', 'email': 'abcrashed@gmail.com'}}\n{'patient_id': 'P002', 'name': 'Rashed', 'age': 40, 'weight': 70.0, 'married': True, 'allergies': ['Pollen'], 'contact_info': {'phone': '+49663882', 'email': 'rashed@gmail.com'}}\n\n\nWhy did not we use list and dict though? Because, we could make sure that the fields are list and string, but we could not check the data types inside those list or dict. That’s why we used 2-step validation using List[str] and Dict[str, str].\nWe could make our model more flexible. For example, not every patient will have allergies, but that field is required now! Let’s work around that.\n\nIn real-world applications, not all fields are required. Let’s make our model more realistic by adding optional fields and custom validation:\n\nfrom pydantic import BaseModel, ValidationError, Field, validator\nfrom typing import List, Dict, Optional\nimport json\nfrom datetime import datetime\n\n# ---------------------\n# 1. Enhanced Patient model with optional fields and validation\n# ---------------------\nclass Patient(BaseModel):\n    patient_id: str = Field(..., min_length=4, max_length=10, description=\"Unique patient identifier\")\n    name: str = Field(..., min_length=2, max_length=50, description=\"Patient full name\")\n    age: int = Field(..., ge=0, le=150, description=\"Patient age in years\")\n    weight: float = Field(..., gt=0, le=500, description=\"Patient weight in kg\")\n    height: Optional[float] = Field(None, gt=0, le=300, description=\"Patient height in cm\")\n    married: bool = False  # Default value\n    allergies: Optional[List[str]] = Field(default=[], description=\"List of known allergies\")\n    contact_info: Dict[str, str] = Field(default_factory=dict, description=\"Contact information\")\n    emergency_contact: Optional[Dict[str, str]] = None\n    blood_type: Optional[str] = Field(None, regex=r'^(A|B|AB|O)[+-]\n, description=\"Blood type (e.g., A+, O-, AB+)\")\n    \n    # Custom validator for name formatting\n    @validator('name')\n    def name_must_not_be_empty_or_just_spaces(cls, v):\n        if not v.strip():\n            raise ValueError('Name cannot be empty or just spaces')\n        return v.strip().title()  # Capitalize properly\n    \n    # Custom validator for phone number in contact_info\n    @validator('contact_info')\n    def validate_contact_info(cls, v):\n        if 'phone' in v:\n            phone = v['phone']\n            # Simple phone validation (starts with + and has digits)\n            if not phone.startswith('+') or not phone[1:].replace('-', '').replace(' ', '').isdigit():\n                raise ValueError('Phone number must start with + and contain valid digits')\n        return v\n    \n    # Calculate BMI if height is provided\n    def calculate_bmi(self) -&gt; Optional[float]:\n        if self.height:\n            height_m = self.height / 100  # Convert cm to meters\n            return round(self.weight / (height_m ** 2), 2)\n        return None\n    \n    # Check if patient is adult\n    def is_adult(self) -&gt; bool:\n        return self.age &gt;= 18\n    \n    # Get formatted patient info\n    def get_summary(self) -&gt; str:\n        bmi = self.calculate_bmi()\n        bmi_str = f\", BMI: {bmi}\" if bmi else \"\"\n        allergies_str = f\", Allergies: {', '.join(self.allergies)}\" if self.allergies else \", No known allergies\"\n        return f\"{self.name} (ID: {self.patient_id}), Age: {self.age}, Weight: {self.weight}kg{bmi_str}{allergies_str}\"\n\n# ---------------------\n# 2. Enhanced database operations\n# ---------------------\ndb: List[Patient] = []\n\ndef insert_patient_data(patient: Patient):\n    global db\n    if any(p.patient_id == patient.patient_id for p in db):\n        print(f\"❌ Patient ID '{patient.patient_id}' already exists. Use update instead.\")\n        return False\n    db.append(patient)\n    print(f\"✅ Inserted patient: {patient.get_summary()}\")\n    return True\n\ndef update_patient_data(patient: Patient):\n    global db\n    for idx, p in enumerate(db):\n        if p.patient_id == patient.patient_id:\n            db[idx] = patient\n            print(f\"✅ Updated patient: {patient.get_summary()}\")\n            return True\n    print(f\"❌ Patient ID '{patient.patient_id}' not found. Use insert instead.\")\n    return False\n\ndef find_patient_by_id(patient_id: str) -&gt; Optional[Patient]:\n    for patient in db:\n        if patient.patient_id == patient_id:\n            return patient\n    return None\n\ndef list_all_patients():\n    if not db:\n        print(\"📭 No patients in database.\")\n        return\n    \n    print(f\"\\n👥 All Patients ({len(db)} total):\")\n    print(\"-\" * 80)\n    for patient in db:\n        print(f\"🏥 {patient.get_summary()}\")\n        if patient.blood_type:\n            print(f\"   🩸 Blood Type: {patient.blood_type}\")\n        if patient.contact_info:\n            contact_str = \", \".join([f\"{k}: {v}\" for k, v in patient.contact_info.items()])\n            print(f\"   📞 Contact: {contact_str}\")\n        print()\n\ndef get_patients_by_age_range(min_age: int, max_age: int) -&gt; List[Patient]:\n    return [p for p in db if min_age &lt;= p.age &lt;= max_age]\n\ndef get_patients_with_allergies() -&gt; List[Patient]:\n    return [p for p in db if p.allergies]\n\n# ---------------------\n# 3. Test the enhanced system\n# ---------------------\nprint(\"🏥 Testing Enhanced Patient Management System\")\nprint(\"=\" * 50)\n\ntry:\n    # Test 1: Valid patient with all fields\n    print(\"\\n🧪 Test 1: Complete patient record\")\n    patient1 = Patient(\n        patient_id='P001',\n        name='   rashed uzzaman   ',  # Will be cleaned and capitalized\n        age=29,\n        weight=65.5,\n        height=175,\n        married=True,\n        allergies=['Dust', 'Pollen', 'Cats'],\n        contact_info={'phone': '+49-123-456789', 'email': 'rashed@email.com'},\n        emergency_contact={'name': 'Jane Doe', 'phone': '+49-987-654321'},\n        blood_type='O+'\n    )\n    insert_patient_data(patient1)\n    print(f\"   BMI: {patient1.calculate_bmi()}\")\n    print(f\"   Adult: {patient1.is_adult()}\")\n    \n    # Test 2: Minimal patient record (using defaults)\n    print(\"\\n🧪 Test 2: Minimal patient record\")\n    patient2 = Patient(\n        patient_id='P002',\n        name='Alice Johnson',\n        age=35,\n        weight=58.2\n    )\n    insert_patient_data(patient2)\n    \n    # Test 3: Child patient\n    print(\"\\n🧪 Test 3: Child patient\")\n    patient3 = Patient(\n        patient_id='P003',\n        name='Bobby Smith',\n        age=12,\n        weight=40.0,\n        height=150,\n        allergies=['Peanuts'],\n        contact_info={'phone': '+49-555-123456'},\n        blood_type='A-'\n    )\n    insert_patient_data(patient3)\n    print(f\"   Adult: {patient3.is_adult()}\")\n    \n    # Test 4: Try to insert duplicate\n    print(\"\\n🧪 Test 4: Duplicate insertion attempt\")\n    insert_patient_data(patient1)\n    \n    # Test 5: Update patient\n    print(\"\\n🧪 Test 5: Update patient weight\")\n    patient1_updated = Patient(\n        patient_id='P001',\n        name='Rashed Uzzaman',\n        age=30,  # Birthday!\n        weight=67.0,  # Gained weight\n        height=175,\n        married=True,\n        allergies=['Dust', 'Pollen'],  # No longer allergic to cats!\n        contact_info={'phone': '+49-123-456789', 'email': 'rashed.new@email.com'},\n        blood_type='O+'\n    )\n    update_patient_data(patient1_updated)\n    \nexcept ValidationError as e:\n    print(f\"❌ Validation Error: {e}\")\n\n# Display all patients\nlist_all_patients()\n\n# Query examples\nprint(\"\\n🔍 Query Examples:\")\nprint(\"-\" * 30)\nadults = [p for p in db if p.is_adult()]\nprint(f\"👨‍👩‍👧‍👦 Adult patients: {len(adults)}\")\n\npatients_with_allergies = get_patients_with_allergies()\nprint(f\"🤧 Patients with allergies: {len(patients_with_allergies)}\")\nfor p in patients_with_allergies:\n    print(f\"   - {p.name}: {', '.join(p.allergies)}\")\n\nyoung_adults = get_patients_by_age_range(18, 30)\nprint(f\"🧑 Young adults (18-30): {len(young_adults)}\")\nfor p in young_adults:\n    print(f\"   - {p.name} ({p.age} years old)\")\n\nunterminated string literal (detected at line 19) (&lt;string&gt;, line 19)\n\n\n\nNow let’s see what happens when we try to insert invalid data. Pydantic will catch these errors and give us helpful messages:\n\nprint(\"\\n🚨 Testing Validation Errors\")\nprint(\"=\" * 40)\n\n# Test invalid data scenarios\ntest_cases = [\n    {\n        'name': 'Invalid Age Test',\n        'data': {'patient_id': 'P999', 'name': 'Test Patient', 'age': -5, 'weight': 70},\n        'expected_error': 'Age cannot be negative'\n    },\n    {\n        'name': 'Invalid Weight Test', \n        'data': {'patient_id': 'P998', 'name': 'Test Patient', 'age': 25, 'weight': 0},\n        'expected_error': 'Weight must be greater than 0'\n    },\n    {\n        'name': 'Invalid Blood Type Test',\n        'data': {'patient_id': 'P997', 'name': 'Test Patient', 'age': 25, 'weight': 70, 'blood_type': 'XYZ'},\n        'expected_error': 'Invalid blood type format'\n    },\n    {\n        'name': 'Invalid Phone Number Test',\n        'data': {'patient_id': 'P996', 'name': 'Test Patient', 'age': 25, 'weight': 70, 'contact_info': {'phone': 'invalid-phone'}},\n        'expected_error': 'Invalid phone number format'\n    },\n    {\n        'name': 'Empty Name Test',\n        'data': {'patient_id': 'P995', 'name': '   ', 'age': 25, 'weight': 70},\n        'expected_error': 'Name cannot be empty'\n    }\n]\n\nfor test in test_cases:\n    print(f\"\\n🧪 {test['name']}:\")\n    try:\n        invalid_patient = Patient(**test['data'])\n        print(f\"   ⚠️ Unexpectedly succeeded: {invalid_patient.name}\")\n    except ValidationError as e:\n        print(f\"   ✅ Correctly caught error: {str(e).split('\\n')[0]}\")\n    except Exception as e:\n        print(f\"   ❌ Unexpected error type: {type(e).__name__}: {e}\")\n\nf-string expression part cannot include a backslash (&lt;string&gt;, line 39)\n\n\n\nLet’s simulate reading patient data from a CSV file and using Pydantic to validate and clean it:\n\nimport csv\nfrom io import StringIO\n\n# Simulate CSV data (in real world, you'd read from a file)\ncsv_data = \"\"\"patient_id,name,age,weight,height,married,allergies,phone,email,blood_type\nP101,john doe,25,70.5,180,true,\"Dust,Pollen\",+49-111-222333,john@email.com,A+\nP102,JANE SMITH,35,65.0,,false,Peanuts,+49-444-555666,jane@email.com,O-\nP103,bob wilson,17,55.2,165,false,,+49-777-888999,bob@email.com,\nP104,invalid patient,-5,0,200,maybe,Bad Data,invalid-phone,not-an-email,XYZ\nP105,mary johnson,45,72.3,168,true,\"Shellfish,Latex\",+49-123-987654,mary@email.com,B+\n\"\"\"\n\ndef process_csv_data(csv_content: str):\n    \"\"\"Process CSV data and create Patient objects with validation\"\"\"\n    successful_patients = []\n    failed_records = []\n    \n    csv_reader = csv.DictReader(StringIO(csv_content))\n    \n    for row_num, row in enumerate(csv_reader, 1):\n        try:\n            # Clean and prepare data\n            processed_row = {\n                'patient_id': row['patient_id'].strip(),\n                'name': row['name'].strip(),\n                'age': int(row['age']),\n                'weight': float(row['weight']),\n                'married': row['married'].lower() in ['true', '1', 'yes'],\n            }\n            \n            # Handle optional fields\n            if row['height'].strip():\n                processed_row['height'] = float(row['height'])\n            \n            # Process allergies (split by comma if present)\n            if row['allergies'].strip():\n                processed_row['allergies'] = [a.strip() for a in row['allergies'].split(',')]\n            \n            # Build contact info\n            contact_info = {}\n            if row['phone'].strip():\n                contact_info['phone'] = row['phone'].strip()\n            if row['email'].strip():\n                contact_info['email'] = row['email'].strip()\n            if contact_info:\n                processed_row['contact_info'] = contact_info\n            \n            # Blood type\n            if row['blood_type'].strip():\n                processed_row['blood_type'] = row['blood_type'].strip()\n            \n            # Create Patient object (this will validate everything)\n            patient = Patient(**processed_row)\n            successful_patients.append(patient)\n            print(f\"✅ Row {row_num}: Successfully processed {patient.name}\")\n            \n        except ValidationError as e:\n            error_msg = str(e).split('\\n')[0]  # Get first error line\n            failed_records.append({'row': row_num, 'data': row, 'error': error_msg})\n            print(f\"❌ Row {row_num}: Validation failed - {error_msg}\")\n        except Exception as e:\n            failed_records.append({'row': row_num, 'data': row, 'error': str(e)})\n            print(f\"❌ Row {row_num}: Processing failed - {e}\")\n    \n    return successful_patients, failed_records\n\nprint(\"\\n📊 Processing CSV Data with Pydantic Validation\")\n\n\n📊 Processing CSV Data with Pydantic Validation\n\nprint(\"=\" * 55)\n\n=======================================================\n\nsuccessful, failed = process_csv_data(csv_data)\n\n✅ Row 1: Successfully processed john doe\n✅ Row 2: Successfully processed JANE SMITH\n❌ Row 3: Validation failed - 1 validation error for Patient\n✅ Row 4: Successfully processed invalid patient\n✅ Row 5: Successfully processed mary johnson\n\nprint(f\"\\n📈 Summary:\")\n\n\n📈 Summary:\n\nprint(f\"✅ Successfully processed: {len(successful)} patients\")\n\n✅ Successfully processed: 4 patients\n\nprint(f\"❌ Failed to process: {len(failed)} records\")\n\n❌ Failed to process: 1 records\n\nif successful:\n    print(f\"\\n👥 Successfully Imported Patients:\")\n    for patient in successful:\n        print(f\"   🏥 {patient.get_summary()}\")\n\nAttributeError: 'Patient' object has no attribute 'get_summary'\n\nif failed:\n    print(f\"\\n⚠️ Failed Records (need manual review):\")\n    for failure in failed:\n        print(f\"   Row {failure['row']}: {failure['data']['name']} - {failure['error']}\")\n\n\n⚠️ Failed Records (need manual review):\n   Row 3: bob wilson - 1 validation error for Patient\n\n\n\nPydantic can also generate JSON schemas and work seamlessly with JSON data:\n\nimport json\nfrom datetime import datetime\n\n# Generate JSON schema for our Patient model\npatient_schema = Patient.model_json_schema()\n\nprint(\"📋 Patient Model JSON Schema:\")\n\n📋 Patient Model JSON Schema:\n\nprint(\"=\" * 35)\n\n===================================\n\nprint(json.dumps(patient_schema, indent=2)[:500] + \"...\\n(truncated)\")\n\n{\n  \"properties\": {\n    \"patient_id\": {\n      \"title\": \"Patient Id\",\n      \"type\": \"string\"\n    },\n    \"name\": {\n      \"title\": \"Name\",\n      \"type\": \"string\"\n    },\n    \"age\": {\n      \"title\": \"Age\",\n      \"type\": \"integer\"\n    },\n    \"weight\": {\n      \"title\": \"Weight\",\n      \"type\": \"number\"\n    },\n    \"married\": {\n      \"title\": \"Married\",\n      \"type\": \"boolean\"\n    },\n    \"allergies\": {\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"Allergies\",\n      \"type\": \"array\"\n   ...\n(truncated)\n\n# Save all our patients to JSON with timestamp\ndef save_patients_with_metadata(filename: str = \"patients_database.json\"):\n    data = {\n        'timestamp': datetime.now().isoformat(),\n        'total_patients': len(db),\n        'schema_version': '1.0',\n        'patients': [patient.model_dump() for patient in db]\n    }\n    \n    with open(filename, 'w') as f:\n        json.dump(data, f, indent=2)\n    \n    print(f\"💾 Saved {len(db)} patients to {filename}\")\n    return filename\n\n# Load patients from JSON with validation\ndef load_patients_with_validation(filename: str = \"patients_database.json\"):\n    global db\n    try:\n        with open(filename, 'r') as f:\n            data = json.load(f)\n        \n        print(f\"📖 Loading database from {filename}\")\n        print(f\"   📅 Saved on: {data['timestamp']}\")\n        print(f\"   👥 Expected patients: {data['total_patients']}\")\n        \n        # Validate and load each patient\n        loaded_patients = []\n        for patient_data in data['patients']:\n            try:\n                patient = Patient(**patient_data)\n                loaded_patients.append(patient)\n            except ValidationError as e:\n                print(f\"   ❌ Failed to load patient {patient_data.get('name', 'Unknown')}: {e}\")\n        \n        db = loaded_patients\n        print(f\"   ✅ Successfully loaded {len(db)} patients\")\n        \n    except FileNotFoundError:\n        print(f\"❌ File {filename} not found\")\n    except json.JSONDecodeError as e:\n        print(f\"❌ Invalid JSON in {filename}: {e}\")\n    except Exception as e:\n        print(f\"❌ Error loading database: {e}\")\n\n# Save current database\nfilename = save_patients_with_metadata()\n\n💾 Saved 2 patients to patients_database.json\n\n# Clear database and reload to test\noriginal_db = db.copy()\ndb = []\nprint(f\"\\n🗑️ Cleared database (now has {len(db)} patients)\")\n\n\n🗑️ Cleared database (now has 0 patients)\n\n# Reload\nload_patients_with_validation(filename)\n\n📖 Loading database from patients_database.json\n   📅 Saved on: 2025-09-26T21:19:32.325143\n   👥 Expected patients: 2\n   ✅ Successfully loaded 2 patients\n\nprint(f\"🔄 Reloaded database (now has {len(db)} patients)\")\n\n🔄 Reloaded database (now has 2 patients)\n\n# Verify data integrity\nprint(f\"\\n🔍 Data Integrity Check:\")\n\n\n🔍 Data Integrity Check:\n\nif len(original_db) == len(db):\n    print(\"✅ Patient count matches\")\n    for orig, loaded in zip(original_db, db):\n        if orig.model_dump() == loaded.model_dump():\n            print(f\"   ✅ {orig.name} data matches perfectly\")\n        else:\n            print(f\"   ❌ {orig.name} data mismatch detected\")\nelse:\n    print(f\"❌ Patient count mismatch: original {len(original_db)}, loaded {len(db)}\")\n\n✅ Patient count matches\n   ✅ Rashed data matches perfectly\n   ✅ Rashed data matches perfectly\n\n\n\nThroughout this journey, we’ve seen how Pydantic transforms our approach to data handling:\n\n\n\n🛡️ Automatic Validation: No more manual type checking - Pydantic does it automatically\n\n🔄 Type Coercion: Smart conversion of compatible types (string “55” → float 55.0)\n\n📝 Clear Error Messages: Helpful validation errors that pinpoint exactly what’s wrong\n\n🎨 Clean Code: Models serve as documentation and enforce data contracts\n\n🔧 Flexibility: Optional fields, default values, and custom validators\n\n🌐 JSON Integration: Seamless serialization/deserialization with validation\n\n📊 Real-world Ready: Handles complex data scenarios like CSV imports\n\n\nStarted with basic type hints (limited enforcement)\nAdded manual validation (not scalable)\nIntroduced Pydantic models (automatic validation)\nEnhanced with optional fields and custom validators\nIntegrated with real data processing (CSV, JSON)\nBuilt a complete data management system\n\n\n\nAPI Development: Validate request/response data\n\nData Processing: Clean and validate CSV/JSON imports\n\nConfiguration Management: Validate application settings\n\nDatabase Models: Ensure data integrity before persistence\n\nMicroservices: Validate inter-service communication\n\nPydantic transforms unreliable, error-prone data handling into robust, self-documenting, and maintainable code. It’s not just about validation - it’s about building confidence in your data throughout your entire application! 🎉"
  },
  {
    "objectID": "ch/python/AI.html",
    "href": "ch/python/AI.html",
    "title": "Artificial Inteligence",
    "section": "",
    "text": "Let’s load pytorch library and see the version of it.\n\nimport torch\nprint(torch.__version__)\n\n2.7.0\n\n\nUse CPU if GPU (CUDA) is not available.\n\nif torch.cuda.is_available():\n    print(\"GPU is available!\")\n    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\nelse:\n    print(\"GPU not available. Using CPU.\")\n\nGPU not available. Using CPU.\n\n\nSo, I am using CPU. Let’s start making tensors and build from very basics.\n\n\n\n# using empty\na = torch.empty(2,3)\na\n\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\n\n\nLet’s check type of pur tensor.\n\n# check type\ntype(a)\n\ntorch.Tensor\n\n\n\n# using ones\ntorch.ones(3,3)\n\ntensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]])\n\n\n\n# using zeros\ntorch.zeros(3,3)\n\ntensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])\n\n\n\n# using rand\ntorch.manual_seed(40)\ntorch.rand(2,3)\n\ntensor([[0.3679, 0.8661, 0.1737],\n        [0.7157, 0.8649, 0.4878]])\n\n\n\ntorch.manual_seed(40)\ntorch.rand(2,3)\n\ntensor([[0.3679, 0.8661, 0.1737],\n        [0.7157, 0.8649, 0.4878]])\n\n\n\ntorch.randint(size=(2,3), low=0, high=10, dtype=torch.float32)\n\ntensor([[6., 3., 6.],\n        [7., 6., 5.]])\n\n\n\n# using tensor\ntorch.tensor([[3,2,1],[4,5,6]])\n\ntensor([[3, 2, 1],\n        [4, 5, 6]])\n\n\n\n# other ways\n\n# arange\na = torch.arange(0,15,3)\nprint(\"using arange -&gt;\", a)\n\n# using linspace\nb = torch.linspace(0,15,10)\nprint(\"using linspace -&gt;\", b)\n\n# using eye\nc = torch.eye(4)\nprint(\"using eye -&gt;\", c)\n\n# using full\nd = torch.full((3, 3), 5)\nprint(\"using full -&gt;\", d)\n\nusing arange -&gt; tensor([ 0,  3,  6,  9, 12])\nusing linspace -&gt; tensor([ 0.0000,  1.6667,  3.3333,  5.0000,  6.6667,  8.3333, 10.0000, 11.6667,\n        13.3333, 15.0000])\nusing eye -&gt; tensor([[1., 0., 0., 0.],\n        [0., 1., 0., 0.],\n        [0., 0., 1., 0.],\n        [0., 0., 0., 1.]])\nusing full -&gt; tensor([[5, 5, 5],\n        [5, 5, 5],\n        [5, 5, 5]])\n\n\n\n\n\nWe are making a new tensor (x) and checking shape of it. We can use the shape of x or any other already created tensor to make new tensors of that shape.\n\nx = torch.tensor([[1,2,3],[5,6,7]])\nx\n\ntensor([[1, 2, 3],\n        [5, 6, 7]])\n\n\n\nx.shape\n\ntorch.Size([2, 3])\n\n\n\ntorch.empty_like(x)\n\ntensor([[0, 0, 0],\n        [0, 0, 0]])\n\n\n\ntorch.zeros_like(x)\n\ntensor([[0, 0, 0],\n        [0, 0, 0]])\n\n\n\ntorch.rand_like(x)\n\nRuntimeError: \"check_uniform_bounds\" not implemented for 'Long'\n\n\nIt’s not working, since rand makes float values in the tensor. So, we need to specify data type as float.\n\ntorch.rand_like(x, dtype=torch.float32)\n\ntensor([[0.7583, 0.8896, 0.6959],\n        [0.4810, 0.8545, 0.1130]])\n\n\n\n\n\n\n# find data type\nx.dtype\n\ntorch.int64\n\n\nWe are changing data type from float to int using dtype here.\n\n# assign data type\ntorch.tensor([1.0,2.0,3.0], dtype=torch.int32)\n\ntensor([1, 2, 3], dtype=torch.int32)\n\n\nSimilarly, from int to float using dtype here.\n\ntorch.tensor([1,2,3], dtype=torch.float64)\n\ntensor([1., 2., 3.], dtype=torch.float64)\n\n\n\n#using to()\nx.to(torch.float32)\n\ntensor([[1., 2., 3.],\n        [5., 6., 7.]])\n\n\nSome common data types in torch:\n\n\n\n\n\n\n\n\nData Type\nDtype\nDescription\n\n\n\n\n32-bit Floating Point\ntorch.float32\nStandard floating-point type used for most deep learning tasks. Provides a balance between precision and memory usage.\n\n\n64-bit Floating Point\ntorch.float64\nDouble-precision floating point. Useful for high-precision numerical tasks but uses more memory.\n\n\n16-bit Floating Point\ntorch.float16\nHalf-precision floating point. Commonly used in mixed-precision training to reduce memory and computational overhead on modern GPUs.\n\n\nBFloat16\ntorch.bfloat16\nBrain floating-point format with reduced precision compared to float16. Used in mixed-precision training, especially on TPUs.\n\n\n8-bit Floating Point\ntorch.float8\nUltra-low-precision floating point. Used for experimental applications and extreme memory-constrained environments (less common).\n\n\n8-bit Integer\ntorch.int8\n8-bit signed integer. Used for quantized models to save memory and computation in inference.\n\n\n16-bit Integer\ntorch.int16\n16-bit signed integer. Useful for special numerical tasks requiring intermediate precision.\n\n\n32-bit Integer\ntorch.int32\nStandard signed integer type. Commonly used for indexing and general-purpose numerical tasks.\n\n\n64-bit Integer\ntorch.int64\nLong integer type. Often used for large indexing arrays or for tasks involving large numbers.\n\n\n8-bit Unsigned Integer\ntorch.uint8\n8-bit unsigned integer. Commonly used for image data (e.g., pixel values between 0 and 255).\n\n\nBoolean\ntorch.bool\nBoolean type, stores True or False values. Often used for masks in logical operations.\n\n\nComplex 64\ntorch.complex64\nComplex number type with 32-bit real and 32-bit imaginary parts. Used for scientific and signal processing tasks.\n\n\nComplex 128\ntorch.complex128\nComplex number type with 64-bit real and 64-bit imaginary parts. Offers higher precision but uses more memory.\n\n\nQuantized Integer\ntorch.qint8\nQuantized signed 8-bit integer. Used in quantized models for efficient inference.\n\n\nQuantized Unsigned Integer\ntorch.quint8\nQuantized unsigned 8-bit integer. Often used for quantized tensors in image-related tasks.\n\n\n\n\n\n\n\n\nLet’s define a tensor x first.\n\nx = torch.rand(2, 3)\nx\n\ntensor([[0.6779, 0.0173, 0.1203],\n        [0.1363, 0.8089, 0.8229]])\n\n\nNow, let’s see some scalar operation on this tensor.\n\n#addition\nx + 2\n#subtraction\nx - 3\n#multiplication\nx*4\n#division\nx/2\n#integer division\n(x*40)//3\n#modulus division\n((x*40)//3)%2\n#power\nx**2\n\ntensor([[4.5950e-01, 2.9987e-04, 1.4484e-02],\n        [1.8587e-02, 6.5435e-01, 6.7723e-01]])\n\n\n\n\n\nLet’s make 2 new tensors first. To do anything element-wise, the shape of the tensors should be the same.\n\na = torch.rand(2, 3)\nb = torch.rand(2, 3)\nprint(a)\nprint(b)\n\ntensor([[0.3759, 0.0295, 0.4132],\n        [0.0791, 0.0489, 0.9287]])\ntensor([[0.4924, 0.8416, 0.1756],\n        [0.5687, 0.4447, 0.0310]])\n\n\n\n#add\na + b\n#subtract\na - b\n#multiply\na*b\n#division\na/b\n#power\na**b\n#mod\na%b\n#int division\na//b\n\ntensor([[ 0.,  0.,  2.],\n        [ 0.,  0., 29.]])\n\n\nLet’s apply absolute function on a custom tensor.\n\n#abs\nc = torch.tensor([-1, 2, -3, 4, -5, -6, 7, -8])\ntorch.abs(c)\n\ntensor([1, 2, 3, 4, 5, 6, 7, 8])\n\n\nWe only have positive values, right? As expected.\nLet’s apply negative on the tensor.\n\ntorch.neg(c)\n\ntensor([ 1, -2,  3, -4,  5,  6, -7,  8])\n\n\nWe have negative signs on the previously positives, and positive signs on the previously negatives, right?\n\n#round\nd = torch.tensor([1.4, 4.4, 3.6, 3.01, 4.55, 4.9])\ntorch.round(d)\n# ceil\ntorch.ceil(d)\n# floor\ntorch.floor(d)\n\ntensor([1., 4., 3., 3., 4., 4.])\n\n\nDo you see what round, ciel, floor are doing here? It is not that difficult, try to see.\nLet’s do some clamping. So, if a value is smaller than the min value provided, that value will be equal to the min value and values bigger than the max value will be made equal to the max value. All other values in between the range will be kept as they are.\n\n# clamp\nd\ntorch.clamp(d, min=2, max=4)\n\ntensor([2.0000, 4.0000, 3.6000, 3.0100, 4.0000, 4.0000])\n\n\n\n\n\n\ne = torch.randint(size=(2,3), low=0, high=10, dtype=torch.float32)\ne\n\ntensor([[5., 1., 7.],\n        [7., 1., 5.]])\n\n\n\n# sum\ntorch.sum(e)\n# sum along columns\ntorch.sum(e, dim=0)\n# sum along rows\ntorch.sum(e, dim=1)\n# mean\ntorch.mean(e)\n# mean along col\ntorch.mean(e, dim=0)\n# mean along row\ntorch.mean(e, dim=1)\n# median\ntorch.median(e)\ntorch.median(e, dim=0)\ntorch.median(e, dim=1)\n\ntorch.return_types.median(\nvalues=tensor([5., 5.]),\nindices=tensor([0, 2]))\n\n\n\n# max and min\ntorch.max(e)\ntorch.max(e, dim=0)\ntorch.max(e, dim=1)\n\ntorch.min(e)\ntorch.min(e, dim=0)\ntorch.min(e, dim=1)\n\ntorch.return_types.min(\nvalues=tensor([1., 1.]),\nindices=tensor([1, 1]))\n\n\n\n# product\ntorch.prod(e)\n#do yourself dimension-wise\n\ntensor(1225.)\n\n\n\n# standard deviation\ntorch.std(e)\n#do yourself dimension-wise\n\ntensor(2.7325)\n\n\n\n# variance\ntorch.var(e)\n#do yourself dimension-wise\n\ntensor(7.4667)\n\n\nWhich value is the biggest here? How to get its position/index? Use argmax.\n\n# argmax\ntorch.argmax(e)\n\ntensor(2)\n\n\nWhich value is the smallest here? How to get its position/index? Use argmin.\n\n# argmin\ntorch.argmin(e)\n\ntensor(1)\n\n\n\n\n\n\nm1 = torch.randint(size=(2,3), low=0, high=10)\nm2 = torch.randint(size=(3,2), low=0, high=10)\n\nprint(m1)\nprint(m2)\n\ntensor([[8, 9, 1],\n        [2, 4, 5]])\ntensor([[6, 5],\n        [6, 2],\n        [0, 6]])\n\n\n\n# matrix multiplcation\ntorch.matmul(m1, m2)\n\ntensor([[102,  64],\n        [ 36,  48]])\n\n\n\n\n\n\nvector1 = torch.tensor([1, 2])\nvector2 = torch.tensor([3, 4])\n\n# dot product\ntorch.dot(vector1, vector2)\n\ntensor(11)\n\n\n\n# transpose\ntorch.transpose(m2, 0, 1)\n\ntensor([[6, 6, 0],\n        [5, 2, 6]])\n\n\n\nh = torch.randint(size=(3,3), low=0, high=8, dtype=torch.float32)\nh\n\ntensor([[7., 1., 3.],\n        [3., 2., 2.],\n        [7., 2., 4.]])\n\n\n\n# determinant\ntorch.det(h)\n\ntensor(6.0000)\n\n\n\n# inverse\ntorch.inverse(h)\n\ntensor([[ 0.6667,  0.3333, -0.6667],\n        [ 0.3333,  1.1667, -0.8333],\n        [-1.3333, -1.1667,  1.8333]])\n\n\n\n\n\n\ni = torch.randint(size=(2,3), low=0, high=10)\nj = torch.randint(size=(2,3), low=0, high=10)\n\nprint(i)\nprint(j)\n\ntensor([[1, 0, 1],\n        [7, 8, 9]])\ntensor([[1, 9, 7],\n        [4, 5, 9]])\n\n\n\n# greater than\ni &gt; j\n# less than\ni &lt; j\n# equal to\ni == j\n# not equal to\ni != j\n# greater than equal to\n\n# less than equal to\n\ntensor([[False,  True,  True],\n        [ True,  True, False]])\n\n\n\n\n\n\nk = torch.randint(size=(2,3), low=0, high=10, dtype=torch.float32)\nk\n\ntensor([[5., 8., 1.],\n        [3., 4., 4.]])\n\n\n\n# log\ntorch.log(k)\n\ntensor([[1.6094, 2.0794, 0.0000],\n        [1.0986, 1.3863, 1.3863]])\n\n\n\n# exp\ntorch.exp(k)\n\ntensor([[1.4841e+02, 2.9810e+03, 2.7183e+00],\n        [2.0086e+01, 5.4598e+01, 5.4598e+01]])\n\n\n\n# sqrt\ntorch.sqrt(k)\n\ntensor([[2.2361, 2.8284, 1.0000],\n        [1.7321, 2.0000, 2.0000]])\n\n\n\nk\n# sigmoid\ntorch.sigmoid(k)\n\ntensor([[0.9933, 0.9997, 0.7311],\n        [0.9526, 0.9820, 0.9820]])\n\n\n\nk\n# softmax\ntorch.softmax(k, dim=0)\n\ntensor([[0.8808, 0.9820, 0.0474],\n        [0.1192, 0.0180, 0.9526]])\n\n\n\n# relu\ntorch.relu(k)\n\ntensor([[5., 8., 1.],\n        [3., 4., 4.]])\n\n\n\n\n\n\nm = torch.rand(2,3)\nn = torch.rand(2,3)\n\nprint(m)\nprint(n)\n\ntensor([[0.2179, 0.5475, 0.4801],\n        [0.2278, 0.7175, 0.8381]])\ntensor([[0.2569, 0.9879, 0.0779],\n        [0.3233, 0.7714, 0.9524]])\n\n\n\nm.add_(n)\nm\nn\n\ntensor([[0.2569, 0.9879, 0.0779],\n        [0.3233, 0.7714, 0.9524]])\n\n\n\ntorch.relu(m)\n\ntensor([[0.4748, 1.5353, 0.5580],\n        [0.5511, 1.4889, 1.7905]])\n\n\n\nm.relu_()\nm\n\ntensor([[0.4748, 1.5353, 0.5580],\n        [0.5511, 1.4889, 1.7905]])\n\n\nCopying a Tensor\n\na = torch.rand(2,3)\na\n\ntensor([[0.1013, 0.2033, 0.2292],\n        [0.6055, 0.3249, 0.9225]])\n\n\n\nb = a\na\nb\n\ntensor([[0.1013, 0.2033, 0.2292],\n        [0.6055, 0.3249, 0.9225]])\n\n\n\na[0][0] = 0\na\n\ntensor([[0.0000, 0.2033, 0.2292],\n        [0.6055, 0.3249, 0.9225]])\n\n\n\nb\n\ntensor([[0.0000, 0.2033, 0.2292],\n        [0.6055, 0.3249, 0.9225]])\n\n\n\nid(a)\n\n4624181456\n\n\n\nid(b)\n\n4624181456\n\n\nBetter way of making a copy\n\nb = a.clone()\na\nb\n\ntensor([[0.0000, 0.2033, 0.2292],\n        [0.6055, 0.3249, 0.9225]])\n\n\n\na[0][0] = 10\na\n\ntensor([[10.0000,  0.2033,  0.2292],\n        [ 0.6055,  0.3249,  0.9225]])\n\n\n\nb\n\ntensor([[0.0000, 0.2033, 0.2292],\n        [0.6055, 0.3249, 0.9225]])\n\n\nNow, let’s check their memory locations. They are at different locations.\n\nid(a)\nid(b)\n\n4624182608"
  },
  {
    "objectID": "ch/rbasics/firststeps.html",
    "href": "ch/rbasics/firststeps.html",
    "title": "Basic R",
    "section": "",
    "text": "Let’s do some basic calculation.\n\n5+3\n\n[1] 8\n\n3+2\n\n[1] 5\n\n3-2\n\n[1] 1\n\n3*2\n\n[1] 6\n\n3/2 #normal division\n\n[1] 1.5\n\n7 %/% 2 #integer division, only the quotient\n\n[1] 3\n\n5 %% 3 #modulus division, the remainder\n\n[1] 2\n\n(10-5)*(2+4) #use of parentheses\n\n[1] 30\n\n10-5*2+4 #Noticed BODMAS?\n\n[1] 4\n\n(10-5)*(2+4) #Noticed BODMAS\n\n[1] 30\n\n7/(1+3); 7/1+3 #multi-line codes, separated with semi-colon\n\n[1] 1.75\n\n\n[1] 10\n\n1+2; log(1); 1/10 #more multi-line codes\n\n[1] 3\n\n\n[1] 0\n\n\n[1] 0.1\n\n\n\nVariables are variable. We have freedom to name them as we wish. But make any variable name meaningful and identifiable.\n\na &lt;- 5 #assign value 5 to a \nb = 10\na\n\n[1] 5\n\nb\n\n[1] 10\n\na &lt;- a + 10\nb = b + 15\na\n\n[1] 15\n\na^2 #a squared\n\n[1] 225\n\na**2 #a squared again, in a different way.\n\n[1] 225\n\na^3 #a qubed\n\n[1] 3375\n\n\n\n\n\n\n\n\nNote\n\n\n\n&lt;- and = are used to assign values. It is not mathematical equality. b &lt;- b + 15 might make better sense than b = b + 15.\n\n\n\nDo some more practice.\n\n7/3\n\n[1] 2.333333\n\n7%/%3\n\n[1] 2\n\n7%%3\n\n[1] 1\n\n\n\nSome important functions we apply on numerical values\n\nx &lt;- 9/4\nfloor(x)\n\n[1] 2\n\nceiling(x)\n\n[1] 3\n\nround(x)\n\n[1] 2\n\nround(x, 2) #round till 2 decimal points\n\n[1] 2.25\n\n\n\nGet to know TRUE/FALSE in R.\n\na = 5\nb = 7\nc = 10\nd = 3\na == b #is a equal to b? Ans: No/FALSE\n\n[1] FALSE\n\na != b #is a not equal to b? Ans: Yes/TRUE\n\n[1] TRUE\n\na &gt; b #is a greater than b? Ans: FALSE\n\n[1] FALSE\n\na &lt; b #is a less than b? Ans: TRUE\n\n[1] TRUE\n\na &gt;= b #is a greater than or equal to b? Ans: FALSE\n\n[1] FALSE\n\na &lt;= b #is a less than or equal to b? Ans: TRUE\n\n[1] TRUE\n\na &lt; b | d &gt; b #is a less than b OR d greater than b?\n\n[1] TRUE\n\n#It's answer will be TRUE OR FALSE --&gt; So, TRUE\na &lt; b & c &gt; d #is a less than b AND a greater than b? It's answer will be TRUE AND TRUE --&gt; So, TRUE\n\n[1] TRUE\n\na &lt; b & d &gt; c #is a less than b AND a greater than b? It's answer will be TRUE AND FALSE --&gt; So, FALSE\n\n[1] FALSE\n\n\n\nBut how to know more about a function? The package/library developer have written helpful documentation for us.\n\n?log\nexample(log)\n\n\nlog&gt; log(exp(3))\n[1] 3\n\nlog&gt; log10(1e7) # = 7\n[1] 7\n\nlog&gt; x &lt;- 10^-(1+2*1:9)\n\nlog&gt; cbind(deparse.level=2, # to get nice column names\nlog+       x, log(1+x), log1p(x), exp(x)-1, expm1(x))\n          x   log(1 + x)     log1p(x)   exp(x) - 1     expm1(x)\n [1,] 1e-03 9.995003e-04 9.995003e-04 1.000500e-03 1.000500e-03\n [2,] 1e-05 9.999950e-06 9.999950e-06 1.000005e-05 1.000005e-05\n [3,] 1e-07 1.000000e-07 1.000000e-07 1.000000e-07 1.000000e-07\n [4,] 1e-09 1.000000e-09 1.000000e-09 1.000000e-09 1.000000e-09\n [5,] 1e-11 1.000000e-11 1.000000e-11 1.000000e-11 1.000000e-11\n [6,] 1e-13 9.992007e-14 1.000000e-13 9.992007e-14 1.000000e-13\n [7,] 1e-15 1.110223e-15 1.000000e-15 1.110223e-15 1.000000e-15\n [8,] 1e-17 0.000000e+00 1.000000e-17 0.000000e+00 1.000000e-17\n [9,] 1e-19 0.000000e+00 1.000000e-19 0.000000e+00 1.000000e-19\n\n?log()\n\n\nWhat is a vector? See the example and think.\n\nx &lt;- c(1, 2, 3, 4, 5) #c means concatenate\nz &lt;- 1:5 #consecutively, from 1 through 5. A short-hand notation using :\ny &lt;- c(3, 6, 9, 12, 15, 20)\nlength(x)\n\n[1] 5\n\nmode(x)\n\n[1] \"numeric\"\n\nis(x)\n\n[1] \"numeric\" \"vector\" \n\nx[1] #first entry in vector y\n\n[1] 1\n\nx[2:5] #2nd to 5th entries in vector y\n\n[1] 2 3 4 5\n\nDNA &lt;- c(\"A\", \"T\", \"G\", \"C\") #character vector. Notice the quotation marks.\ndec &lt;- c(10.0, 20.5, 30, 60, 80.9, 90, 100.7, 50, 40, 45, 48, 56, 55) #vector of floats. All numbers became floats, it's called coercion\ndec[c(1:3, 7:length(dec))] #1st to 3rd and then 7th till the end of vector `dec`. Output as a vector.\n\n [1]  10.0  20.5  30.0 100.7  50.0  40.0  45.0  48.0  56.0  55.0\n\n\n\nNotice the element-wise or index-wise mathematical operations (+, /, log2(), round(), etc.). Noticed?\n\nx &lt;- 1:10\ny &lt;- 2:11\n#x and y are of same length\nx + y\n\n [1]  3  5  7  9 11 13 15 17 19 21\n\ny / x\n\n [1] 2.000000 1.500000 1.333333 1.250000 1.200000 1.166667 1.142857 1.125000\n [9] 1.111111 1.100000\n\nlog2(x)\n\n [1] 0.000000 1.000000 1.584963 2.000000 2.321928 2.584963 2.807355 3.000000\n [9] 3.169925 3.321928\n\nround(log2(x), 1) #log2 of all the values of `x`, 1 digit after decimal to round.\n\n [1] 0.0 1.0 1.6 2.0 2.3 2.6 2.8 3.0 3.2 3.3\n\nround(log2(x), 3) #same logic\n\n [1] 0.000 1.000 1.585 2.000 2.322 2.585 2.807 3.000 3.170 3.322\n\n\n\n\n\n\n\n\nNote\n\n\n\nNested functions work inside out. Think again about round(log2(x), 1) and you will see it. At first, it is making log2 of vector x and then it is rounding the log2 values to one digit after decimal. Got it?\n\n\n\nNow, it’s time to use vectors to make data sets…..\n\nnames &lt;- c(\"Mina\", \"Raju\", \"Mithu\", \"Lali\")\ngender &lt;- c(\"Female\", \"Male\", \"Female\", \"Female\")\nage &lt;- c(15, 12, 2, 3)\nis_human &lt;- c(TRUE, TRUE, FALSE, FALSE)\ncartoon &lt;- data.frame(names, gender, age, is_human)\nwrite.table(cartoon, \"cartoon.csv\", sep = \",\", col.names = TRUE)\ndf &lt;- read.table(\"cartoon.csv\", header = TRUE, sep = \",\")\ndim(df) #`dim` means dimension. so, rows * columns\n\n[1] 4 4\n\nstr(df) #structure of `df`\n\n'data.frame':   4 obs. of  4 variables:\n $ names   : chr  \"Mina\" \"Raju\" \"Mithu\" \"Lali\"\n $ gender  : chr  \"Female\" \"Male\" \"Female\" \"Female\"\n $ age     : int  15 12 2 3\n $ is_human: logi  TRUE TRUE FALSE FALSE\n\n\nWe made the vectors first, and the used them to make the cartton data frame or table. We learned how to export the data frame using write.table function. Also, we learned to import or read back the table using read.table function. What are the sep, col.names, header arguments there? Why do we need them? Think. Try thinking of different properties of a data set.\n\n\ngene_expr &lt;- data.frame(\n  genes = c(\"TP53\", \"BRCA1\", \"MYC\", \"EGFR\", \"GAPDH\", \"CDC2\"),\n  sample1 = c(8.2, 6.1, 9.5, 7.0, 10.0, 12),\n  Sample2 = c(5.9, 3.9, 7.2, 4.8, 7.9, 9),\n  Sample3 = c(8.25, 6.15, 9.6, 7.1, 10.1, 11.9),\n  pathways = c(\"Apoptosis\", \"DNA Repair\", \"Cell Cycle\", \"Signaling\", \"Housekeeping\", \"Cell Division\")\n)\nwrite.table(gene_expr, \"gene_expr.csv\", sep = \",\", col.names = TRUE)\ngene_set &lt;- read.table(\"gene_expr.csv\", header = TRUE, sep = \",\")\n\n\n\n\n\n\n\nNote\n\n\n\nHere, we directly used the vectors as different columns while making the data frame. Did you notice that? Also, the syntax is different here. We can’t assign the vectors with the assignment operator (means we can’t use &lt;- sign. We have to use the = sign). Try using the &lt;- sign. Did you notice the column names?\n\n\n\n\nCompute the difference between this year (2025) and the year you started at the university and divide this by the difference between this year and the year you were born. Multiply this with 100 to get the percentage of your life you have spent at the university.\nMake different kinds of variables and vectors with the data types we learned together.\nWhat are the properties of a data frame?\n\nHint: Open an excel/csv/txt file you have and try to “generalize”.\n\nCan you make logical questions on the 2 small data sets we used? Try. It will help you understanding the logical operations we tried on variables. Now we are going to apply them on vectors (columns) on the data sets. For example, in the cartoon data set, we can ask/try to subset the data set filtering for females only, or for both females and age greater than 2 years.\nIf you are writing or practicing coding in R, write comment for each line on what it is doing. It will help to chunk it better into your brain.\nPush the script and/or your answers to the questions (with your solutions) to one of your GitHub repo (and send me the repo link).\n\n\nFriday, 10pm BD Time."
  },
  {
    "objectID": "ch/rbasics/firststeps.html#using-r-as-a-calculator",
    "href": "ch/rbasics/firststeps.html#using-r-as-a-calculator",
    "title": "Basic R",
    "section": "",
    "text": "Let’s do some basic calculation.\n\n5+3\n\n[1] 8\n\n3+2\n\n[1] 5\n\n3-2\n\n[1] 1\n\n3*2\n\n[1] 6\n\n3/2 #normal division\n\n[1] 1.5\n\n7 %/% 2 #integer division, only the quotient\n\n[1] 3\n\n5 %% 3 #modulus division, the remainder\n\n[1] 2\n\n(10-5)*(2+4) #use of parentheses\n\n[1] 30\n\n10-5*2+4 #Noticed BODMAS?\n\n[1] 4\n\n(10-5)*(2+4) #Noticed BODMAS\n\n[1] 30\n\n7/(1+3); 7/1+3 #multi-line codes, separated with semi-colon\n\n[1] 1.75\n\n\n[1] 10\n\n1+2; log(1); 1/10 #more multi-line codes\n\n[1] 3\n\n\n[1] 0\n\n\n[1] 0.1"
  },
  {
    "objectID": "ch/rbasics/firststeps.html#variables",
    "href": "ch/rbasics/firststeps.html#variables",
    "title": "Basic R",
    "section": "",
    "text": "Variables are variable. We have freedom to name them as we wish. But make any variable name meaningful and identifiable.\n\na &lt;- 5 #assign value 5 to a \nb = 10\na\n\n[1] 5\n\nb\n\n[1] 10\n\na &lt;- a + 10\nb = b + 15\na\n\n[1] 15\n\na^2 #a squared\n\n[1] 225\n\na**2 #a squared again, in a different way.\n\n[1] 225\n\na^3 #a qubed\n\n[1] 3375\n\n\n\n\n\n\n\n\nNote\n\n\n\n&lt;- and = are used to assign values. It is not mathematical equality. b &lt;- b + 15 might make better sense than b = b + 15.\n\n\n\nDo some more practice.\n\n7/3\n\n[1] 2.333333\n\n7%/%3\n\n[1] 2\n\n7%%3\n\n[1] 1"
  },
  {
    "objectID": "ch/rbasics/firststeps.html#rounding",
    "href": "ch/rbasics/firststeps.html#rounding",
    "title": "Basic R",
    "section": "",
    "text": "Some important functions we apply on numerical values\n\nx &lt;- 9/4\nfloor(x)\n\n[1] 2\n\nceiling(x)\n\n[1] 3\n\nround(x)\n\n[1] 2\n\nround(x, 2) #round till 2 decimal points\n\n[1] 2.25"
  },
  {
    "objectID": "ch/rbasics/firststeps.html#logical-operations",
    "href": "ch/rbasics/firststeps.html#logical-operations",
    "title": "Basic R",
    "section": "",
    "text": "Get to know TRUE/FALSE in R.\n\na = 5\nb = 7\nc = 10\nd = 3\na == b #is a equal to b? Ans: No/FALSE\n\n[1] FALSE\n\na != b #is a not equal to b? Ans: Yes/TRUE\n\n[1] TRUE\n\na &gt; b #is a greater than b? Ans: FALSE\n\n[1] FALSE\n\na &lt; b #is a less than b? Ans: TRUE\n\n[1] TRUE\n\na &gt;= b #is a greater than or equal to b? Ans: FALSE\n\n[1] FALSE\n\na &lt;= b #is a less than or equal to b? Ans: TRUE\n\n[1] TRUE\n\na &lt; b | d &gt; b #is a less than b OR d greater than b?\n\n[1] TRUE\n\n#It's answer will be TRUE OR FALSE --&gt; So, TRUE\na &lt; b & c &gt; d #is a less than b AND a greater than b? It's answer will be TRUE AND TRUE --&gt; So, TRUE\n\n[1] TRUE\n\na &lt; b & d &gt; c #is a less than b AND a greater than b? It's answer will be TRUE AND FALSE --&gt; So, FALSE\n\n[1] FALSE"
  },
  {
    "objectID": "ch/rbasics/firststeps.html#help-and-documentation",
    "href": "ch/rbasics/firststeps.html#help-and-documentation",
    "title": "Basic R",
    "section": "",
    "text": "But how to know more about a function? The package/library developer have written helpful documentation for us.\n\n?log\nexample(log)\n\n\nlog&gt; log(exp(3))\n[1] 3\n\nlog&gt; log10(1e7) # = 7\n[1] 7\n\nlog&gt; x &lt;- 10^-(1+2*1:9)\n\nlog&gt; cbind(deparse.level=2, # to get nice column names\nlog+       x, log(1+x), log1p(x), exp(x)-1, expm1(x))\n          x   log(1 + x)     log1p(x)   exp(x) - 1     expm1(x)\n [1,] 1e-03 9.995003e-04 9.995003e-04 1.000500e-03 1.000500e-03\n [2,] 1e-05 9.999950e-06 9.999950e-06 1.000005e-05 1.000005e-05\n [3,] 1e-07 1.000000e-07 1.000000e-07 1.000000e-07 1.000000e-07\n [4,] 1e-09 1.000000e-09 1.000000e-09 1.000000e-09 1.000000e-09\n [5,] 1e-11 1.000000e-11 1.000000e-11 1.000000e-11 1.000000e-11\n [6,] 1e-13 9.992007e-14 1.000000e-13 9.992007e-14 1.000000e-13\n [7,] 1e-15 1.110223e-15 1.000000e-15 1.110223e-15 1.000000e-15\n [8,] 1e-17 0.000000e+00 1.000000e-17 0.000000e+00 1.000000e-17\n [9,] 1e-19 0.000000e+00 1.000000e-19 0.000000e+00 1.000000e-19\n\n?log()"
  },
  {
    "objectID": "ch/rbasics/firststeps.html#working-with-vectors",
    "href": "ch/rbasics/firststeps.html#working-with-vectors",
    "title": "Basic R",
    "section": "",
    "text": "What is a vector? See the example and think.\n\nx &lt;- c(1, 2, 3, 4, 5) #c means concatenate\nz &lt;- 1:5 #consecutively, from 1 through 5. A short-hand notation using :\ny &lt;- c(3, 6, 9, 12, 15, 20)\nlength(x)\n\n[1] 5\n\nmode(x)\n\n[1] \"numeric\"\n\nis(x)\n\n[1] \"numeric\" \"vector\" \n\nx[1] #first entry in vector y\n\n[1] 1\n\nx[2:5] #2nd to 5th entries in vector y\n\n[1] 2 3 4 5\n\nDNA &lt;- c(\"A\", \"T\", \"G\", \"C\") #character vector. Notice the quotation marks.\ndec &lt;- c(10.0, 20.5, 30, 60, 80.9, 90, 100.7, 50, 40, 45, 48, 56, 55) #vector of floats. All numbers became floats, it's called coercion\ndec[c(1:3, 7:length(dec))] #1st to 3rd and then 7th till the end of vector `dec`. Output as a vector.\n\n [1]  10.0  20.5  30.0 100.7  50.0  40.0  45.0  48.0  56.0  55.0\n\n\n\nNotice the element-wise or index-wise mathematical operations (+, /, log2(), round(), etc.). Noticed?\n\nx &lt;- 1:10\ny &lt;- 2:11\n#x and y are of same length\nx + y\n\n [1]  3  5  7  9 11 13 15 17 19 21\n\ny / x\n\n [1] 2.000000 1.500000 1.333333 1.250000 1.200000 1.166667 1.142857 1.125000\n [9] 1.111111 1.100000\n\nlog2(x)\n\n [1] 0.000000 1.000000 1.584963 2.000000 2.321928 2.584963 2.807355 3.000000\n [9] 3.169925 3.321928\n\nround(log2(x), 1) #log2 of all the values of `x`, 1 digit after decimal to round.\n\n [1] 0.0 1.0 1.6 2.0 2.3 2.6 2.8 3.0 3.2 3.3\n\nround(log2(x), 3) #same logic\n\n [1] 0.000 1.000 1.585 2.000 2.322 2.585 2.807 3.000 3.170 3.322\n\n\n\n\n\n\n\n\nNote\n\n\n\nNested functions work inside out. Think again about round(log2(x), 1) and you will see it. At first, it is making log2 of vector x and then it is rounding the log2 values to one digit after decimal. Got it?"
  },
  {
    "objectID": "ch/rbasics/firststeps.html#data-frame",
    "href": "ch/rbasics/firststeps.html#data-frame",
    "title": "Basic R",
    "section": "",
    "text": "Now, it’s time to use vectors to make data sets…..\n\nnames &lt;- c(\"Mina\", \"Raju\", \"Mithu\", \"Lali\")\ngender &lt;- c(\"Female\", \"Male\", \"Female\", \"Female\")\nage &lt;- c(15, 12, 2, 3)\nis_human &lt;- c(TRUE, TRUE, FALSE, FALSE)\ncartoon &lt;- data.frame(names, gender, age, is_human)\nwrite.table(cartoon, \"cartoon.csv\", sep = \",\", col.names = TRUE)\ndf &lt;- read.table(\"cartoon.csv\", header = TRUE, sep = \",\")\ndim(df) #`dim` means dimension. so, rows * columns\n\n[1] 4 4\n\nstr(df) #structure of `df`\n\n'data.frame':   4 obs. of  4 variables:\n $ names   : chr  \"Mina\" \"Raju\" \"Mithu\" \"Lali\"\n $ gender  : chr  \"Female\" \"Male\" \"Female\" \"Female\"\n $ age     : int  15 12 2 3\n $ is_human: logi  TRUE TRUE FALSE FALSE\n\n\nWe made the vectors first, and the used them to make the cartton data frame or table. We learned how to export the data frame using write.table function. Also, we learned to import or read back the table using read.table function. What are the sep, col.names, header arguments there? Why do we need them? Think. Try thinking of different properties of a data set.\n\n\ngene_expr &lt;- data.frame(\n  genes = c(\"TP53\", \"BRCA1\", \"MYC\", \"EGFR\", \"GAPDH\", \"CDC2\"),\n  sample1 = c(8.2, 6.1, 9.5, 7.0, 10.0, 12),\n  Sample2 = c(5.9, 3.9, 7.2, 4.8, 7.9, 9),\n  Sample3 = c(8.25, 6.15, 9.6, 7.1, 10.1, 11.9),\n  pathways = c(\"Apoptosis\", \"DNA Repair\", \"Cell Cycle\", \"Signaling\", \"Housekeeping\", \"Cell Division\")\n)\nwrite.table(gene_expr, \"gene_expr.csv\", sep = \",\", col.names = TRUE)\ngene_set &lt;- read.table(\"gene_expr.csv\", header = TRUE, sep = \",\")\n\n\n\n\n\n\n\nNote\n\n\n\nHere, we directly used the vectors as different columns while making the data frame. Did you notice that? Also, the syntax is different here. We can’t assign the vectors with the assignment operator (means we can’t use &lt;- sign. We have to use the = sign). Try using the &lt;- sign. Did you notice the column names?"
  },
  {
    "objectID": "ch/rbasics/firststeps.html#homeworks",
    "href": "ch/rbasics/firststeps.html#homeworks",
    "title": "Basic R",
    "section": "",
    "text": "Compute the difference between this year (2025) and the year you started at the university and divide this by the difference between this year and the year you were born. Multiply this with 100 to get the percentage of your life you have spent at the university.\nMake different kinds of variables and vectors with the data types we learned together.\nWhat are the properties of a data frame?\n\nHint: Open an excel/csv/txt file you have and try to “generalize”.\n\nCan you make logical questions on the 2 small data sets we used? Try. It will help you understanding the logical operations we tried on variables. Now we are going to apply them on vectors (columns) on the data sets. For example, in the cartoon data set, we can ask/try to subset the data set filtering for females only, or for both females and age greater than 2 years.\nIf you are writing or practicing coding in R, write comment for each line on what it is doing. It will help to chunk it better into your brain.\nPush the script and/or your answers to the questions (with your solutions) to one of your GitHub repo (and send me the repo link).\n\n\nFriday, 10pm BD Time."
  },
  {
    "objectID": "ch/rbasics/firststeps.html#getting-started",
    "href": "ch/rbasics/firststeps.html#getting-started",
    "title": "Basic R",
    "section": "Getting Started",
    "text": "Getting Started\nInstallation of R Markdown\nWe will use rmarkdown to have the flexibility of writing codes like the one you are reading now. If you haven’t installed the rmarkdown package yet, you can do so with:\n\n# Install rmarkdown package\n#install.packages(\"rmarkdown\")\nlibrary(rmarkdown)\n# Other useful packages we might use\n#install.packages(\"dplyr\")    # Data manipulation\nlibrary(dplyr)\n#install.packages(\"readr\")    # Reading CSV files\nlibrary(readr)\n\nRemove the hash sign before the install.packages(\"rmarkdown\"), install.packages(\"dplyr\"), install.packages(\"readr\") if the library loading fails. That means the package is not there to be loaded. We need to download/install first.\n\n\n\n\n\n\nNote\n\n\n\nDo you remember this book by Hadley Wickham?. Try to follow it to get the hold on the basic R syntax and lexicon.\n\n\nBasic Setup for Today’s Session\n\n# Clear environment\nrm(list = ls())\n\n# Check working directory\ngetwd()\n\n# Set working directory if needed\n# setwd(\"path/to/your/directory\")  # Uncomment and modify as needed\n\nBuilding on Last HW:\n\ncartoon &lt;- data.frame(\n  names = c(\"Mina\", \"Raju\", \"Mithu\", \"Lali\"),\n  gender = c(\"Female\", \"Male\", \"Female\", \"Female\"),\n  age = c(15, 12, 2, 3),\n  is_human = c(TRUE, TRUE, FALSE, FALSE)\n)\ncartoon\n\n  names gender age is_human\n1  Mina Female  15     TRUE\n2  Raju   Male  12     TRUE\n3 Mithu Female   2    FALSE\n4  Lali Female   3    FALSE\n\ndim(cartoon)\n\n[1] 4 4\n\nstr(cartoon)\n\n'data.frame':   4 obs. of  4 variables:\n $ names   : chr  \"Mina\" \"Raju\" \"Mithu\" \"Lali\"\n $ gender  : chr  \"Female\" \"Male\" \"Female\" \"Female\"\n $ age     : num  15 12 2 3\n $ is_human: logi  TRUE TRUE FALSE FALSE\n\nlength(cartoon$names)\n\n[1] 4\n\n##subseting\ncartoon[1:2, 2:3] #row 1-2, column 2-3\n\n  gender age\n1 Female  15\n2   Male  12\n\ncartoon[c(1, 3), c(1:3)] #row 1-3, column 1-3\n\n  names gender age\n1  Mina Female  15\n3 Mithu Female   2\n\n#condition for selecting only male characters\nmale_df &lt;- cartoon[cartoon$gender == \"Male\", ]\nmale_df\n\n  names gender age is_human\n2  Raju   Male  12     TRUE\n\n#condition for selecting female characters with age more than 2 years\nfemale_age &lt;- cartoon[cartoon$gender == \"Female\" & cartoon$age &gt; 2, ]\nfemale_age\n\n  names gender age is_human\n1  Mina Female  15     TRUE\n4  Lali Female   3    FALSE\n\nsum(female_age$age) #sum of age of female_age dataset\n\n[1] 18\n\nsd(cartoon$age) #standard deviation of age of main cartoon dataset\n\n[1] 6.480741\n\nmean(cartoon$age) #mean of age of main cartoon dataset\n\n[1] 8\n\n\nCheck your colleague’s repo for the Q3.\nLogical Operators\n\n\nOperator\nMeaning\nExample\n\n\n\n==\nEqual to\nx == 5\n\n\n!=\nNot equal\nx != 5\n\n\n&lt;\nLess than\nx &lt; 5\n\n\n&gt;\nGreater than\nx &gt; 5\n\n\n&lt;=\nLess or equal\nx &lt;= 5\n\n\n&gt;=\nGreater or equal\nx &gt;= 5\n\n\n!\nNot\n!(x &lt; 5)\n\n\n|\nOR\nx &lt; 5 | x &gt; 10\n\n\n&\nAND\nx &gt; 5 & x &lt; 10\n\n\nPreamble on random variables (RV):\nRV is so fundamental of an idea to interpret and do better in any kind of data analyses. But what is it? Let’s imagine this scenario first. You got 30 mice to do an experiment to check anti-diabetic effect of a plant extract. You randomly assigned them into 3 groups. control, treat1 (meaning insulin receivers), and treat2 (meaning your plant extract receivers). Then you kept testing and measuring. You have mean glucose level of every mouse and show whether the mean value of treat1 is equal to treat2 or not. So, are you done? Not really. Be fastidious about the mice. What if you got some other 30 mice? Are they the same? Will their mean glucose level be the same? No, right. We would end up with different mean value. We call this type of quantities RV. Mean, Standard deviation, median, variance, etc. all are RVs. Do you see the logic? That’s why we put this constraint and look for p-value, confidence interval (or CI), etc. by (null) hypothesis testing and sample distribution analyses. We will get into these stuffs later. But let’s check what I meant. Also ponder about sample vs population.\nLet’s download the data first.\n\n# Download small example dataset\ndownload.file(\"https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/femaleControlsPopulation.csv\",\n              destfile = \"mice.csv\")\n\n# Load data\nmice &lt;- read.csv(\"mice.csv\")\n\nLet’s check now.\n\ncontrol &lt;- sample(mice$Bodyweight,12)\nmean(control)\n\n[1] 24.755\n\ncontrol1 &lt;- sample(mice$Bodyweight,12)\nmean(control1)\n\n[1] 23.6525\n\ncontrol2 &lt;- sample(mice$Bodyweight,12)\nmean(control2)\n\n[1] 23.23\n\n\nDo you see the difference in the mean value now?\nBasic Stuffs: Atomic Vector\n\natomic_vec &lt;- c(Human=0.5, Mouse=0.33)\n\nIt is fast, but has limited access methods.\nHow to access elements here?\n\natomic_vec[\"Human\"]\n\nHuman \n  0.5 \n\natomic_vec[\"Mouse\"]\n\nMouse \n 0.33 \n\n\nBasic Stuffs: Matrices\nMatrices are essential for biologists working with expression data, distance matrices, and other numerical data.\n\n# Create a gene expression matrix: rows=genes, columns=samples\nexpr_matrix &lt;- matrix(\n  c(12.3, 8.7, 15.2, 6.8,\n    9.5, 11.2, 13.7, 7.4,\n    5.6, 6.8, 7.9, 6.5),\n  nrow = 3, ncol = 4, byrow = TRUE\n)\n\n# Add dimension names\nrownames(expr_matrix) &lt;- c(\"BRCA1\", \"TP53\", \"GAPDH\")\ncolnames(expr_matrix) &lt;- c(\"Control_1\", \"Control_2\", \"Treatment_1\", \"Treatment_2\")\nexpr_matrix\n\n      Control_1 Control_2 Treatment_1 Treatment_2\nBRCA1      12.3       8.7        15.2         6.8\nTP53        9.5      11.2        13.7         7.4\nGAPDH       5.6       6.8         7.9         6.5\n\n# Matrix dimensions\ndim(expr_matrix)       # Returns rows and columns\n\n[1] 3 4\n\nnrow(expr_matrix)      # Number of rows\n\n[1] 3\n\nncol(expr_matrix)      # Number of columns\n\n[1] 4\n\n# Matrix subsetting\nexpr_matrix[2, ]       # One gene, all samples\n\n  Control_1   Control_2 Treatment_1 Treatment_2 \n        9.5        11.2        13.7         7.4 \n\nexpr_matrix[, 3:4]     # All genes, treatment samples only\n\n      Treatment_1 Treatment_2\nBRCA1        15.2         6.8\nTP53         13.7         7.4\nGAPDH         7.9         6.5\n\nexpr_matrix[\"TP53\", c(\"Control_1\", \"Treatment_1\")]  # Specific gene and samples\n\n  Control_1 Treatment_1 \n        9.5        13.7 \n\n# Matrix calculations (useful for bioinformatics)\n# Mean expression per gene\ngene_means &lt;- rowMeans(expr_matrix)\ngene_means\n\nBRCA1  TP53 GAPDH \n10.75 10.45  6.70 \n\n# Mean expression per sample\nsample_means &lt;- colMeans(expr_matrix)\nsample_means\n\n  Control_1   Control_2 Treatment_1 Treatment_2 \n   9.133333    8.900000   12.266667    6.900000 \n\n# Calculate fold change (Treatment vs Control)\ncontrol_means &lt;- rowMeans(expr_matrix[, 1:2])\ntreatment_means &lt;- rowMeans(expr_matrix[, 3:4])\nfold_change &lt;- treatment_means / control_means\nfold_change\n\n   BRCA1     TP53    GAPDH \n1.047619 1.019324 1.161290 \n\n# Matrix visualization\n# Heatmap of expression data\nheatmap(expr_matrix, \n        Colv = NA,         # Don't cluster columns\n        scale = \"row\",     # Scale by row (gene)\n        col = heat.colors(16),\n        main = \"Gene Expression Heatmap\")\n\n\n\n\nMore Matrix Practice:\n\n#Create a simple Gene Expression matrix (RNA-seq style)\n\nGene_Expression &lt;- matrix(c(\n  5.2, 3.1, 8.5,   # Sample 1\n  6.0, 2.8, 7.9    # Sample 2\n), nrow = 2, byrow = TRUE)\n\nrownames(Gene_Expression) &lt;- c(\"Sample_1\", \"Sample_2\")\ncolnames(Gene_Expression) &lt;- c(\"GeneA\", \"GeneB\", \"GeneC\")\n\nprint(\"Gene Expression Matrix:\")\n\n[1] \"Gene Expression Matrix:\"\n\nprint(Gene_Expression)\n\n         GeneA GeneB GeneC\nSample_1   5.2   3.1   8.5\nSample_2   6.0   2.8   7.9\n\n#1. Transpose: Genes become rows, Samples become columns\n\nGene_Expression_T &lt;- t(Gene_Expression)\nprint(\"Transpose of Gene Expression Matrix:\")\n\n[1] \"Transpose of Gene Expression Matrix:\"\n\nprint(Gene_Expression_T)\n\n      Sample_1 Sample_2\nGeneA      5.2      6.0\nGeneB      3.1      2.8\nGeneC      8.5      7.9\n\n#2. Matrix multiplication\n# Suppose each gene has an associated \"gene weight\" (e.g., biological importance)\n\nGene_Weights &lt;- matrix(c(0.8, 1.2, 1.0), nrow = 3, byrow = TRUE)\nrownames(Gene_Weights) &lt;- c(\"GeneA\", \"GeneB\", \"GeneC\")\ncolnames(Gene_Weights) &lt;- c(\"Weight\")\n\nTotal_Weighted_Expression &lt;- Gene_Expression %*% Gene_Weights\nprint(\"Total Weighted Expression per Sample:\")\n\n[1] \"Total Weighted Expression per Sample:\"\n\nprint(Total_Weighted_Expression)\n\n         Weight\nSample_1  16.38\nSample_2  16.06\n\n# 3. Matrix addition\n# Hypothetically increase expression by 1 TPM everywhere (technical adjustment)\n\nAdjusted_Expression &lt;- Gene_Expression + 1\nprint(\"Expression Matrix after adding 1 TPM:\")\n\n[1] \"Expression Matrix after adding 1 TPM:\"\n\nprint(Adjusted_Expression)\n\n         GeneA GeneB GeneC\nSample_1   6.2   4.1   9.5\nSample_2   7.0   3.8   8.9\n\n# 4. Identity matrix \nI &lt;- diag(3)\nrownames(I) &lt;- c(\"GeneA\", \"GeneB\", \"GeneC\")\ncolnames(I) &lt;- c(\"GeneA\", \"GeneB\", \"GeneC\")\n\nprint(\"Identity Matrix (for genes):\")\n\n[1] \"Identity Matrix (for genes):\"\n\nprint(I)\n\n      GeneA GeneB GeneC\nGeneA     1     0     0\nGeneB     0     1     0\nGeneC     0     0     1\n\n# Multiplying Gene Expression by Identity\nIdentity_Check &lt;- Gene_Expression %*% I\nprint(\"Gene Expression multiplied by Identity Matrix:\")\n\n[1] \"Gene Expression multiplied by Identity Matrix:\"\n\nprint(Identity_Check)\n\n         GeneA GeneB GeneC\nSample_1   5.2   3.1   8.5\nSample_2   6.0   2.8   7.9\n\n# 5. Scalar multiplication \n# Suppose you want to simulate doubling expression values\n\nDoubled_Expression &lt;- 2 * Gene_Expression\nprint(\"Doubled Gene Expression:\")\n\n[1] \"Doubled Gene Expression:\"\n\nprint(Doubled_Expression)\n\n         GeneA GeneB GeneC\nSample_1  10.4   6.2  17.0\nSample_2  12.0   5.6  15.8\n\n# 6. Summations \n\n# Total expression per sample\nTotal_Expression_Per_Sample &lt;- rowSums(Gene_Expression)\nprint(\"Total Expression per Sample:\")\n\n[1] \"Total Expression per Sample:\"\n\nprint(Total_Expression_Per_Sample)\n\nSample_1 Sample_2 \n    16.8     16.7 \n\n# Total expression per gene\nTotal_Expression_Per_Gene &lt;- colSums(Gene_Expression)\nprint(\"Total Expression per Gene:\")\n\n[1] \"Total Expression per Gene:\"\n\nprint(Total_Expression_Per_Gene)\n\nGeneA GeneB GeneC \n 11.2   5.9  16.4 \n\n# 7. Simple plots \n\n# Barplot: Total expression per sample\nbarplot(Total_Expression_Per_Sample, main=\"Total Expression per Sample\", ylab=\"TPM\", col=c(\"skyblue\", \"salmon\"))\n\n\n\n# Barplot: Total expression per gene\nbarplot(Total_Expression_Per_Gene, main=\"Total Expression per Gene\", ylab=\"TPM\", col=c(\"lightgreen\", \"orange\", \"violet\"))\n\n\n\n# Heatmap: Expression matrix\nheatmap(Gene_Expression, Rowv=NA, Colv=NA, col=heat.colors(256), scale=\"column\", main=\"Gene Expression Heatmap\")\n\n\n\n\nAnother Example: You have counts of cells in different organs for two animal species.\nYou also have a matrix with average cell sizes (micrometer, µm²) for each organ.\nYou can then multiply count × size to get total cell area for each species in each organ.\n\n# Create a matrix: Cell counts\nCell_Counts &lt;- matrix(c(500, 600, 300, 400, 700, 800), nrow = 2, byrow = TRUE)\nrownames(Cell_Counts) &lt;- c(\"Mouse\", \"Rat\")\ncolnames(Cell_Counts) &lt;- c(\"Heart\", \"Liver\", \"Brain\")\n\nprint(\"Cell Counts Matrix:\")\n\n[1] \"Cell Counts Matrix:\"\n\nprint(Cell_Counts)\n\n      Heart Liver Brain\nMouse   500   600   300\nRat     400   700   800\n\n# Create a matrix: Average cell size in µm²\nCell_Size &lt;- matrix(c(50, 200, 150), nrow = 3, byrow = TRUE)\nrownames(Cell_Size) &lt;- c(\"Heart\", \"Liver\", \"Brain\")\ncolnames(Cell_Size) &lt;- c(\"Avg_Cell_Size\")\n\nprint(\"Cell Size Matrix (µm²):\")\n\n[1] \"Cell Size Matrix (µm²):\"\n\nprint(Cell_Size)\n\n      Avg_Cell_Size\nHeart            50\nLiver           200\nBrain           150\n\n# 1. Transpose of Cell Counts\nCell_Counts_T &lt;- t(Cell_Counts)\nprint(\"Transpose of Cell Counts:\")\n\n[1] \"Transpose of Cell Counts:\"\n\nprint(Cell_Counts_T)\n\n      Mouse Rat\nHeart   500 400\nLiver   600 700\nBrain   300 800\n\n# 2. Matrix multiplication: Total cell area\n# (2x3) %*% (3x1) =&gt; (2x1)\nTotal_Cell_Area &lt;- Cell_Counts %*% Cell_Size\ncolnames(Total_Cell_Area) &lt;- \"Cell_area\"\nprint(\"Total Cell Area (Counts × Size) (µm²):\")\n\n[1] \"Total Cell Area (Counts × Size) (µm²):\"\n\nprint(Total_Cell_Area)\n\n      Cell_area\nMouse    190000\nRat      280000\n\n# 3. Matrix addition: Add 10 cells artificially to all counts (for example)\nAdded_Cells &lt;- Cell_Counts + 10\nprint(\"Cell Counts after adding 10 artificial cells:\")\n\n[1] \"Cell Counts after adding 10 artificial cells:\"\n\nprint(Added_Cells)\n\n      Heart Liver Brain\nMouse   510   610   310\nRat     410   710   810\n\n# 4. Identity matrix\nI &lt;- diag(3)\nrownames(I) &lt;- c(\"Heart\", \"Liver\", \"Brain\")\ncolnames(I) &lt;- c(\"Heart\", \"Liver\", \"Brain\")\n\nprint(\"Identity Matrix:\")\n\n[1] \"Identity Matrix:\"\n\nprint(I)\n\n      Heart Liver Brain\nHeart     1     0     0\nLiver     0     1     0\nBrain     0     0     1\n\n# 5. Multiplying Cell Counts by Identity Matrix (no real change but shows dimension rules)\nCheck_Identity &lt;- Cell_Counts %*% I\nprint(\"Cell Counts multiplied by Identity Matrix:\")\n\n[1] \"Cell Counts multiplied by Identity Matrix:\"\n\nprint(Check_Identity)\n\n      Heart Liver Brain\nMouse   500   600   300\nRat     400   700   800\n\n# 6. Scalar multiplication: double the counts (hypothetical growth)\nDouble_Cell_Counts &lt;- 2 * Cell_Counts\nprint(\"Doubled Cell Counts:\")\n\n[1] \"Doubled Cell Counts:\"\n\nprint(Double_Cell_Counts)\n\n      Heart Liver Brain\nMouse  1000  1200   600\nRat     800  1400  1600\n\n# Total number of cells per animal (row sums)\nTotal_Cells_Per_Species &lt;- rowSums(Cell_Counts)\nprint(\"Total number of cells per species:\")\n\n[1] \"Total number of cells per species:\"\n\nprint(Total_Cells_Per_Species)\n\nMouse   Rat \n 1400  1900 \n\n# Total number of cells per organ (column sums)\nTotal_Cells_Per_Organ &lt;- colSums(Cell_Counts)\nprint(\"Total number of cells per organ:\")\n\n[1] \"Total number of cells per organ:\"\n\nprint(Total_Cells_Per_Organ)\n\nHeart Liver Brain \n  900  1300  1100 \n\n# --- Simple plots ---\n\n# Bar plot of total cells per species\nbarplot(Total_Cells_Per_Species, main=\"Total Cell Counts per Species\", ylab=\"Number of Cells\", col=c(\"lightblue\", \"lightgreen\"))\n\n\n\n# Bar plot of total cells per organ\nbarplot(Total_Cells_Per_Organ, main=\"Total Cell Counts per Organ\", ylab=\"Number of Cells\", col=c(\"pink\", \"lightyellow\", \"lightgray\"))\n\n\n\n# Heatmap of the original Cell Counts matrix\nheatmap(Cell_Counts, Rowv=NA, Colv=NA, col=heat.colors(256), scale=\"column\", main=\"Heatmap of Cell Counts\")\n\n\n\n\n\n\n\n\n\n\n\nOperation\nExplanation\nR Function/Example\n\n\n\nMatrix Creation\nCreate gene expression matrix\nmatrix()\n\n\nTranspose\nFlip genes and samples\nt(Gene_Expression)\n\n\nMatrix Multiplication\nCalculate weighted sums\nGene_Expression %*% Gene_Weights\n\n\nMatrix Addition\nAdjust counts\nGene_Expression + 1\n\n\nIdentity Matrix\nSpecial neutral matrix\ndiag(3)\n\n\nScalar Multiplication\nSimulate overall increase\n2 * Gene_Expression\n\n\nRow/Column Summation\nTotal per sample/gene\n\nrowSums(), colSums()\n\n\n\nPlotting\nVisualize expression patterns\n\nbarplot(), heatmap()\n\n\n\nBasic Stuffs: List\nLists are the most flexible data structure in R - they can hold any combination of data types, including other lists! This makes them essential for biological data analysis where we often deal with mixed data types.\n\n# A list storing different types of genomic data\ngenomics_data &lt;- list(\n  gene_names = c(\"TP53\", \"BRCA1\", \"MYC\"),               # Character vector\n  expression = matrix(c(1.2, 3.4, 5.6, 7.8, 9.1, 2.3), nrow=3),    # Numeric matrix\n  is_cancer_gene = c(TRUE, TRUE, FALSE),                 # Logical vector\n  metadata = list(                                       # Nested list!\n    lab = \"CRG\",\n    date = \"2023-05-01\"\n  )\n)\n\nHow to Access Elements of a List?\n\n# Method 1: Double brackets [[ ]] for single element\ngenomics_data[[1]]  # Returns gene_names vector\n\n[1] \"TP53\"  \"BRCA1\" \"MYC\"  \n\n# Method 2: $ operator with names (when elements are named)\ngenomics_data$expression  # Returns the matrix\n\n     [,1] [,2]\n[1,]  1.2  7.8\n[2,]  3.4  9.1\n[3,]  5.6  2.3\n\n# Method 3: Single bracket [ ] returns a sublist\ngenomics_data[1:2]  # Returns list with first two elements\n\n$gene_names\n[1] \"TP53\"  \"BRCA1\" \"MYC\"  \n\n$expression\n     [,1] [,2]\n[1,]  1.2  7.8\n[2,]  3.4  9.1\n[3,]  5.6  2.3\n\n\nKey Difference from Vectors:\n\n# Compare to your prop.table() example:\natomic_vec[\"Human\"]    # Returns named numeric (vector)\n\nHuman \n  0.5 \n\natomic_vec[\"Mouse\"]\n\nMouse \n 0.33 \n\ngenomics_data[1] # Returns list containing the vector\n\n$gene_names\n[1] \"TP53\"  \"BRCA1\" \"MYC\"  \n\n\nWhy Biologists Need Lists?lm(), prcomp() functions, RNAseq analysis packages produces list. So, we need to learn how to handle lists.\nSee these examples:\nA. Storing BLAST results\n\nblast_hits &lt;- list(\n  query_id = \"GeneX\",\n  hit_ids = c(\"NP_123\", \"NP_456\"),\n  e_values = c(1e-50, 3e-12),\n  alignment = matrix(c(\"ATG...\", \"CTA...\"), ncol=1))\n\nB. Handling Mixed Data\n\npatient_data &lt;- list(\n  id = \"P1001\",\n  tests = data.frame(\n    test = c(\"WBC\", \"RBC\"),\n    value = c(4.5, 5.1)\n  ),\n  has_mutation = TRUE\n)\n\nCommon List Operations\n\n# Add new element\ngenomics_data$sequencer &lt;- \"Illumina\"\n\n# Remove element\ngenomics_data$is_cancer_gene &lt;- NULL\n\n# Check structure (critical for complex lists)\nstr(genomics_data)\n\nList of 4\n $ gene_names: chr [1:3] \"TP53\" \"BRCA1\" \"MYC\"\n $ expression: num [1:3, 1:2] 1.2 3.4 5.6 7.8 9.1 2.3\n $ metadata  :List of 2\n  ..$ lab : chr \"CRG\"\n  ..$ date: chr \"2023-05-01\"\n $ sequencer : chr \"Illumina\"\n\n\nBy the way, how would you add more patients?\n\n# Add new patient\npatient_data$P1002 &lt;- list(\n  id = \"P1002\",\n  tests = data.frame(\n    test = c(\"WBC\", \"RBC\", \"Platelets\"),\n    value = c(6.2, 4.8, 150)\n  ),\n  has_mutation = FALSE\n)\n# Access specific patient\npatient_data$P1001$test\n\nNULL\n\n\nFor Batch Processing:\n\npatients &lt;- list(\n  list(\n    id = \"P1001\",\n    tests = data.frame(test = c(\"WBC\", \"RBC\"), value = c(4.5, 5.1)),\n    has_mutation = TRUE\n  ),\n  list(\n    id = \"P1002\",\n    tests = data.frame(test = c(\"WBC\", \"RBC\", \"Platelets\"), value = c(6.2, 4.8, 150)),\n    has_mutation = FALSE\n  )\n)\n\n# Access 2nd patient's WBC value\npatients[[2]]$tests$value[patients[[2]]$tests$test == \"WBC\"]\n\n[1] 6.2\n\n\nConverting Between Structures\n\n# List → Vector\nunlist(genomics_data[1:3])\n\n  gene_names1   gene_names2   gene_names3   expression1   expression2 \n       \"TP53\"       \"BRCA1\"         \"MYC\"         \"1.2\"         \"3.4\" \n  expression3   expression4   expression5   expression6  metadata.lab \n        \"5.6\"         \"7.8\"         \"9.1\"         \"2.3\"         \"CRG\" \nmetadata.date \n \"2023-05-01\" \n\n\nVisualization\n\n# Base R plot from list data\nbarplot(unlist(genomics_data[2]),\n        names.arg = genomics_data[[1]])\n\nThis code won’t work if you run. unlist(genomics_data[2] creates a vector of length 6 from our 3*2 matrix but genomics_data[[1]] has 3 things inside the gene_names vector. Debug like this:\n\ndim(genomics_data$expression)  # e.g., 2 rows x 2 cols\n\n[1] 3 2\n\nlength(genomics_data$gene_names) # e.g., 3 genes\n\n[1] 3\n\n\nA. Gene-Centric (Mean Expression)\n\nbarplot(rowMeans(genomics_data$expression),\n        names.arg = genomics_data$gene_names,\n        col = \"steelblue\",\n        ylab = \"Mean Expression\",\n        main = \"Average Gene Expression\")\n\n\n\n\nB. Sample-Centric (All Measurements)\n\nbarplot(genomics_data$expression,\n        beside = TRUE,\n        names.arg = paste0(\"Sample_\", 1:ncol(genomics_data$expression)),\n        legend.text = genomics_data$gene_names,\n        args.legend = list(x = \"topright\", bty = \"n\"),\n        col = c(\"blue\", \"red\", \"green\"),\n        main = \"Expression Across Samples\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis matches real-world scenarios:\nRNA-seq: Rows=genes, cols=samples\nrowMeans() = average expression per gene\nbeside=TRUE =&gt; compare samples within genes\nProteomics: Rows=proteins, cols=replicates\nSame principles apply\n\n\n\n# Calculate stats\ngene_means &lt;- rowMeans(genomics_data$expression)\ngene_sds &lt;- apply(genomics_data$expression, 1, sd)\n\n# Plot with error bars\nbp &lt;- barplot(gene_means, ylim = c(0, max(gene_means + gene_sds)))\narrows(bp, gene_means - gene_sds, bp, gene_means + gene_sds, \n       angle = 90, code = 3)\n\n\n\n\nTask: Create a list containing:\n\nA character vector of 3 gene names\nA numeric matrix of expression values\nA logical vector indicating pathway membership\nA nested list with lab metadata"
  },
  {
    "objectID": "ch/rbasics/firststeps.html#homeworks-matrix-and-list-operations",
    "href": "ch/rbasics/firststeps.html#homeworks-matrix-and-list-operations",
    "title": "Basic R",
    "section": "🏡 Homeworks: Matrix and List Operations",
    "text": "🏡 Homeworks: Matrix and List Operations\n🧪 Protein Quantification in Biological Samples\nYou are given the following protein concentration matrix:\n\\[\n\\text{ProteinMatrix} =\n\\begin{bmatrix}\n  5 & 3 & 2 \\\\\n  7 & 6 & 4 \\\\\n\\end{bmatrix}\n\\]\n\nRows represent samples:\n\nSample1\nSample2\n\n\nColumns represent proteins:\n\nProteinX\nProteinY\nProteinZ\n\n\n\nYou are also given a weight (importance) matrix for the proteins:\n\\[\n\\text{WeightVector} =\n\\begin{bmatrix}\n  0.5 \\\\\n  1.0 \\\\\n  1.5 \\\\\n\\end{bmatrix}\n\\]\n🔧 Tasks\n\nMake the matrices (with exact names) and multiply the ProteinMatrix by the WeightVector.\n\nThat is:\n\\[\n  \\text{ProteinMatrix} \\times \\text{WeightVector}\n  \\]\n\nTranspose the ProteinMatrix and show what it looks like.\nCreate the Identity matrix of compatible size and show what happens when you multiply: \\[\n\\text{ProteinMatrix} \\times I\n\\]\n\n4.Do the calculations (rowSums, colSums, etc.) and visualization (barplot, heatmap) as shown in the class.\n🧠 Interpretation Questions\n\nWhat does multiplying the protein levels by the weight vector mean biologically?\nWhat does the result tell you about total protein burden (or total protein impact) for each sample?\nWhat do the identity matrix represent in the context of protein interactions or measurement biases?\nIf you changed the weight of ProteinZ to 3.0, how would the result change?\n🧬 Gene-to-Protein Translation\nYou are given the following matrix representing normalized gene expression levels (e.g., TPM): \\[\n\\text{GeneExpression} =\n\\begin{bmatrix}\n  10 &  8 &  5 \\\\\n  15 & 12 & 10 \\\\\n\\end{bmatrix}\n\\]\n\nRows = Samples:\n\nSample1\nSample2\n\n\nColumns = Genes:\n\nGeneA\nGeneB\nGeneC\n\n\n\nEach gene translates into proteins with a certain efficiency. The efficiency of translation from each gene to its corresponding protein is given by the following diagonal matrix:\n\\[\n\\text{TranslationMatrix} =\n\\begin{bmatrix}\n  1.5 & 0   & 0 \\\\\n  0   & 1.2 & 0 \\\\\n  0   & 0   & 1.8 \\\\\n\\end{bmatrix}\n\\]\nThis means:\n\nGeneA → ProteinA with 1.5× efficiency\n\nGeneB → ProteinB with 1.2× efficiency\n\nGeneC → ProteinC with 1.8× efficiency\n🔧 Tasks\n\nMake the matrices and multiply GeneExpression × TranslationMatrix to compute the resulting ProteinMatrix.\n\n\nShow the result step-by-step.\n\n\nTranspose the GeneExpression matrix. What does this new matrix represent?\nCreate the Identity matrix I₃ and multiply it with the TranslationMatrix. What happens?\nCreate a new matrix containing only the expression of GeneA and GeneB across both samples. Call this submatrix A.\n\n\nCompute the inverse A⁻¹ using solve() function.\nThen verify:\nA × A⁻¹ = I₂\n📊 Visualization Tasks\n\nPlot a MARplot-style scatter plot:\n\n\nx-axis: Gene expression values (GeneExpression matrix, flattened)\ny-axis: Corresponding Protein values (ProteinMatrix, flattened)\nLabel each point as “Sample-Gene”\n\n\nGenerate a heatmap of the ProteinMatrix using R’s heatmap() function.\n\n\nAdd meaningful row and column labels.\nEnable clustering by rows and columns.\n🧠 Interpretation Questions\n\nWhat does matrix multiplication represent biologically in this case?\nWhy does the diagonal TranslationMatrix make sense biologically?\nWhat does it mean if Sample2 has higher protein levels even with similar gene expression?\nHow does the MARplot help interpret translation efficiency?\nHow does clustering in the heatmap reveal relationships between samples and proteins?\n🐄 Animal Breeding – Economic Ranking of Bulls by Traits\nYou are evaluating two bulls for use in a dairy breeding program. Their Estimated Breeding Values (EBVs) are:\n\\[\n\\text{BullEBVs} =\n\\begin{bmatrix}\n  400 & 1.2 & 0.8 \\\\\\\\\n  500 & 1.5 & 0.6 \\\\\\\\\n\\end{bmatrix}\n\\]\n\nRows:\n\nBull1\n\nBull2\n\n\nColumns:\n\nTrait1 = Milk yield (liters/year)\n\nTrait2 = Growth rate (kg/day)\n\nTrait3 = Fertility (calving interval adjustment)\n\n\n\nYou assign economic weights to each trait:\n\\[\n\\text{EconomicWeights} =\n\\begin{bmatrix}\n  0.002 \\\\\\\\\n  50 \\\\\\\\\n  100 \\\\\\\\\n\\end{bmatrix}\n\\]\n🔧 Tasks\n\nCompute:\n\n\\[\n  \\text{TotalValue} = \\text{BullEBVs} \\times \\text{EconomicWeights}\n  \\]\n\nWhat are the resulting values?\nWhich bull is more valuable economically?\n\n\nInterpret what multiplying by the economic weights means biologically.\nCreate the 3×3 identity matrix I₃ and multiply it with BullEBVs.\n\n\nWhat does it return?\nWhat does the identity matrix mean in this case?\n\n\nSubset the BullEBVs matrix to remove Trait1 (milk yield) and recalculate TotalValue.\n\n\nHow does this change the ranking?\n📊 Visualization Tasks\n\nCreate a bar plot comparing TotalValue for Bull1 and Bull2.\nCreate a heatmap of the EBVs.\n\n\nLabel rows and columns.\nEnable clustering.\n🧠 Interpretation Questions\n\nHow do economic weights affect trait importance?\nWhy might you ignore milk yield in some breeding programs?\nWhat is the value of heatmaps in visualizing multivariate trait data?\nCan this method be extended to more bulls and more traits?\n🌾 Plant Breeding – Trait Contributions from Parental Lines\nYou are breeding a new rice variety from three parental lines. The key traits are:\n\nT1 = Drought resistance\nT2 = Yield\nT3 = Maturation time\n\nThe following trait values (normalized 1–10) have been measured:\n\\[\n\\text{ParentTraits} =\n\\begin{bmatrix}\n  7 & 5 & 3 \\\\\\\\\n  6 & 8 & 4 \\\\\\\\\n  5 & 6 & 6 \\\\\\\\\n\\end{bmatrix}\n\\]\n\nRows:\n\nP1 (Parent 1)\nP2 (Parent 2)\nP3 (Parent 3)\n\n\nColumns:\n\nT1 = Drought resistance\nT2 = Yield\nT3 = Maturation time\n\n\n\nYou design a hybrid with contributions from each parent as follows:\n\\[\n\\text{HybridWeights} =\n\\begin{bmatrix}\n  0.5 \\\\\\\\\n  0.3 \\\\\\\\\n  0.2 \\\\\\\\\n\\end{bmatrix}\n\\]\n🔧 Tasks\n\nCompute the HybridTrait vector by:\n\n\\[\n  \\text{HybridTraits} = \\text{HybridWeights}^T \\times \\text{ParentTraits}\n  \\]\nShow the steps and result.\n\nExplain what it means biologically when one parent contributes more to a particular trait.\nCreate an identity matrix I₃ and multiply it with ParentTraits.\n\n\nWhat do you observe?\nWhat does I × ParentTraits represent?\n\n\nSubset the ParentTraits matrix to include only T1 and T2. Recalculate the hybrid traits.\n\n\nDiscuss how removing a trait affects your outcome.\n📊 Visualization Tasks\n\nGenerate a heatmap of the ParentTraits matrix.\n\n\nLabel the rows with parent names and columns with trait names.\nEnable row/column clustering.\n\n\nCreate a bar plot showing the HybridTraits (T1, T2, T3).\n\n\nColor code each bar by trait.\nWhat trait contributes most?\n🧠 Interpretation Questions\n\nHow does the weighting of parents affect the hybrid’s performance?\nWhat does the identity matrix represent here?\nIf you used equal weights (⅓ for each), how would the hybrid traits change?\nWhat real-world limitations does this simplified model ignore?\n🧠 Managing Matrices and Weight Vectors Using Lists in R\nNow that you have completed four biological matrix problems — Protein concentration, gene-to-protein mapping, bull breeding value ranking, and plant trait combinations — it’s time to organize your data and weights using R’s list structure.\nIn this task, you will:\n\nGroup each example’s matrix and its corresponding weight vector inside a named list.\nCombine these named lists into a larger list called bioList.\nUse list indexing to repeat your earlier calculations and visualizations.\nReflect on the benefits and challenges of using structured data objects.\n📦 Step 1: Create a master list\nYou should now build a named list called bioList containing the following four elements:\n\nProteinConc = list(matrix = ProteinMatrix, weights = WeightVector)\nProteinMap = list(matrix = ProteinMapping, weights = TranslationWeights)\nPlant = list(matrix = ParentTraits, weights = HybridWeights)\nAnimal = list(matrix = BullEBVs, weights = EconomicWeights)\n\nHint: Each inner list should contain both:\n\nmatrix = the main data matrix\nweights = the vector used for multiplication\n\nNo R code is required here (You have them from previous part, use inside same rmd/notebook file) — just structure your data like this in your workspace.\n🔧 Tasks\n\nList the full names of each component in bioList. What are the names of the top-level and nested components?\nAccess each matrix and its corresponding weights using list indexing.\n\n\nHow would you extract only the matrix of the Plant entry?\nHow would you extract the weights for the Protein concentration entry?\n\n\nUse the correct matrix and weights to perform:\n\n\nProteinConc: Weighted gene expression score\nProteinMap: Contribution of transcripts to each protein\nPlant: Hybrid trait values\nAnimal: Bull total economic value\n\n\nSubset one matrix in each sublist (e.g., drop a trait or feature) and repeat the weighted calculation.\n\n\nWhat changes in the results?\nWhich traits/genes have the strongest influence?\n📊 Visualization Tasks\n\nGenerate one heatmap for any matrix stored in bioList.\n\n\nChoose one (e.g., ProteinMap or Plant)\nApply clustering to rows and/or columns\nLabel appropriately\n\n\nGenerate two bar plots:\n\n\nOne showing the result of weighted trait aggregation for the Plant hybrid\nOne showing the total breeding values for each bull\n🧠 Interpretation Questions\n\nHow does structuring your data using a list help with clarity and reproducibility?\nWhat risks or challenges might occur when accessing elements from nested lists?\nCould this structure be scaled for real datasets with many samples or traits?\nHow would you loop over all elements in bioList to apply the same function?\nHow can this list structure be useful for building automated bioinformatics pipelines?\n\n📝 Your Rmarkdown file(s) should include:\n\nAll matrix calculations (tasks). Also, name the rows and columns of each matrix accordingly.\nAll interpretation answers\nAll plots (output from embedded code)\nAnd your commentary blocks for each code chunk\n\nknit your rmd (or Notebook) file as html/pdf file and push both the rmd (or Notebook) and html/pdf files"
  },
  {
    "objectID": "ch/rbasics/firststeps.html#factor-variables",
    "href": "ch/rbasics/firststeps.html#factor-variables",
    "title": "Basic R",
    "section": "Factor Variables",
    "text": "Factor Variables\nImportant for categorical data\nCreating Factors\nFactors are used to represent categorical data in R. They are particularly important for biological data like genotypes, phenotypes, and experimental conditions.\n\n# Simple factor: DNA sample origins\norigins &lt;- c(\"Human\", \"Mouse\", \"Human\", \"Zebrafish\", \"Mouse\", \"Human\")\norigins_factor &lt;- factor(origins, levels = c(\"Human\", \"Zebrafish\", \"Mouse\"))\norigins_factor\n\n[1] Human     Mouse     Human     Zebrafish Mouse     Human    \nLevels: Human Zebrafish Mouse\n\n# Check levels (categories)\nlevels(origins_factor)\n\n[1] \"Human\"     \"Zebrafish\" \"Mouse\"    \n\n# Create a factor with predefined levels\ntreatment_groups &lt;- factor(c(\"Control\", \"Low_dose\", \"High_dose\", \"Control\", \"Low_dose\"),\n                         levels = c(\"Control\", \"Low_dose\", \"High_dose\"))\ntreatment_groups\n\n[1] Control   Low_dose  High_dose Control   Low_dose \nLevels: Control Low_dose High_dose\n\n# Ordered factors (important for severity, stages, etc.)\ndisease_severity &lt;- factor(c(\"Mild\", \"Severe\", \"Moderate\", \"Mild\", \"Critical\"),\n                         levels = c(\"Mild\", \"Moderate\", \"Severe\", \"Critical\"),\n                         ordered = TRUE)\ndisease_severity\n\n[1] Mild     Severe   Moderate Mild     Critical\nLevels: Mild &lt; Moderate &lt; Severe &lt; Critical\n\n# Compare with ordered factors\ndisease_severity[1] &gt; disease_severity[2]  # Is Mild less severe than Severe?\n\n[1] FALSE\n\n\nFactor Operations\n\n# Count frequencies\nout &lt;- table(origins_factor)\nout\n\norigins_factor\n    Human Zebrafish     Mouse \n        3         1         2 \n\nout[\"Human\"]\n\nHuman \n    3 \n\n# Calculate proportions\nprop.table(out)\n\norigins_factor\n    Human Zebrafish     Mouse \n0.5000000 0.1666667 0.3333333 \n\n# Change reference level (important for statistical models)\norigins_factor_relevel &lt;- relevel(origins_factor, ref = \"Mouse\")\norigins_factor_relevel\n\n[1] Human     Mouse     Human     Zebrafish Mouse     Human    \nLevels: Mouse Human Zebrafish\n\n# Convert to character\norigins_char &lt;- as.character(origins_factor)\n\n# Plot factors - Basic barplot\nbarplot(table(origins_factor), \n        col = c(\"blue\", \"green\", \"red\"),\n        main = \"Sample Origins\",\n        ylab = \"Count\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFactor level-ing and relevel-ing are different. relevel redefines what the reference should be. For example, in an experiment, you have control, treatment1, treatment2 groups. Your reference might be control. So, all of your comparisons/statistics are on the basis of control. But you might change the reference (by relevel to treatment1 and all of your comparison will be on the basis of treatment1 group. Got it?\n\n\nMore advanced plot with factors:\n\ngene_expr &lt;- c(5.2, 7.8, 4.5, 12.3, 8.1, 3.7)\nnames(gene_expr) &lt;- as.character(origins)\n\n# Boxplot by factor\nboxplot(gene_expr ~ origins, \n        col = \"lightblue\",\n        main = \"Gene Expression by Sample Origin\", \n        xlab = \"Origin\", \n        ylab = \"Expression Level\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDid you notice how factor level-ing changes the appearance of the categories in the plots? See the barplot and the boxplot again. Where are Zebrafish and Mouse now in the plots? Why are their positions on the x-axis changed?\n\n\n\n\n\n\n\n\nNote\n\n\n\nKeep noticing the output formats. Sometimes the output is just a number, sometimes a vector or table or list, etc. Check prop.table(table(origins_factor)). How is it?\n\n\n\n\n\n\n\n\nGot it?\n\n\n\n\n\nprop &lt;- prop.table(table(origins_factor)) – is a named numeric vector (atomic vector). prop$Human or similar won’t work. Check this way: prop prop[\"Human\"]; prop[\"Mouse\"]; prop[\"Zebrafish\"]\nOr make it a data frame (df) first, then try to use normal way of handling df.\n\n\n\nAccessing the Output:\n\nprop &lt;- prop.table(table(origins_factor))\nprop #What do you see? A data frame? No difference?\n\norigins_factor\n    Human Zebrafish     Mouse \n0.5000000 0.1666667 0.3333333 \n\nprop[\"Human\"]; prop[\"Mouse\"]; prop[\"Zebrafish\"]\n\nHuman \n  0.5 \n\n\n    Mouse \n0.3333333 \n\n\nZebrafish \n0.1666667"
  },
  {
    "objectID": "ch/rbasics/firststeps.html#subsetting-data",
    "href": "ch/rbasics/firststeps.html#subsetting-data",
    "title": "Basic R",
    "section": "Subsetting Data",
    "text": "Subsetting Data\nVectors\n\n# Create a vector\nexpression_data &lt;- c(3.2, 4.5, 2.1, 6.7, 5.9, 3.3, 7.8, 2.9)\nnames(expression_data) &lt;- paste0(\"Sample_\", 1:8)\nexpression_data\n\nSample_1 Sample_2 Sample_3 Sample_4 Sample_5 Sample_6 Sample_7 Sample_8 \n     3.2      4.5      2.1      6.7      5.9      3.3      7.8      2.9 \n\n# Subset by position\nexpression_data[3]             # Single element\n\nSample_3 \n     2.1 \n\nexpression_data[c(1, 3, 5)]    # Multiple elements\n\nSample_1 Sample_3 Sample_5 \n     3.2      2.1      5.9 \n\nexpression_data[2:5]           # Range\n\nSample_2 Sample_3 Sample_4 Sample_5 \n     4.5      2.1      6.7      5.9 \n\n# Subset by name\nexpression_data[\"Sample_6\"]\n\nSample_6 \n     3.3 \n\nexpression_data[c(\"Sample_1\", \"Sample_8\")]\n\nSample_1 Sample_8 \n     3.2      2.9 \n\n# Subset by condition\nexpression_data[expression_data &gt; 5]              # Values &gt; 5\n\nSample_4 Sample_5 Sample_7 \n     6.7      5.9      7.8 \n\nexpression_data[expression_data &gt;= 3 & expression_data &lt;= 6]  # Values between 3 and 6\n\nSample_1 Sample_2 Sample_5 Sample_6 \n     3.2      4.5      5.9      3.3 \n\n\nData Frames\n\n# Create a data frame\ngene_df &lt;- data.frame(\n  gene_id = c(\"BRCA1\", \"TP53\", \"MYC\", \"EGFR\", \"GAPDH\"),\n  expression = c(8.2, 6.1, 9.5, 7.0, 10.0),\n  mutation = factor(c(\"Yes\", \"No\", \"Yes\", \"No\", \"No\")),\n  pathway = c(\"DNA Repair\", \"Apoptosis\", \"Cell Cycle\", \"Signaling\", \"Metabolism\")\n)\n\ngene_df\n\n  gene_id expression mutation    pathway\n1   BRCA1        8.2      Yes DNA Repair\n2    TP53        6.1       No  Apoptosis\n3     MYC        9.5      Yes Cell Cycle\n4    EGFR        7.0       No  Signaling\n5   GAPDH       10.0       No Metabolism\n\n# Subsetting by row index\ngene_df[1:3, ]         # First three rows, all columns\n\n  gene_id expression mutation    pathway\n1   BRCA1        8.2      Yes DNA Repair\n2    TP53        6.1       No  Apoptosis\n3     MYC        9.5      Yes Cell Cycle\n\n# Subsetting by column index\ngene_df[, 1:2]     # All rows, first two columns\n\n  gene_id expression\n1   BRCA1        8.2\n2    TP53        6.1\n3     MYC        9.5\n4    EGFR        7.0\n5   GAPDH       10.0\n\n# Subsetting by column name\ngene_df[, c(\"gene_id\", \"mutation\")]\n\n  gene_id mutation\n1   BRCA1      Yes\n2    TP53       No\n3     MYC      Yes\n4    EGFR       No\n5   GAPDH       No\n\n# Using the $ operator\ngene_df$expression\n\n[1]  8.2  6.1  9.5  7.0 10.0\n\ngene_df$mutation\n\n[1] Yes No  Yes No  No \nLevels: No Yes\n\n# Subsetting by condition\ngene_df[gene_df$expression &gt; 8, ]\n\n  gene_id expression mutation    pathway\n1   BRCA1        8.2      Yes DNA Repair\n3     MYC        9.5      Yes Cell Cycle\n5   GAPDH       10.0       No Metabolism\n\ngene_df[gene_df$mutation == \"Yes\", ]\n\n  gene_id expression mutation    pathway\n1   BRCA1        8.2      Yes DNA Repair\n3     MYC        9.5      Yes Cell Cycle\n\n# Multiple conditions\ngene_df[gene_df$expression &gt; 7 & gene_df$mutation == \"No\", ]\n\n  gene_id expression mutation    pathway\n5   GAPDH         10       No Metabolism\n\n\nLogical Operators\n\n\nOperator\nMeaning\nExample\n\n\n\n==\nEqual to\nx == 5\n\n\n!=\nNot equal\nx != 5\n\n\n&lt;\nLess than\nx &lt; 5\n\n\n&gt;\nGreater than\nx &gt; 5\n\n\n&lt;=\nLess or equal\nx &lt;= 5\n\n\n&gt;=\nGreater or equal\nx &gt;= 5\n\n\n!\nNot\n!(x &lt; 5)\n\n\n|\nOR\nx &lt; 5 | x &gt; 10\n\n\n&\nAND\nx &gt; 5 & x &lt; 10\n\n\nRow Names in Data Frames\nRow names are particularly important in bioinformatics where genes, proteins, or samples are often used as identifiers.\n\n# Setting row names for gene_df\nrownames(gene_df) &lt;- gene_df$gene_id\ngene_df\n\n      gene_id expression mutation    pathway\nBRCA1   BRCA1        8.2      Yes DNA Repair\nTP53     TP53        6.1       No  Apoptosis\nMYC       MYC        9.5      Yes Cell Cycle\nEGFR     EGFR        7.0       No  Signaling\nGAPDH   GAPDH       10.0       No Metabolism\n\n\nWe can now drop the gene_id column, if required.\n\ngene_df_clean &lt;- gene_df[, -1]  # Remove the first column\ngene_df_clean\n\n      expression mutation    pathway\nBRCA1        8.2      Yes DNA Repair\nTP53         6.1       No  Apoptosis\nMYC          9.5      Yes Cell Cycle\nEGFR         7.0       No  Signaling\nGAPDH       10.0       No Metabolism\n\n# Access rows by name\ngene_df_clean[\"TP53\", ]\n\n     expression mutation   pathway\nTP53        6.1       No Apoptosis\n\n# Check if row names are unique\nany(duplicated(rownames(gene_df_clean)))\n\n[1] FALSE\n\n# Handle potential duplicated row names\n# NOTE: R doesn't allow duplicate row names by default\ndup_genes &lt;- data.frame(\n  expression = c(5.2, 6.3, 5.2, 8.1),\n  mutation = c(\"Yes\", \"No\", \"Yes\", \"No\")\n)\n\n# This would cause an error:\n#rownames(dup_genes) &lt;- c(\"BRCA1\", \"BRCA1\", \"TP53\", \"EGFR\")\n\n# Instead, we can preemptively make them unique:\nproposed_names &lt;- c(\"BRCA1\", \"BRCA1\", \"TP53\", \"EGFR\")\nunique_names &lt;- make.unique(proposed_names)\nunique_names  # Show the generated unique names\n\n[1] \"BRCA1\"   \"BRCA1.1\" \"TP53\"    \"EGFR\"   \n\n# Now we can safely assign them\nrownames(dup_genes) &lt;- unique_names\ndup_genes\n\n        expression mutation\nBRCA1          5.2      Yes\nBRCA1.1        6.3       No\nTP53           5.2      Yes\nEGFR           8.1       No\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhy is unique name important for us? Imagine this meaningful biological scenario: one gene might transcribed into many transcript isoforms and hence many protein isoforms. From RNAseq data, we might get alignment count for each gene. But then we can separate the count for each transcript. One gene has one name or ID, but the transcripts are many for the same gene! So, we can denote, for example, 21 isoform of geneA like genA.1, geneA.2, geneA.3,……., geneA.21. See this link for MBP gene. How many transcript isoforms does it have?"
  },
  {
    "objectID": "ch/rbasics/firststeps.html#homeworks-factors-subsetting-and-biological-insight",
    "href": "ch/rbasics/firststeps.html#homeworks-factors-subsetting-and-biological-insight",
    "title": "Basic R",
    "section": "🏡 Homeworks: Factors, Subsetting, and Biological Insight",
    "text": "🏡 Homeworks: Factors, Subsetting, and Biological Insight\n\n(Factor vs Character) Explain the difference between a character vector and a factor in R. Why would mutation_status be a factor and not just a character vector?\n(Factor Level Order) You observed the following bacterial species in gut microbiome samples:\n\n\nspecies &lt;- c(\"Lactobacillus\", \"Bacteroides\", \"Escherichia\", \"Bacteroides\", \"Lactobacillus\")\nspecies_factor &lt;- factor(species, levels = c(\"Bacteroides\", \"Escherichia\", \"Lactobacillus\"))\n\nWhat will levels(species_factor) return? Why?\n\nGiven the factor:\n\n\ndisease_severity &lt;- factor(c(\"Mild\", \"Severe\", \"Moderate\"), levels = c(\"Mild\", \"Moderate\", \"Severe\", \"Critical\"), ordered = TRUE)\n\nWhat will be the result of disease_severity[1] &lt; disease_severity[2] and why?\n\nYou computed:\n\n\nprop &lt;- prop.table(table(species_factor))\n\nHow do you extract the proportion of “Escherichia” samples from prop? Is prop$Escherichia valid?\n\nInterpret what this query returns:\n\n\ngene_df[gene_df$expression &gt; 7 & gene_df$mutation == \"No\", ]\n\nWhat type of genes does it select?\n\nYou have:\n\n\nsamples &lt;- c(\"WT\", \"KO\", \"WT\", \"KO\", \"WT\")\nexpression &lt;- c(5.2, 8.1, 4.3, 9.0, 5.7)\n\nMake a dataframe using these 2 vectors first. Then,\n\nCreate a factor group_factor for the samples.\nUse tapply() to calculate mean expression per group.\n\n\n\n\n\n\n\nNote\n\n\n\nUse ?tapply() to see how to use it.\nHint: You need to provide things for X, INDEX, FUN. You have X, INDEX in this small dataframe. The FUN should be applied thinking of what you are trying to do. You are trying to get the mean or average, right?\n\n\n\nPlot a barplot of average expression for each group.\n\n\nUse the gene_df example. Subset the data to find genes with:\n\n\nexpression &gt; 8\npathway is either “Cell Cycle” or “Signaling”\n\n\nCreate an ordered factor for the disease stages: c(\"Stage I\", \"Stage III\", \"Stage II\", \"Stage IV\", \"Stage I\"). Then plot the number of patients per stage using barplot(). Confirm that \"Stage III\" &gt; \"Stage I\" is logical in your factor.\nSuppose gene_data has a column type with values “Oncogene”, “Tumor Suppressor”, and “Housekeeping”.\n\n\nSubset all “Oncogene” rows where expression &gt; 8.\nChange the reference level of the factor type to “Housekeeping”\n\n\nSimulate expression data for 3 tissues (see the code chunk below): We are going to use rnorm() function to generate random values from a normal distribution for this purpose. The example values inside the rnorm() function means we want:\n\n\n30 values in total,\naverage or mean value = 8,\nstandard deviation of expression is 2.\n\nYou can play with the numbers to make your own values.\nrep() function is to replicate things (many times). In this example, we have rep(c(\"brain\", \"liver\", \"kidney\"), each = 10). We will be having 10x “brains”, followed by 10x “liver”, followed by 10x “kidney”. So, if you have changed your values inside the rnorm() function, make this value meaningful for you. Now we have 3 things, each=10. So, 3*10=30 is matching with the total value inside rnorm() function. Got it?\n\nset.seed(42) #just for reproducibility. Not completely needed\ngene_expr &lt;- rnorm(30, mean = 8, sd = 2)\ntissue &lt;- rep(c(\"brain\", \"liver\", \"kidney\"), each = 10)\ntissue_factor &lt;- factor(tissue, levels = c(\"liver\", \"brain\", \"kidney\"))\n\n\nMake a boxplot showing expression per tissue.\nWhich tissue shows the most variable gene expression? (Use tapply() + sd())\n\n\n\n\n\n\n\nNote\n\n\n\nHint: Variability is an expression of measuring standard deviation (sd) just by squaring it. So, var = sd^2. Well, do you see how to use sd inside tapply() function? Use ?tapply() to know how to use it.\n\n\nUse these questions as a self-check – reflect on why each step works before moving on to the next level (question).\nPush your .Rmd file and share by Friday 10PM BD Time."
  },
  {
    "objectID": "ch/rbasics/firststeps.html#conditionals",
    "href": "ch/rbasics/firststeps.html#conditionals",
    "title": "Basic R",
    "section": "Conditionals",
    "text": "Conditionals\n\nif-else statement\nGeneral structure of if-else statement:\n\nif (condition1) {\n  # Code executed when condition1 is TRUE\n} else if (condition2) {\n  # Code executed when condition1 is FALSE but condition2 is TRUE\n} else {\n  # Code executed when all conditions above are FALSE\n}\n\nLet’s use it now.\n\n# if-else statement\ngene_value &lt;- 6.8\n\nif(gene_value &gt; 10) {\n  print(\"High expression\")\n} else if(gene_value &gt; 5) {\n  print(\"Medium expression\")\n} else {\n  print(\"Low expression\")\n}\n\n[1] \"Medium expression\"\n\n\nVisualize it using the image below.\n\n\n\n\nifelse statement for vectors\nifelse is binary in nature. So, we can categorize only 2 things using ifelse. The structure is:\n\nifelse(test_or_condition, \"value_if_condition_is_TRUE\", \"value_if_condition_is_FALSE\")\n\nSee this example:\n\nexpression_values &lt;- c(12.5, 4.3, 8.1, 2.2)\nlabels &lt;- ifelse(expression_values &gt; 5, \"Upregulated\", \"Downregulated\")\nlabels\n\n[1] \"Upregulated\"   \"Downregulated\" \"Upregulated\"   \"Downregulated\"\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nifelse has 3 things inside the parentheses, right? The first one is the condition, the second one is the category we define if the condition is met, and the third thing is the other remaining category we want to assign if the condition is not met. So, it’s usage is perfect to say if a gene/transcript is upregulated or downregulated (binary classification).\n\n\nIf we still want to categorize more than 2 categories using ifelse, we need to use it in a nested way. The structure will be like this:\n\nifelse(test1, value1,\n       ifelse(test2, value2, \n              value3))\n\nSee this example:\n\n# ifelse() for vectors\nexpression_levels &lt;- c(2.5, 5.8, 7.2, 3.1, 6.9)\nexpression_category &lt;- ifelse(expression_levels &gt; 6, \n                             \"High\", \n                             ifelse(expression_levels &gt; 4, \"Medium\", \"Low\"))\nexpression_category\n\n[1] \"Low\"    \"Medium\" \"High\"   \"Low\"    \"High\"  \n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou remember the general structure of ifelse loop, right? the second thing after the first , is the assigned category if the condition is met. So, we assigned it as High here in this example. But then after the second , there is a second ifelse loop instead of a category. The second loop makes 2 more binary categories Medium and Low, and our task of assigning 3 categories is achieved.\n\n\ndplyr package has a function named case-when() to help us use as many categories we want. The same task would be achieved like this:\n\n# Requires dplyr package\n#install.packages(\"dplyr\") #decomment if you need to install the package\nlibrary(dplyr)\nexpression_levels &lt;- c(2.5, 5.8, 7.2, 3.1, 6.9)\nlabels &lt;- case_when(\n  expression_levels &gt; 6 ~ \"High\",\n  expression_levels &gt; 4 ~ \"Medium\",\n  TRUE ~ \"Low\"  # Default case\n)\nlabels\n\n[1] \"Low\"    \"Medium\" \"High\"   \"Low\"    \"High\"  \n\n\n\n\n\n\n\n\nNote\n\n\n\nDo you see the point how you would use the ifelse loop if you wanted to write a function to make 4 or 5 categories? If not, pause and re-think. You need to see the point. But anyway, categorizing more than 2 is better using if-else statement\n\n\n\nfor loop\n\ngenes &lt;- c(\"BRCA1\", \"TP53\", \"MYC\", \"CDC2\", \"MBP\")\nexpr &lt;- c(8.2, 5.4, 11.0, 5.4, 13.0)\n\nfor (i in 1:length(genes)) {\n  status &lt;- if (expr[i] &gt; 10) \"High\" \n  else if (expr[i] &gt; 6) \"Moderate\" \n  else \"Low\"\n  cat(genes[i], \"has\", status, \"expression\\n\")\n}\n\nBRCA1 has Moderate expression\nTP53 has Low expression\nMYC has High expression\nCDC2 has Low expression\nMBP has High expression\n\n\nIn-class Task:\n\nMake a data frame using genes and expr.\nAdd/flag the categories High, Moderate and Low you get using the for loop in a new column named expression_level or similar.\n\n\n# Step 1: Vectors\ngenes &lt;- c(\"BRCA1\", \"TP53\", \"MYC\", \"CDC2\", \"MBP\")\nexpr &lt;- c(8.2, 5.4, 11.0, 5.4, 13.0)\n\n# Step 2: Create a data frame\ngene_df &lt;- data.frame(gene = genes, expression = expr)\n\n# Step 3: Add an empty column for expression level\ngene_df$expression_level &lt;- NA\n\n# Step 4: Use for loop to fill in the expression_level column\nfor (i in 1:nrow(gene_df)) {\n  gene_df$expression_level[i] &lt;- if (gene_df$expression[i] &gt; 10) {\n    \"High\"\n  } else if (gene_df$expression[i] &gt; 6) {\n    \"Moderate\"\n  } else {\n    \"Low\"\n  }\n}\n\n# View the final data frame\nprint(gene_df)\n\n   gene expression expression_level\n1 BRCA1        8.2         Moderate\n2  TP53        5.4              Low\n3   MYC       11.0             High\n4  CDC2        5.4              Low\n5   MBP       13.0             High\n\n\n\nwhile loop\nContext:\nYou are preparing biological samples (e.g., blood, DNA extracts) for analysis. You have a set of samples labeled Sample 1 to Sample 5. You want to check each one in order and confirm that it’s ready for analysis. Use a while loop to process the samples sequentially.\n\ni &lt;- 1\nwhile (i &lt;= 5) {\n  cat(\"Sample\", i, \"is ready for analysis\\n\")\n  i &lt;- i + 1\n}\n\nSample 1 is ready for analysis\nSample 2 is ready for analysis\nSample 3 is ready for analysis\nSample 4 is ready for analysis\nSample 5 is ready for analysis\n\n\n\nnext and break\n\nContext:\nYou are screening biological samples (e.g., tissue or blood) in a quality control process. Some samples are good, some are suboptimal (not contaminated but poor quality), and some are contaminated (must be flagged and stop further processing). Use next to skip suboptimal samples and break to immediately stop when a contaminated sample is found.\n\nsamples &lt;- c(\"good\", \"bad\", \"good\", \"contaminated\")\n\nfor (i in samples) {\n  if (i == \"contaminated\") {\n    print(\"Stop! Contaminated sample.\")\n    break\n  }\n  if (i == \"bad\") next\n  print(paste(\"Processing\", i))\n}\n\n[1] \"Processing good\"\n[1] \"Processing good\"\n[1] \"Stop! Contaminated sample.\""
  },
  {
    "objectID": "ch/rbasics/firststeps.html#writing-functions-in-r",
    "href": "ch/rbasics/firststeps.html#writing-functions-in-r",
    "title": "Basic R",
    "section": "Writing Functions in R",
    "text": "Writing Functions in R\nThe syntax to write an R function is:\n\nfunction_name &lt;- function() {\n}\n\nLet’s use it.\nFlag gene expression\n\nflag_expression &lt;- function(value) {\n  if (value &gt; 10) {\n    return(\"High\")\n  } else if (value &gt; 5) {\n    return(\"Moderate\")\n  } else {\n    return(\"Low\")\n  }\n}\n\nflag_expression(8.3)\n\n[1] \"Moderate\"\n\n\nApply to a vector\n\nexpr_values &lt;- c(12.2, 4.4, 7.5)\nsapply(X=expr_values, FUN=flag_expression)\n\n[1] \"High\"     \"Low\"      \"Moderate\"\n\n\nFunction with multiple arguments\n\ngene_status &lt;- function(gene, expression, threshold = 6) { #gene = \"string\", expression = value\n  label &lt;- ifelse(expression &gt; threshold, \"Up\", \"Down\")\n  return(paste(gene, \"is\", label, \"regulated\"))\n}\n\ngene_status(\"TP53\", 8.1, threshold = 9)\n\n[1] \"TP53 is Down regulated\"\n\n\nReturn a list\n\ncalc_stats &lt;- function(values) {\n  return(list(mean = mean(values), sd = sd(values)))\n}\n\ncalc_stats(c(4.2, 5.5, 7.8))\n\n$mean\n[1] 5.833333\n\n$sd\n[1] 1.823001"
  },
  {
    "objectID": "ch/rbasics/firststeps.html#homeworks-control-structures-and-functions",
    "href": "ch/rbasics/firststeps.html#homeworks-control-structures-and-functions",
    "title": "Basic R",
    "section": "🏡 Homeworks: Control Structures and Functions",
    "text": "🏡 Homeworks: Control Structures and Functions\n\n\nfor Loop + Conditional: Gene Expression Classifier\n\nYou have:\n\ngenes &lt;- c(\"TP53\", \"EGFR\", \"MYC\", \"BRCA2\")\nexpr &lt;- c(4.8, 6.3, 11.7, 9.5)\n\nTask:\nLoop over the gene names and print: &lt;Gene&gt; has &lt;Low/Moderate/High&gt; expression\nBased on:\n\nHigh: &gt;10\nModerate: 6–10\nLow: ≤6\n\n\n\nnext: Sample Screening Skips Bad Samples\n\n\nsamples &lt;- c(\"good\", \"bad\", \"good\", \"contaminated\", \"good\", \"bad\")\n\nContext:\nBad samples should be skipped, only good and contaminated should be reported.\nTask:\nUse a for loop with next to skip bad samples and print a message for others: Sample X is flagged for analysis\n\n\nbreak: Stop at Contaminated Sample Use the same samples vector above.\n\nTask:\nPrint Analyzing sample X for each sample. But if a contaminated sample is found, stop processing immediately and print Contamination detected! Halting...\n\n\nwhile Loop: Sample Prep Countdown\n\nContext:\nYou’re preparing 5 samples.\nTask:\nUse a while loop to print: Sample &lt;i&gt; is ready for analysis from 1 to 5.\n\n\nwhile + Condition: Until Threshold You’re measuring a protein level that starts at 2. With each measurement, it increases randomly between 0.5 and 1.5 units.\n\nTask: Use a while loop to simulate the increase until the protein level exceeds 10. Print each step.\n\nlevel &lt;- 2\n#your code goes here\n\n\nCustom Function: Classify BMI\n\nWrite a function bmi_category(weight, height) that:\n\nTakes weight (kg) and height (m)\nCalculates BMI: weight/height²\n\nReturns Underweight if &lt;18.5, Normal if 18.5–24.9, Overweight if 25–29.9, Obese if ≥30.\nTest it:\nbmi_category(65, 1.70)\n\nFunction: Gene Risk Calculator\n\nWrite a function gene_risk(expr, mut_status):\nHigh if expression &gt; 9 and mutation = Yes.\nModerate if expression &gt; 6.\nLow otherwise.\nTest on vectors:\n\nexpr &lt;- c(5.5, 10.1, 7.3)\nmut_status &lt;- c(\"No\", \"Yes\", \"No\")\n\nHint: Use mapply().\n\nfor Loop + Data Frame Make a data frame of 4 genes and expression values. Add a new column category using a for loop that assigns High, Moderate, Low based on expression.\n\nFunction + Error Checking Write a function get_expression_level(gene, df) that:\nTakes a gene name and a data frame with gene and expr columns\nIf the gene is present, returns Low, Moderate or High.\nIf missing, returns Gene not found\nTest it with:\n\n\n\ndf &lt;- data.frame(gene = c(\"TP53\", \"BRCA1\"), expr = c(5.5, 10.5))\nget_expression_level(\"BRCA1\", df)\nget_expression_level(\"EGFR\", df)"
  },
  {
    "objectID": "ch/rbasics/firststeps.html#handling-missingwrong-values",
    "href": "ch/rbasics/firststeps.html#handling-missingwrong-values",
    "title": "Basic R",
    "section": "Handling Missing/Wrong Values",
    "text": "Handling Missing/Wrong Values\nIdentifying Issues\n\n# Create data with missing values\nclinical_data &lt;- data.frame(\n  patient_id = 1:5,\n  age = c(25, 99, 30, -5, 40),    # -5 is wrong, 99 is suspect\n  bp = c(120, NA, 115, 125, 118),  # NA is missing\n  weight = c(65, 70, NA, 68, -1)   # -1 is wrong\n)\nclinical_data\n\n  patient_id age  bp weight\n1          1  25 120     65\n2          2  99  NA     70\n3          3  30 115     NA\n4          4  -5 125     68\n5          5  40 118     -1\n\n# Check for missing values\nis.na(clinical_data)\n\n     patient_id   age    bp weight\n[1,]      FALSE FALSE FALSE  FALSE\n[2,]      FALSE FALSE  TRUE  FALSE\n[3,]      FALSE FALSE FALSE   TRUE\n[4,]      FALSE FALSE FALSE  FALSE\n[5,]      FALSE FALSE FALSE  FALSE\n\ncolSums(is.na(clinical_data))  # Count NAs by column\n\npatient_id        age         bp     weight \n         0          0          1          1 \n\n# Check for impossible values\nclinical_data$age &lt; 0\n\n[1] FALSE FALSE FALSE  TRUE FALSE\n\nclinical_data$weight &lt; 0\n\n[1] FALSE FALSE    NA FALSE  TRUE\n\n# Find indices of problematic values\nwhich(clinical_data$age &lt; 0 | clinical_data$age &gt; 90)\n\n[1] 2 4\n\n\nFixing Data\n\n# Replace impossible values with NA\nclinical_data$age[clinical_data$age &lt; 0 | clinical_data$age &gt; 90] &lt;- NA\nclinical_data$weight[clinical_data$weight &lt; 0] &lt;- NA\nclinical_data\n\n  patient_id age  bp weight\n1          1  25 120     65\n2          2  NA  NA     70\n3          3  30 115     NA\n4          4  NA 125     68\n5          5  40 118     NA\n\n# Replace NAs with mean (common in biological data)\nclinical_data$bp[is.na(clinical_data$bp)] &lt;- mean(clinical_data$bp, na.rm = TRUE)\nclinical_data$weight[is.na(clinical_data$weight)] &lt;- mean(clinical_data$weight, na.rm = TRUE)\nclinical_data\n\n  patient_id age    bp   weight\n1          1  25 120.0 65.00000\n2          2  NA 119.5 70.00000\n3          3  30 115.0 67.66667\n4          4  NA 125.0 68.00000\n5          5  40 118.0 67.66667\n\n# Replace NAs with median (better for skewed data)\nclinical_data$age[is.na(clinical_data$age)] &lt;- median(clinical_data$age, na.rm = TRUE)\nclinical_data\n\n  patient_id age    bp   weight\n1          1  25 120.0 65.00000\n2          2  30 119.5 70.00000\n3          3  30 115.0 67.66667\n4          4  30 125.0 68.00000\n5          5  40 118.0 67.66667"
  },
  {
    "objectID": "ch/rbasics/firststeps.html#data-transformation",
    "href": "ch/rbasics/firststeps.html#data-transformation",
    "title": "Basic R",
    "section": "Data Transformation",
    "text": "Data Transformation\nIntroduction to Outliers\nOutliers can significantly affect statistical analyses, especially in biological data where sample variation can be high.\n\n# Create data with outliers\nexpression_levels &lt;- c(2.3, 2.7, 3.1, 2.9, 2.5, 3.0, 15.2, 2.8)\nboxplot(expression_levels, \n        main = \"Expression Levels with Outlier\",\n        ylab = \"Expression\")\n\n\n\n\nIdentifying Outliers\n\n# Statistical approach: Values beyond 1.5*IQR\ndata_summary &lt;- summary(expression_levels)\ndata_summary\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.300   2.650   2.850   4.312   3.025  15.200 \n\nIQR_value &lt;- IQR(expression_levels)\nupper_bound &lt;- data_summary[\"3rd Qu.\"] + 1.5 * IQR_value\nlower_bound &lt;- data_summary[\"1st Qu.\"] - 1.5 * IQR_value\n\n# Find outliers\noutliers &lt;- expression_levels[expression_levels &gt; upper_bound | \n                             expression_levels &lt; lower_bound]\noutliers\n\n[1] 15.2\n\n\nTransforming Vectors\nMathematical transformations can normalize data, reduce outlier effects, and make data more suitable for statistical analyses.\n\n# Original data\ngene_exp &lt;- c(15, 42, 87, 115, 320, 560, 1120)\nhist(gene_exp, main = \"Original Expression Values\", xlab = \"Expression\")\n\n\n\n# Log transformation (common in gene expression analysis)\nlog_exp &lt;- log2(gene_exp)\nhist(log_exp, main = \"Log2 Transformed Expression\", xlab = \"Log2 Expression\")\n\n\n\n# Square root transformation (less aggressive than log)\nsqrt_exp &lt;- sqrt(gene_exp)\nhist(sqrt_exp, main = \"Square Root Transformed Expression\", xlab = \"Sqrt Expression\")\n\n\n\n# Z-score normalization (standardization)\nz_exp &lt;- scale(gene_exp)\nhist(z_exp, main = \"Z-score Normalized Expression\", xlab = \"Z-score\")\n\n\n\n# Compare transformations\npar(mfrow = c(2, 2))\nhist(gene_exp, main = \"Original\")\nhist(log_exp, main = \"Log2\")\nhist(sqrt_exp, main = \"Square Root\")\nhist(z_exp, main = \"Z-score\")\n\n\n\npar(mfrow = c(1, 1))  # Reset plotting layout\n\nLogical Expressions\n\n# Create gene expression vector\nexp_data &lt;- c(5.2, 3.8, 7.1, 2.9, 6.5, 8.0, 4.3)\nnames(exp_data) &lt;- paste0(\"Gene_\", 1:7)\n\n# Basic comparisons\nexp_data &gt; 5    # Which genes have expression &gt; 5?\n\nGene_1 Gene_2 Gene_3 Gene_4 Gene_5 Gene_6 Gene_7 \n  TRUE  FALSE   TRUE  FALSE   TRUE   TRUE  FALSE \n\nexp_data &lt;= 4   # Which genes have expression &lt;= 4?\n\nGene_1 Gene_2 Gene_3 Gene_4 Gene_5 Gene_6 Gene_7 \n FALSE   TRUE  FALSE   TRUE  FALSE  FALSE  FALSE \n\n# Store results in logical vector\nhigh_exp &lt;- exp_data &gt; 6\nhigh_exp\n\nGene_1 Gene_2 Gene_3 Gene_4 Gene_5 Gene_6 Gene_7 \n FALSE  FALSE   TRUE  FALSE   TRUE   TRUE  FALSE \n\n# Use logical vectors for subsetting\nexp_data[high_exp]  # Get high expression values\n\nGene_3 Gene_5 Gene_6 \n   7.1    6.5    8.0 \n\n\nLogical Operators\n\n# Combining conditions with AND (&)\nexp_data &gt; 4 & exp_data &lt; 7  # Expression between 4 and 7\n\nGene_1 Gene_2 Gene_3 Gene_4 Gene_5 Gene_6 Gene_7 \n  TRUE  FALSE  FALSE  FALSE   TRUE  FALSE   TRUE \n\n# Combining conditions with OR (|)\nexp_data &lt; 4 | exp_data &gt; 7  # Expression less than 4 OR greater than 7\n\nGene_1 Gene_2 Gene_3 Gene_4 Gene_5 Gene_6 Gene_7 \n FALSE   TRUE   TRUE   TRUE  FALSE   TRUE  FALSE \n\n# Using NOT (!)\n!high_exp  # Not high expression\n\nGene_1 Gene_2 Gene_3 Gene_4 Gene_5 Gene_6 Gene_7 \n  TRUE   TRUE  FALSE   TRUE  FALSE  FALSE   TRUE \n\n# Subsetting with combined conditions\nexp_data[exp_data &gt; 4 & exp_data &lt; 7]  # Get values between 4 and 7\n\nGene_1 Gene_5 Gene_7 \n   5.2    6.5    4.3 \n\n\nLogical Functions\n\n# all() - Are all values TRUE?\nall(exp_data &gt; 0)  # Are all expressions positive?\n\n[1] TRUE\n\n# any() - Is at least one value TRUE?\nany(exp_data &gt; 7)  # Is any expression greater than 7?\n\n[1] TRUE\n\n# which() - Get indices of TRUE values\nwhich(exp_data &gt; 6)  # Which elements have expressions &gt; 6?\n\nGene_3 Gene_5 Gene_6 \n     3      5      6 \n\n# %in% operator - Test for membership\ntest_genes &lt;- c(\"Gene_1\", \"Gene_5\", \"Gene_9\")\nnames(exp_data) %in% test_genes  # Which names match test_genes?\n\n[1]  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE\n\n\nPractical Session\nCheck out this repo: https://github.com/genomicsclass/dagdata/\n\n# Download small example dataset\ndownload.file(\"https://github.com/genomicsclass/dagdata/raw/master/inst/extdata/msleep_ggplot2.csv\",\n              destfile = \"msleep_data.csv\")\n\n# Load data\nmsleep &lt;- read.csv(\"msleep_data.csv\")\n\n\nConvert ‘vore’ column to factor and plot its distribution.\nCreate a matrix of sleep data columns and add row names.\nFind and handle any missing values.\nCalculate mean sleep time by diet category (vore).\nIdentify outliers in sleep_total."
  },
  {
    "objectID": "ch/rbasics/firststeps.html#summary-of-the-lesson",
    "href": "ch/rbasics/firststeps.html#summary-of-the-lesson",
    "title": "Basic R",
    "section": "Summary of the Lesson",
    "text": "Summary of the Lesson\nIn this lesson, we covered:\n\n\nFactor Variables: Essential for categorical data in biology (genotypes, treatments, etc.)\n\n\nCreation, levels, ordering, and visualization\n\n\n\nSubsetting Techniques: Critical for data extraction and analysis\n\n\nVector and data frame subsetting with various methods\nUsing row names effectively for biological identifiers\n\n\n\nMatrix Operations: Fundamental for expression data\n\n\nCreation, manipulation, and biological applications\nCalculating fold changes and other common operations\n\n\n\nMissing Values: Practical approaches for real-world biological data\n\n\nIdentification and appropriate replacement methods\n\n\n\nData Transformation: Making data suitable for statistical analysis\n\n\nLog, square root, and z-score transformations\nOutlier identification and handling\n\n\n\nLogical Operations: For data filtering and decision making\n\n\n\nConditions, combinations, and applications\nThese skills form the foundation for the more advanced visualization techniques we’ll cover in future lessons.\n\n\n\n\nList: Fundamental for many biological data and packages’ output.\n\n\nProperties, accessing, and applications\n\n\nWe will know more about conditionals, R packages to handle data and visualization in a better and efficient way."
  },
  {
    "objectID": "ch/rbasics/firststeps.html#homework",
    "href": "ch/rbasics/firststeps.html#homework",
    "title": "Basic R",
    "section": "Homework",
    "text": "Homework\n\n\nMatrix Operations:\n\n\nCreate a gene expression matrix with 8 genes and 4 conditions\nCalculate the mean expression for each gene\nCalculate fold change between condition 4 and condition 1\nCreate a heatmap of your matrix\n\n\n\nFactor Analysis:\n\n\nUsing the iris dataset, convert Species to an ordered factor\nCreate boxplots showing Sepal.Length by Species\nCalculate mean petal length for each species level\n\n\n\nData Cleaning Challenge:\n\n\nIn the downloaded msleep_data.csv:\nIdentify all columns with missing values\nReplace missing values appropriately\nCreate a new categorical variable “sleep_duration” with levels “Short”, “Medium”, “Long”\n\n\n\nList challenge:\n\n\nMake your own lists\nReplicate all the tasks we did\nYou may ask AI to give you beginner-level questions but don’t ask to solve the questions programmatically. Tell AI not to provide answers.\n\n\n\nComplete Documentation:\n\n\nWrite all code in R Markdown\nInclude comments explaining your approach\nPush to GitHub\n\nDue date: Friday 10pm BD Time\n\nset.seed(42) # For reproducibility\ngene_expr &lt;- rnorm(30, mean = 8, sd = 2)\ntissue &lt;- rep(c(\"brain\", \"liver\", \"kidney\"), each = 10)\ntissue_factor &lt;- factor(tissue, levels = c(\"liver\", \"brain\", \"kidney\"))\n\ntissue_factor\n\n [1] brain  brain  brain  brain  brain  brain  brain  brain  brain  brain \n[11] liver  liver  liver  liver  liver  liver  liver  liver  liver  liver \n[21] kidney kidney kidney kidney kidney kidney kidney kidney kidney kidney\nLevels: liver brain kidney\n\n\n\nexpr_data &lt;- data.frame(\n  tissue = tissue_factor,\n  expression = gene_expr\n)\n\n#expr_data\nhead(expr_data)\n\n  tissue expression\n1  brain  10.741917\n2  brain   6.870604\n3  brain   8.726257\n4  brain   9.265725\n5  brain   8.808537\n6  brain   7.787751\n\n\n\nboxplot(expression ~ tissue, \n        data = expr_data,\n        main = \"Gene Expression by Tissue\",\n        xlab = \"Tissue\",\n        ylab = \"Expression\",\n        col = c(\"lightblue\", \"lightgreen\", \"lightpink\"))\n\n\n\n\n\nsd_expression &lt;- tapply(expr_data$expression, expr_data$tissue, sd)\nsd_expression\n\n   liver    brain   kidney \n3.261169 1.670898 2.312116 \n\n\n\nmax_sd_tissue &lt;- names(sd_expression)[which.max(sd_expression)]\nmax_sd_value &lt;- sd_expression[which.max(sd_expression)]\ncat(\"The tissue with the highest sd is:\", max_sd_tissue,\", with SD =\", round(max_sd_value, 2), \"\\n\")\n\nThe tissue with the highest sd is: liver , with SD = 3.26"
  },
  {
    "objectID": "ch/rbasics/git.html",
    "href": "ch/rbasics/git.html",
    "title": "Git and CLI",
    "section": "",
    "text": "So, the summary goes first. Once you finish reading, fall back here to remind you the important things and steps of working with gitand GitHub and terminal. Regular practice will be easier in this way.\n\ngit and GitHub configuration is a bit tedious process (to make it secure for us though), but we are supposed to do it one time only. We don’t need to do it again and again.\nAnother important aspect is we are learning “how to follow instructions”.\nMany personal things in programming are in generic terms. We have to replace them accordingly. For example, your_name@gmail.com, your_user_name, your_password, path/to/your/file,your meaningful message etc.\nLearning to ask questions.\nThe standard workflow to work with a GitHub repository follows these few steps:\n\n\nGo to your GitHub account → Find the + sign and click on it → Click on New Repository.\nClone it in your computer (inside your preferred folder) running git clone git@github.com:yourusername/test1.git. Don’t forget to replace git@github.com:yourusername/test1.git with the SSH link you copied for your repo.\nMake changes in your folder by adding or creating files/folders. You can optionally check status of your folder running git status.\nStage/prepare/add them all to git running git add ..\n\nCommit/decorate it with a meaningful message running git commit -m \"your meaningful message\".\nNow push it to your remote (GitHub) repo running git push origin main.\n\n\nWe are recognizing code chunks in the document/page. Single line, multi-line code, and how to run them (run codes from the code chunks, even if I don’t explicitly say Run). Did you catch that running a code means writing (or copy-pasting) and pressing “Enter”/“Return” key on keyboard? Also, run multi-line code line by line, meaning next line after finishing the previous one running.\nGetting used to different vocabulary related to Computer and programming.\nWe learned navigating the file system from the bash/terminal."
  },
  {
    "objectID": "ch/rbasics/git.html#install-git",
    "href": "ch/rbasics/git.html#install-git",
    "title": "Git and CLI",
    "section": "Install Git",
    "text": "Install Git\nFor Windows\n\n\nDownload: Download Git from https://git-scm.com/download/win\n\n\nInstall: Now install, accept the default settings while installing\n\nOpen git bash: After installation, open Git Bash (not cmd/PowerShell). Find it and open by double clicking. Just keep it open, nothing else. We will return to this a bit later.\nFor MacOS\nIt should already be there. Open your terminal/shell now (Press \"command\" + \"space\" buttons together and write “terminal”. Choose the terminal). Then run:\n\ngit --version\n\nIf Git isn’t installed, macOS will prompt you to install the Xcode Command Line Tools. Click “Install” and you are done. But find the recommended below. Just run these two commands to keep your directory/repo clean.\n\necho .DS_Store &gt;&gt; ~/.gitignore_global\ngit config --global core.excludesfile ~/.gitignore_global\n\nMacOS makes .DS_Store which is not really needed for any projects (this is just an internal thingy for MacOS). But to make it work, we should have chosen the option for a gitignore file while opening the GitHub repository. We will know more about it later. Don’t worry. We are adding it to gitignore and telling git to ignore it for file tracking.\nFor Ubuntu/Debian\nOpen your terminal (Press Ctrl + Alt + T.) and run these two commands one after another:\n\nsudo apt update\nsudo apt install git\n\nProvide your password if prompted. You are all set!"
  },
  {
    "objectID": "ch/rbasics/git.html#create-a-github-account",
    "href": "ch/rbasics/git.html#create-a-github-account",
    "title": "Git and CLI",
    "section": "Create a GitHub Account",
    "text": "Create a GitHub Account\n\nNow go to https://github.com → Sign up with your email\nChoose a username and password. Make the username easy to remember (for example, with your names, maybe), password should be as difficult as possible.\n\nHint, it can be different than your email password. Use combination of capital and small letters, numbers and special characters like “@”, “!”, “?”, etc.\n\nVerify email (with the OTP) and you are set."
  },
  {
    "objectID": "ch/rbasics/git.html#set-up-git-for-the-first-time-only",
    "href": "ch/rbasics/git.html#set-up-git-for-the-first-time-only",
    "title": "Git and CLI",
    "section": "Set Up Git (for the first time only)",
    "text": "Set Up Git (for the first time only)\n\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your_email@example.com\"\n\n\n\n\n\n\n\nInfo!\n\n\n\nIt is a multi-line code chunk (2 lines). Run the first one first, then the second one.\n\n\n\n\n\n\n\n\nStop!\n\n\n\nHere, Your Name and your_email@example.com means your user name (the one you set up just a bit ago. You don’t remember? Explore your GitHub account, you will see it there.) and the email address you used for GitHub. Use them correctly. Space, punctuation marks, capital or small letters should exactly match (Don’t be a “Murad Takla”).\n\n\nYou can check your config:\n\ngit config --global --list\n\nIsn’t it showing things correctly? It should!"
  },
  {
    "objectID": "ch/rbasics/git.html#generate-ssh-key-and-connect-to-github",
    "href": "ch/rbasics/git.html#generate-ssh-key-and-connect-to-github",
    "title": "Git and CLI",
    "section": "Generate SSH Key and Connect to GitHub",
    "text": "Generate SSH Key and Connect to GitHub\nRecommended, need to do only once\n\n\n\n\n\n\nStop!\n\n\n\nThis step is creating a private and public key for you to identify that it is really you who is interacting (pushing, pulling, cloning for example) with the GitHub account later. We only upload the public key to GitHub, but never the private key. That is exclusively for yourself. Never share it with anybody.\n\n\n\nssh-keygen -t ed25519 -C \"you@example.com\"\n\nRead and just press Enter for all the prompts\n\n\n\n\n\n\nStop!\n\n\n\nDid you replace you@example.com with yours? If not, do it again replacing it with correct email address for your GitHub account.\n\n\n\n\n\n\n\n\nStop!\n\n\n\nDid you press Enter or return key multiple times or just kept staring at the screen? Read the messages on your screen to understand, and keep pressing Enter."
  },
  {
    "objectID": "ch/rbasics/git.html#add-ssh-key-to-github",
    "href": "ch/rbasics/git.html#add-ssh-key-to-github",
    "title": "Git and CLI",
    "section": "Add SSH Key to GitHub",
    "text": "Add SSH Key to GitHub\nNow, we are going to add the public key to GitHub. But do we know it? Get it this way:\n\nRun:\n\n\ncat ~/.ssh/id_ed25519.pub\n\n\nCopy the output: Be careful here. Just copy the output. No extra space after your email address (i.e. till .com).\n\n\n\n\n\n\n\nStop!\n\n\n\nDid you copied it correctly? Check again, maybe.\n\n\n\n\n\n\n\n\nHow is your key?\n\n\n\n\n\nIt has 3 fields separated with 2 spaces. Did you notice that?\n\n\n\n\nGo to GitHub → Settings → SSH and GPG keys → New SSH Key\n\nTitle: Whatever you want (but keep it simple, like my ssh key) Paste the key you copied and save it by clicking on Add SSH key. You are done.\n\n\n\n\n\n\nStop!\n\n\n\nDid you copy-pasted it correctly and is the key visible if you check under SSH and GPG keys now? It shows only the second field of your key though. If not, it is not done correctly. Work around!\n\n\n\nTest it:\n\n\nssh -T git@github.com\n\n\n\n\n\n\n\nHow is it?\n\n\n\nHi Your_user_name! You’ve successfully authenticated, but GitHub does not provide shell access.\n\n\nIt is totally fine, ignore the warning.\nSetting and configuration is done. Voila!"
  },
  {
    "objectID": "ch/rbasics/tidyverse.html",
    "href": "ch/rbasics/tidyverse.html",
    "title": "The tidyverse",
    "section": "",
    "text": "The tidyverse is a collection of R packages designed to make data science easier and more intuitive. Think of it as a toolkit where all the tools work well together and share a similar design philosophy. The packages help you:\n\nImport data\nClean and organize data\nTransform and manipulate data\nVisualize data\nModel data\n\n\nBefore we can use the tidyverse, we need to install it. First, let’s install a helpful package manager called pacman:\n\n# Install pacman if you haven't already\n#install.packages(\"pacman\")\n\n# Load pacman\nlibrary(pacman)\n\n# Now use pacman to install and load tidyverse\n#pacman::p_load(tidyverse)\n# (install &) load packages\npacman::p_load(\n  broom,\n  conflicted,\n  here,\n  janitor,\n  naniar,\n  readxl,\n  tibble,\n  tidyverse\n)\n\n# Alternative: traditional installation\n# install.packages(\"tidyverse\")\n# library(tidyverse)\nconflicts_prefer(dplyr::filter) \nconflicts_prefer(dplyr::select)\n#dplyr::select()\n\n\nHere are the main packages you’ll use most often:\n\n\nPackage\nPurpose\n\n\n\n\n ggplot2\n\nCreating beautiful graphs\n\n\n\n dplyr\n\nData manipulation\n\n\n\n tibble\n\nModern data frames\n\n\n\n tidyr\n\nTidying data\n\n\n\n readr\n\nReading data files"
  },
  {
    "objectID": "ch/linux-and-ctl/basics.html",
    "href": "ch/linux-and-ctl/basics.html",
    "title": "Linux Basics",
    "section": "",
    "text": "If you have Linux already, stay calm. Scrol down to learn things instead of installing WSL2.\nTry to use Linux to do better in life. The black screen of Linux is way more powerful than you (probably) know till now. Follow this step-by-step guide to install Ubuntu (a Linux distribution).\nFollow this pages to install Windows Subsystem for Linux if you are using Windows. Then you can use the WSL just like a native Linux using your system. You can’t do anything except using Linux to analyse Genomic (sequencing) data. So, better start now.\nFollow link1 & link2 to get step-by-step guideline to do it.\nA pro tip will be update your kernel first and then try. But here goes something."
  },
  {
    "objectID": "ch/linux-and-ctl/basics.html#linux-and-wsl2",
    "href": "ch/linux-and-ctl/basics.html#linux-and-wsl2",
    "title": "Linux Basics",
    "section": "",
    "text": "Try to use Linux to do better in life. The black screen of Linux is way more powerful than you (probably) know till now. Follow this step-by-step guide to install Ubuntu (a Linux distribution).\nFollow this pages to install Windows Subsystem for Linux if you are using Windows. Then you can use the WSL just like a native Linux using your system. You can’t do anything except using Linux to analyse Genomic (sequencing) data. So, better start now.\nFollow link1 & link2 to get step-by-step guideline to do it.\nThe problem here is, I am not using Windows. So, I might not be able to help you if there is any issue you are facing while installing WSL2. But I can try. A tip will be update your kernel first and then try."
  },
  {
    "objectID": "ch/linux-and-ctl/basics.html#intro",
    "href": "ch/linux-and-ctl/basics.html#intro",
    "title": "Linux Basics",
    "section": "Intro",
    "text": "Intro\n\n# Your Linux/Bash code here \necho \"Hello from Bash!\" \nls -l\n\nHello from Bash!\ntotal 200\n-rw-r--r--  1 md.rasheduzzaman  staff  84578 Sep 17 19:56 advanced.html\n-rw-r--r--  1 md.rasheduzzaman  staff   6549 Sep 17 19:55 advanced.qmd\ndrwxr-xr-x  3 md.rasheduzzaman  staff     96 Sep 17 19:56 basics_files\n-rw-r--r--  1 md.rasheduzzaman  staff   1255 Sep 11 14:36 basics.qmd\n-rw-r--r--  1 md.rasheduzzaman  staff   1260 Sep 17 19:56 basics.rmarkdown\n\n\nLearn a lot from https://github.com/hbctraining"
  },
  {
    "objectID": "ch/prob_stat/prob_basic.html",
    "href": "ch/prob_stat/prob_basic.html",
    "title": "Probabilty Basics",
    "section": "",
    "text": "CitationBibTeX citation:@online{rasheduzzaman2024,\n  author = {Md Rasheduzzaman},\n  title = {Probabilty {Basics}},\n  date = {2024-08-14},\n  langid = {en},\n  abstract = {Experiment, trial, distribution, PMF, PDF, CDF, etc.}\n}\nFor attribution, please cite this work as:\nMd Rasheduzzaman. 2024. “Probabilty Basics.” August 14,\n2024."
  },
  {
    "objectID": "ch/python/AI.html#tensor-creation",
    "href": "ch/python/AI.html#tensor-creation",
    "title": "Artificial Inteligence",
    "section": "",
    "text": "# using empty\na = torch.empty(2,3)\na\n\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\n\n\nLet’s check type of pur tensor.\n\n# check type\ntype(a)\n\ntorch.Tensor\n\n\n\n# using ones\ntorch.ones(3,3)\n\ntensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]])\n\n\n\n# using zeros\ntorch.zeros(3,3)\n\ntensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])\n\n\n\n# using rand\ntorch.manual_seed(40)\ntorch.rand(2,3)\n\ntensor([[0.3679, 0.8661, 0.1737],\n        [0.7157, 0.8649, 0.4878]])\n\n\n\ntorch.manual_seed(40)\ntorch.rand(2,3)\n\ntensor([[0.3679, 0.8661, 0.1737],\n        [0.7157, 0.8649, 0.4878]])\n\n\n\ntorch.randint(size=(2,3), low=0, high=10, dtype=torch.float32)\n\ntensor([[6., 3., 6.],\n        [7., 6., 5.]])\n\n\n\n# using tensor\ntorch.tensor([[3,2,1],[4,5,6]])\n\ntensor([[3, 2, 1],\n        [4, 5, 6]])\n\n\n\n# other ways\n\n# arange\na = torch.arange(0,15,3)\nprint(\"using arange -&gt;\", a)\n\n# using linspace\nb = torch.linspace(0,15,10)\nprint(\"using linspace -&gt;\", b)\n\n# using eye\nc = torch.eye(4)\nprint(\"using eye -&gt;\", c)\n\n# using full\nd = torch.full((3, 3), 5)\nprint(\"using full -&gt;\", d)\n\nusing arange -&gt; tensor([ 0,  3,  6,  9, 12])\nusing linspace -&gt; tensor([ 0.0000,  1.6667,  3.3333,  5.0000,  6.6667,  8.3333, 10.0000, 11.6667,\n        13.3333, 15.0000])\nusing eye -&gt; tensor([[1., 0., 0., 0.],\n        [0., 1., 0., 0.],\n        [0., 0., 1., 0.],\n        [0., 0., 0., 1.]])\nusing full -&gt; tensor([[5, 5, 5],\n        [5, 5, 5],\n        [5, 5, 5]])"
  },
  {
    "objectID": "ch/python/AI.html#tensor-shape",
    "href": "ch/python/AI.html#tensor-shape",
    "title": "Artificial Inteligence",
    "section": "",
    "text": "We are making a new tensor (x) and checking shape of it. We can use the shape of x or any other already created tensor to make new tensors of that shape.\n\nx = torch.tensor([[1,2,3],[5,6,7]])\nx\n\ntensor([[1, 2, 3],\n        [5, 6, 7]])\n\n\n\nx.shape\n\ntorch.Size([2, 3])\n\n\n\ntorch.empty_like(x)\n\ntensor([[0, 0, 0],\n        [0, 0, 0]])\n\n\n\ntorch.zeros_like(x)\n\ntensor([[0, 0, 0],\n        [0, 0, 0]])\n\n\n\ntorch.rand_like(x)\n\nRuntimeError: \"check_uniform_bounds\" not implemented for 'Long'\n\n\nIt’s not working, since rand makes float values in the tensor. So, we need to specify data type as float.\n\ntorch.rand_like(x, dtype=torch.float32)\n\ntensor([[0.7583, 0.8896, 0.6959],\n        [0.4810, 0.8545, 0.1130]])"
  },
  {
    "objectID": "ch/python/AI.html#tensor-data-types",
    "href": "ch/python/AI.html#tensor-data-types",
    "title": "Artificial Inteligence",
    "section": "",
    "text": "# find data type\nx.dtype\n\ntorch.int64\n\n\nWe are changing data type from float to int using dtype here.\n\n# assign data type\ntorch.tensor([1.0,2.0,3.0], dtype=torch.int32)\n\ntensor([1, 2, 3], dtype=torch.int32)\n\n\nSimilarly, from int to float using dtype here.\n\ntorch.tensor([1,2,3], dtype=torch.float64)\n\ntensor([1., 2., 3.], dtype=torch.float64)\n\n\n\n#using to()\nx.to(torch.float32)\n\ntensor([[1., 2., 3.],\n        [5., 6., 7.]])\n\n\nSome common data types in torch:\n\n\n\n\n\n\n\n\nData Type\nDtype\nDescription\n\n\n\n\n32-bit Floating Point\ntorch.float32\nStandard floating-point type used for most deep learning tasks. Provides a balance between precision and memory usage.\n\n\n64-bit Floating Point\ntorch.float64\nDouble-precision floating point. Useful for high-precision numerical tasks but uses more memory.\n\n\n16-bit Floating Point\ntorch.float16\nHalf-precision floating point. Commonly used in mixed-precision training to reduce memory and computational overhead on modern GPUs.\n\n\nBFloat16\ntorch.bfloat16\nBrain floating-point format with reduced precision compared to float16. Used in mixed-precision training, especially on TPUs.\n\n\n8-bit Floating Point\ntorch.float8\nUltra-low-precision floating point. Used for experimental applications and extreme memory-constrained environments (less common).\n\n\n8-bit Integer\ntorch.int8\n8-bit signed integer. Used for quantized models to save memory and computation in inference.\n\n\n16-bit Integer\ntorch.int16\n16-bit signed integer. Useful for special numerical tasks requiring intermediate precision.\n\n\n32-bit Integer\ntorch.int32\nStandard signed integer type. Commonly used for indexing and general-purpose numerical tasks.\n\n\n64-bit Integer\ntorch.int64\nLong integer type. Often used for large indexing arrays or for tasks involving large numbers.\n\n\n8-bit Unsigned Integer\ntorch.uint8\n8-bit unsigned integer. Commonly used for image data (e.g., pixel values between 0 and 255).\n\n\nBoolean\ntorch.bool\nBoolean type, stores True or False values. Often used for masks in logical operations.\n\n\nComplex 64\ntorch.complex64\nComplex number type with 32-bit real and 32-bit imaginary parts. Used for scientific and signal processing tasks.\n\n\nComplex 128\ntorch.complex128\nComplex number type with 64-bit real and 64-bit imaginary parts. Offers higher precision but uses more memory.\n\n\nQuantized Integer\ntorch.qint8\nQuantized signed 8-bit integer. Used in quantized models for efficient inference.\n\n\nQuantized Unsigned Integer\ntorch.quint8\nQuantized unsigned 8-bit integer. Often used for quantized tensors in image-related tasks."
  },
  {
    "objectID": "ch/python/AI.html#mathematical-operations",
    "href": "ch/python/AI.html#mathematical-operations",
    "title": "Artificial Inteligence",
    "section": "",
    "text": "Let’s define a tensor x first.\n\nx = torch.rand(2, 3)\nx\n\ntensor([[0.6779, 0.0173, 0.1203],\n        [0.1363, 0.8089, 0.8229]])\n\n\nNow, let’s see some scalar operation on this tensor.\n\n#addition\nx + 2\n#subtraction\nx - 3\n#multiplication\nx*4\n#division\nx/2\n#integer division\n(x*40)//3\n#modulus division\n((x*40)//3)%2\n#power\nx**2\n\ntensor([[4.5950e-01, 2.9987e-04, 1.4484e-02],\n        [1.8587e-02, 6.5435e-01, 6.7723e-01]])\n\n\n\n\n\nLet’s make 2 new tensors first. To do anything element-wise, the shape of the tensors should be the same.\n\na = torch.rand(2, 3)\nb = torch.rand(2, 3)\nprint(a)\nprint(b)\n\ntensor([[0.3759, 0.0295, 0.4132],\n        [0.0791, 0.0489, 0.9287]])\ntensor([[0.4924, 0.8416, 0.1756],\n        [0.5687, 0.4447, 0.0310]])\n\n\n\n#add\na + b\n#subtract\na - b\n#multiply\na*b\n#division\na/b\n#power\na**b\n#mod\na%b\n#int division\na//b\n\ntensor([[ 0.,  0.,  2.],\n        [ 0.,  0., 29.]])\n\n\nLet’s apply absolute function on a custom tensor.\n\n#abs\nc = torch.tensor([-1, 2, -3, 4, -5, -6, 7, -8])\ntorch.abs(c)\n\ntensor([1, 2, 3, 4, 5, 6, 7, 8])\n\n\nWe only have positive values, right? As expected.\nLet’s apply negative on the tensor.\n\ntorch.neg(c)\n\ntensor([ 1, -2,  3, -4,  5,  6, -7,  8])\n\n\nWe have negative signs on the previously positives, and positive signs on the previously negatives, right?\n\n#round\nd = torch.tensor([1.4, 4.4, 3.6, 3.01, 4.55, 4.9])\ntorch.round(d)\n# ceil\ntorch.ceil(d)\n# floor\ntorch.floor(d)\n\ntensor([1., 4., 3., 3., 4., 4.])\n\n\nDo you see what round, ciel, floor are doing here? It is not that difficult, try to see.\nLet’s do some clamping. So, if a value is smaller than the min value provided, that value will be equal to the min value and values bigger than the max value will be made equal to the max value. All other values in between the range will be kept as they are.\n\n# clamp\nd\ntorch.clamp(d, min=2, max=4)\n\ntensor([2.0000, 4.0000, 3.6000, 3.0100, 4.0000, 4.0000])\n\n\n\n\n\n\ne = torch.randint(size=(2,3), low=0, high=10, dtype=torch.float32)\ne\n\ntensor([[5., 1., 7.],\n        [7., 1., 5.]])\n\n\n\n# sum\ntorch.sum(e)\n# sum along columns\ntorch.sum(e, dim=0)\n# sum along rows\ntorch.sum(e, dim=1)\n# mean\ntorch.mean(e)\n# mean along col\ntorch.mean(e, dim=0)\n# mean along row\ntorch.mean(e, dim=1)\n# median\ntorch.median(e)\ntorch.median(e, dim=0)\ntorch.median(e, dim=1)\n\ntorch.return_types.median(\nvalues=tensor([5., 5.]),\nindices=tensor([0, 2]))\n\n\n\n# max and min\ntorch.max(e)\ntorch.max(e, dim=0)\ntorch.max(e, dim=1)\n\ntorch.min(e)\ntorch.min(e, dim=0)\ntorch.min(e, dim=1)\n\ntorch.return_types.min(\nvalues=tensor([1., 1.]),\nindices=tensor([1, 1]))\n\n\n\n# product\ntorch.prod(e)\n#do yourself dimension-wise\n\ntensor(1225.)\n\n\n\n# standard deviation\ntorch.std(e)\n#do yourself dimension-wise\n\ntensor(2.7325)\n\n\n\n# variance\ntorch.var(e)\n#do yourself dimension-wise\n\ntensor(7.4667)\n\n\nWhich value is the biggest here? How to get its position/index? Use argmax.\n\n# argmax\ntorch.argmax(e)\n\ntensor(2)\n\n\nWhich value is the smallest here? How to get its position/index? Use argmin.\n\n# argmin\ntorch.argmin(e)\n\ntensor(1)\n\n\n\n\n\n\nm1 = torch.randint(size=(2,3), low=0, high=10)\nm2 = torch.randint(size=(3,2), low=0, high=10)\n\nprint(m1)\nprint(m2)\n\ntensor([[8, 9, 1],\n        [2, 4, 5]])\ntensor([[6, 5],\n        [6, 2],\n        [0, 6]])\n\n\n\n# matrix multiplcation\ntorch.matmul(m1, m2)\n\ntensor([[102,  64],\n        [ 36,  48]])\n\n\n\n\n\n\nvector1 = torch.tensor([1, 2])\nvector2 = torch.tensor([3, 4])\n\n# dot product\ntorch.dot(vector1, vector2)\n\ntensor(11)\n\n\n\n# transpose\ntorch.transpose(m2, 0, 1)\n\ntensor([[6, 6, 0],\n        [5, 2, 6]])\n\n\n\nh = torch.randint(size=(3,3), low=0, high=8, dtype=torch.float32)\nh\n\ntensor([[7., 1., 3.],\n        [3., 2., 2.],\n        [7., 2., 4.]])\n\n\n\n# determinant\ntorch.det(h)\n\ntensor(6.0000)\n\n\n\n# inverse\ntorch.inverse(h)\n\ntensor([[ 0.6667,  0.3333, -0.6667],\n        [ 0.3333,  1.1667, -0.8333],\n        [-1.3333, -1.1667,  1.8333]])\n\n\n\n\n\n\ni = torch.randint(size=(2,3), low=0, high=10)\nj = torch.randint(size=(2,3), low=0, high=10)\n\nprint(i)\nprint(j)\n\ntensor([[1, 0, 1],\n        [7, 8, 9]])\ntensor([[1, 9, 7],\n        [4, 5, 9]])\n\n\n\n# greater than\ni &gt; j\n# less than\ni &lt; j\n# equal to\ni == j\n# not equal to\ni != j\n# greater than equal to\n\n# less than equal to\n\ntensor([[False,  True,  True],\n        [ True,  True, False]])\n\n\n\n\n\n\nk = torch.randint(size=(2,3), low=0, high=10, dtype=torch.float32)\nk\n\ntensor([[5., 8., 1.],\n        [3., 4., 4.]])\n\n\n\n# log\ntorch.log(k)\n\ntensor([[1.6094, 2.0794, 0.0000],\n        [1.0986, 1.3863, 1.3863]])\n\n\n\n# exp\ntorch.exp(k)\n\ntensor([[1.4841e+02, 2.9810e+03, 2.7183e+00],\n        [2.0086e+01, 5.4598e+01, 5.4598e+01]])\n\n\n\n# sqrt\ntorch.sqrt(k)\n\ntensor([[2.2361, 2.8284, 1.0000],\n        [1.7321, 2.0000, 2.0000]])\n\n\n\nk\n# sigmoid\ntorch.sigmoid(k)\n\ntensor([[0.9933, 0.9997, 0.7311],\n        [0.9526, 0.9820, 0.9820]])\n\n\n\nk\n# softmax\ntorch.softmax(k, dim=0)\n\ntensor([[0.8808, 0.9820, 0.0474],\n        [0.1192, 0.0180, 0.9526]])\n\n\n\n# relu\ntorch.relu(k)\n\ntensor([[5., 8., 1.],\n        [3., 4., 4.]])\n\n\n\n\n\n\nm = torch.rand(2,3)\nn = torch.rand(2,3)\n\nprint(m)\nprint(n)\n\ntensor([[0.2179, 0.5475, 0.4801],\n        [0.2278, 0.7175, 0.8381]])\ntensor([[0.2569, 0.9879, 0.0779],\n        [0.3233, 0.7714, 0.9524]])\n\n\n\nm.add_(n)\nm\nn\n\ntensor([[0.2569, 0.9879, 0.0779],\n        [0.3233, 0.7714, 0.9524]])\n\n\n\ntorch.relu(m)\n\ntensor([[0.4748, 1.5353, 0.5580],\n        [0.5511, 1.4889, 1.7905]])\n\n\n\nm.relu_()\nm\n\ntensor([[0.4748, 1.5353, 0.5580],\n        [0.5511, 1.4889, 1.7905]])\n\n\nCopying a Tensor\n\na = torch.rand(2,3)\na\n\ntensor([[0.1013, 0.2033, 0.2292],\n        [0.6055, 0.3249, 0.9225]])\n\n\n\nb = a\na\nb\n\ntensor([[0.1013, 0.2033, 0.2292],\n        [0.6055, 0.3249, 0.9225]])\n\n\n\na[0][0] = 0\na\n\ntensor([[0.0000, 0.2033, 0.2292],\n        [0.6055, 0.3249, 0.9225]])\n\n\n\nb\n\ntensor([[0.0000, 0.2033, 0.2292],\n        [0.6055, 0.3249, 0.9225]])\n\n\n\nid(a)\n\n4624181456\n\n\n\nid(b)\n\n4624181456\n\n\nBetter way of making a copy\n\nb = a.clone()\na\nb\n\ntensor([[0.0000, 0.2033, 0.2292],\n        [0.6055, 0.3249, 0.9225]])\n\n\n\na[0][0] = 10\na\n\ntensor([[10.0000,  0.2033,  0.2292],\n        [ 0.6055,  0.3249,  0.9225]])\n\n\n\nb\n\ntensor([[0.0000, 0.2033, 0.2292],\n        [0.6055, 0.3249, 0.9225]])\n\n\nNow, let’s check their memory locations. They are at different locations.\n\nid(a)\nid(b)\n\n4624182608"
  },
  {
    "objectID": "ch/python/AI.html#real-world-example",
    "href": "ch/python/AI.html#real-world-example",
    "title": "Artificial Inteligence",
    "section": "Real-world example:",
    "text": "Real-world example:\nLet’s say a student got CGPA 3.10 and did not get a placement in an institute. So, we can try to make a prediction.\n\nimport torch\n\n# Inputs\nx = torch.tensor(6.70)  # Input feature\ny = torch.tensor(0.0)  # True label (binary)\n\nw = torch.tensor(1.0)  # Weight\nb = torch.tensor(0.0)  # Bias\n\n\n# Binary Cross-Entropy Loss for scalar\ndef binary_cross_entropy_loss(prediction, target):\n    epsilon = 1e-8  # To prevent log(0)\n    prediction = torch.clamp(prediction, epsilon, 1 - epsilon)\n    return -(target * torch.log(prediction) + (1 - target) * torch.log(1 - prediction))\n\n\n# Forward pass\nz = w * x + b  # Weighted sum (linear part)\ny_pred = torch.sigmoid(z)  # Predicted probability\n\n# Compute binary cross-entropy loss\nloss = binary_cross_entropy_loss(y_pred, y)\n\n\n# Derivatives:\n# 1. dL/d(y_pred): Loss with respect to the prediction (y_pred)\ndloss_dy_pred = (y_pred - y)/(y_pred*(1-y_pred))\n\n# 2. dy_pred/dz: Prediction (y_pred) with respect to z (sigmoid derivative)\ndy_pred_dz = y_pred * (1 - y_pred)\n\n# 3. dz/dw and dz/db: z with respect to w and b\ndz_dw = x  # dz/dw = x\ndz_db = 1  # dz/db = 1 (bias contributes directly to z)\n\ndL_dw = dloss_dy_pred * dy_pred_dz * dz_dw\ndL_db = dloss_dy_pred * dy_pred_dz * dz_db\n\n\nprint(f\"Manual Gradient of loss w.r.t weight (dw): {dL_dw}\")\nprint(f\"Manual Gradient of loss w.r.t bias (db): {dL_db}\")\n\nManual Gradient of loss w.r.t weight (dw): 6.691762447357178\nManual Gradient of loss w.r.t bias (db): 0.998770534992218\n\n\nBut let’s use our friend again.\n\nx = torch.tensor(6.7)\ny = torch.tensor(0.0)\n\n\nw = torch.tensor(1.0, requires_grad=True)\nb = torch.tensor(0.0, requires_grad=True)\nw\nb\n\ntensor(0., requires_grad=True)\n\n\n\nz = w*x + b\nz\ny_pred = torch.sigmoid(z)\ny_pred\nloss = binary_cross_entropy_loss(y_pred, y)\nloss\n\ntensor(6.7012, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nloss.backward()\n\n\nprint(w.grad)\nprint(b.grad)\n\ntensor(6.6918)\ntensor(0.9988)\n\n\nLet’s insert multiple values (or a vector).\n\nx = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\nx\n\ntensor([1., 2., 3.], requires_grad=True)\n\n\n\ny = (x**2).mean()\ny\n\ntensor(4.6667, grad_fn=&lt;MeanBackward0&gt;)\n\n\n\ny.backward()\nx.grad\n\ntensor([0.6667, 1.3333, 2.0000])\n\n\nIf we rerun all these things, the values get updtaed. So, we need to stop this behavior. How to do it?\n\n# clearing grad\nx = torch.tensor(2.0, requires_grad=True)\nx\n\ntensor(2., requires_grad=True)\n\n\n\ny = x ** 2\ny\n\ntensor(4., grad_fn=&lt;PowBackward0&gt;)\n\n\n\ny.backward()\n\n\nx.grad\n\ntensor(4.)\n\n\n\nx.grad.zero_()\n\ntensor(0.)\n\n\nNow, we don’t see requires_grad=True part here. So, it is off. Another way:\n\n# option 1 - requires_grad_(False)\n# option 2 - detach()\n# option 3 - torch.no_grad()\n\n\nx = torch.tensor(2.0, requires_grad=True)\nx\nx.requires_grad_(False)\n\ntensor(2.)\n\n\n\ny = x ** 2\ny\n\ntensor(4.)\n\n\n\n#not possible now\ny.backward()\n\nRuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n\n\n\nx = torch.tensor(2.0, requires_grad=True)\nx\n\ntensor(2., requires_grad=True)\n\n\n\nz = x.detach()\nz\n\ntensor(2.)\n\n\n\ny = x ** 2\ny\n\ntensor(4., grad_fn=&lt;PowBackward0&gt;)\n\n\n\ny1 = z ** 2\ny1\n\ntensor(4.)\n\n\n\ny.backward() #possible\n\n\ny1.backward() #not possible\n\nRuntimeError: element 0 of tensors does not require grad and does not have a grad_fn"
  },
  {
    "objectID": "ch/python/AI.html#train-test-split",
    "href": "ch/python/AI.html#train-test-split",
    "title": "Artificial Inteligence",
    "section": "Train test split",
    "text": "Train test split\n\nX_train, X_test, y_train, y_test = train_test_split(df.iloc[:, 1:], df.iloc[:, 0], test_size=0.2)"
  },
  {
    "objectID": "ch/python/AI.html#scaling",
    "href": "ch/python/AI.html#scaling",
    "title": "Artificial Inteligence",
    "section": "Scaling",
    "text": "Scaling\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n\nX_train\n\narray([[ 1.77838216,  0.33663923,  1.74213583, ...,  0.74198987,\n         0.54222412, -1.11360554],\n       [ 1.61664581,  0.24142272,  1.57193275, ...,  0.74198987,\n        -0.55309099,  0.40465846],\n       [ 0.16380724,  0.21123212,  0.12277511, ...,  0.73584429,\n         0.23044292, -0.10326986],\n       ...,\n       [-0.0118719 ,  1.84152454, -0.01338735, ..., -0.12607243,\n        -1.04116733, -0.04529978],\n       [-1.07152384, -0.70609766, -1.02082747, ..., -0.23915099,\n        -0.45678162,  0.7171448 ],\n       [ 1.77001649,  0.59442051,  1.70566374, ...,  0.97552167,\n        -0.45514925, -0.92092404]], shape=(455, 30))\n\n\n\ny_train\n\n533    M\n517    M\n16     M\n101    B\n109    B\n      ..\n207    M\n419    B\n560    B\n320    B\n365    M\nName: diagnosis, Length: 455, dtype: object"
  },
  {
    "objectID": "ch/python/AI.html#label-encoding",
    "href": "ch/python/AI.html#label-encoding",
    "title": "Artificial Inteligence",
    "section": "Label Encoding",
    "text": "Label Encoding\n\nencoder = LabelEncoder()\ny_train = encoder.fit_transform(y_train)\ny_test = encoder.transform(y_test)\n\n\ny_train\n\narray([1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,\n       1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n       1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n       0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n       0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n       0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,\n       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n       0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n       0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,\n       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n       1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n       0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,\n       1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,\n       0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n       1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n       0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1])"
  },
  {
    "objectID": "ch/python/AI.html#numpy-arrays-to-pytorch-tensors",
    "href": "ch/python/AI.html#numpy-arrays-to-pytorch-tensors",
    "title": "Artificial Inteligence",
    "section": "Numpy arrays to PyTorch tensors",
    "text": "Numpy arrays to PyTorch tensors\n\nX_train_tensor = torch.from_numpy(X_train)\nX_test_tensor = torch.from_numpy(X_test)\ny_train_tensor = torch.from_numpy(y_train)\ny_test_tensor = torch.from_numpy(y_test)\n\n\nX_train_tensor.shape\n\ntorch.Size([455, 30])\n\n\n\ny_train_tensor.shape\n\ntorch.Size([455])"
  },
  {
    "objectID": "ch/python/AI.html#defining-the-model",
    "href": "ch/python/AI.html#defining-the-model",
    "title": "Artificial Inteligence",
    "section": "Defining the model",
    "text": "Defining the model\n\nclass MySimpleNN():\n\n  def __init__(self, X):\n\n    self.weights = torch.rand(X.shape[1], 1, dtype=torch.float64, requires_grad=True)\n    self.bias = torch.zeros(1, dtype=torch.float64, requires_grad=True)\n\n  def forward(self, X):\n    z = torch.matmul(X, self.weights) + self.bias\n    y_pred = torch.sigmoid(z)\n    return y_pred\n\n  def loss_function(self, y_pred, y):\n    # Clamp predictions to avoid log(0)\n    epsilon = 1e-7\n    y_pred = torch.clamp(y_pred, epsilon, 1 - epsilon)\n\n    # Calculate loss\n    loss = -(y_train_tensor * torch.log(y_pred) + (1 - y_train_tensor) * torch.log(1 - y_pred)).mean()\n    return loss"
  },
  {
    "objectID": "ch/python/AI.html#important-parameters",
    "href": "ch/python/AI.html#important-parameters",
    "title": "Artificial Inteligence",
    "section": "Important Parameters",
    "text": "Important Parameters\n\nlearning_rate = 0.1\nepochs = 25"
  },
  {
    "objectID": "ch/python/AI.html#training-pipeline",
    "href": "ch/python/AI.html#training-pipeline",
    "title": "Artificial Inteligence",
    "section": "Training Pipeline",
    "text": "Training Pipeline\n\n# create model\nmodel = MySimpleNN(X_train_tensor)\n\n# define loop\nfor epoch in range(epochs):\n\n  # forward pass\n  y_pred = model.forward(X_train_tensor)\n\n  # loss calculate\n  loss = model.loss_function(y_pred, y_train_tensor)\n\n  # backward pass\n  loss.backward()\n\n  # parameters update\n  with torch.no_grad():\n    model.weights -= learning_rate * model.weights.grad\n    model.bias -= learning_rate * model.bias.grad\n\n  # zero gradients\n  model.weights.grad.zero_()\n  model.bias.grad.zero_()\n\n  # print loss in each epoch\n  print(f'Epoch: {epoch + 1}, Loss: {loss.item()}')\n\nEpoch: 1, Loss: 3.968025963705283\nEpoch: 2, Loss: 3.864118509120392\nEpoch: 3, Loss: 3.7541330076509594\nEpoch: 4, Loss: 3.638331163567124\nEpoch: 5, Loss: 3.5174341902058233\nEpoch: 6, Loss: 3.3933643569290846\nEpoch: 7, Loss: 3.2621320961969693\nEpoch: 8, Loss: 3.1262147737756134\nEpoch: 9, Loss: 2.988698930957073\nEpoch: 10, Loss: 2.846942763920921\nEpoch: 11, Loss: 2.6958299495752183\nEpoch: 12, Loss: 2.5419935910597404\nEpoch: 13, Loss: 2.3831413982619805\nEpoch: 14, Loss: 2.2235735410390314\nEpoch: 15, Loss: 2.066447122377066\nEpoch: 16, Loss: 1.910431766057619\nEpoch: 17, Loss: 1.7594278162011676\nEpoch: 18, Loss: 1.6166291902936634\nEpoch: 19, Loss: 1.4746437685958356\nEpoch: 20, Loss: 1.3437662136173296\nEpoch: 21, Loss: 1.2277088323530687\nEpoch: 22, Loss: 1.1276737971797886\nEpoch: 23, Loss: 1.0442162895988343\nEpoch: 24, Loss: 0.9769917653527498\nEpoch: 25, Loss: 0.9246446252633511\n\n\n\nmodel.bias\n\ntensor([-0.1058], dtype=torch.float64, requires_grad=True)\n\n\n\nmodel.weights\n\ntensor([[ 0.2628],\n        [ 0.0059],\n        [ 0.4382],\n        [ 0.2972],\n        [ 0.0064],\n        [-0.6488],\n        [-0.3415],\n        [ 0.0583],\n        [ 0.0673],\n        [ 0.6336],\n        [-0.0643],\n        [ 0.4676],\n        [ 0.1455],\n        [ 0.3115],\n        [ 0.1001],\n        [-0.0605],\n        [-0.2559],\n        [ 0.3491],\n        [ 0.6941],\n        [-0.0211],\n        [ 0.3778],\n        [ 0.2139],\n        [-0.5063],\n        [ 0.0311],\n        [ 0.3313],\n        [-0.4985],\n        [ 0.2924],\n        [-0.0689],\n        [ 0.0554],\n        [ 0.3736]], dtype=torch.float64, requires_grad=True)"
  },
  {
    "objectID": "ch/python/AI.html#evaluation",
    "href": "ch/python/AI.html#evaluation",
    "title": "Artificial Inteligence",
    "section": "Evaluation",
    "text": "Evaluation\n\n# model evaluation\nwith torch.no_grad():\n  y_pred = model.forward(X_test_tensor)\n  y_pred = (y_pred &gt; 0.9).float()\n  accuracy = (y_pred == y_test_tensor).float().mean()\n  print(f'Accuracy: {accuracy.item()}')\n\nAccuracy: 0.586334228515625"
  },
  {
    "objectID": "ch/rbasics/git.html#what-if-your-cloned-repo-is-out-of-date-new-changes-happened",
    "href": "ch/rbasics/git.html#what-if-your-cloned-repo-is-out-of-date-new-changes-happened",
    "title": "Git and CLI",
    "section": "What if your cloned repo is out of date (new changes happened)?",
    "text": "What if your cloned repo is out of date (new changes happened)?\nRun this to pull the up to date repo.\n\ngit pull origin main\n\n(Use master if that’s your primary branch name.)\nGit will then download any new commits and files from GitHub and integrate them into your local branch. If there are no new changes, it will simply tell you “Already up to date.”\nImportant: It’s a good habit to git pull before you start working on a project each day, or before you begin a new set of changes, to ensure you’re always working with the most current version. This helps avoid potential merge conflicts later on."
  },
  {
    "objectID": "ch/rbasics/git.html#how-to-make-changes-directly-on-github-and-then-pull-them-and-then-well-get-into-the-exciting-world-of-branches-and-collaboration",
    "href": "ch/rbasics/git.html#how-to-make-changes-directly-on-github-and-then-pull-them-and-then-well-get-into-the-exciting-world-of-branches-and-collaboration",
    "title": "Git and CLI",
    "section": "How to make changes directly on GitHub and then pull them, and then we’ll get into the exciting world of branches and collaboration!",
    "text": "How to make changes directly on GitHub and then pull them, and then we’ll get into the exciting world of branches and collaboration!\nDo these 1. Navigate to your repository on GitHub.com 2. Edit the file 3. Commit the changes 4. Pull the changes to your local machine\n\nOpen your terminal or Git Bash.\nNavigate to your local repository folder using cd your-repo-name.\n\nRun the git pull command:\n\ngit pull origin main\n\n(Again, use master if that’s your primary branch name.)"
  },
  {
    "objectID": "ch/rbasics/git.html#collaborating-with-others-branches-and-pull-requests",
    "href": "ch/rbasics/git.html#collaborating-with-others-branches-and-pull-requests",
    "title": "Git and CLI",
    "section": "Collaborating with Others: Branches and Pull Requests",
    "text": "Collaborating with Others: Branches and Pull Requests\nNow for some really powerful stuff! So far, we’ve been working on a single line of development (the main or master branch). But what if you’re working with a team, or you want to experiment with new features without breaking the main, working version of your project? This is where branches come in.\n1. Understanding Branches\nThink of a branch as an independent line of development. When you create a branch, you’re essentially making a copy of your project at a specific point in time. You can then make changes on this new branch without affecting the original (main) branch.\n\n\nmain (or master) branch: This is typically considered the stable, production-ready version of your project.\nFeature branches: You create these for new features, bug fixes, or experiments. You work on them in isolation. Once you’re done with your work on a feature branch and it’s stable, you can merge it back into the main branch.\n\nWhy use branches?\n\nIsolation: Work on new features without impacting the stable code.\nCollaboration: Multiple developers can work on different features simultaneously without stepping on each others toes.\nExperimentation: Try out new ideas without fear of breaking the main project.\n2. Creating and Switching Branches Locally\nLet’s practice creating a new branch.\nScenario: You want to add a new “Contact Us” page to your well set website (code), but you don’t want to touch the live code until it’s ready.\nSteps:\n\nEnsure you’re on the main branch and up to date:\n\n\ngit checkout main\ngit pull origin main\n\ngit checkout main ensures you’re on the main branch. git pull ensures your main branch has all the latest changes from GitHub.\n\nCreate a new branch:\n\n\ngit branch feature/contact-page\n\nIt’s common to name branches descriptively, often starting with feature/, bugfix/, hotfix/, etc. Name them meaningfully as you need them for.\nThis command creates the branch but doesn’t switch you to it yet.\n\nSwitch to your new branch:\n\n\ngit checkout feature/contact-page\n\nYou’ll see a message like “Switched to branch ‘feature/contact-page’”.\nPro Tip: You can combine steps 2 and 3 into one command:\n\ngit checkout -b feature/contact-page\n\n\nVerify your current branch:\n\n\ngit branch\n\nThe branch you’re currently on will have an asterisk (*) next to its name.\nNow you are on your feature/contact-page branch. Any changes you make, commit, and push will be on this branch, completely separate from main.\n\ngit status\ngit add .\ngit commit -m \"created contatc.txt file in the feature branch\"\ngit push origin feature/contact-page\n\nOnce you’ve finished your work on a branch and thoroughly tested it, the next step is to merge it back into the main branch.\n3. Merging a Branch (Bringing Your Changes Back)\nMerging is the process of integrating the changes from one branch (like your feature/contact-page branch) into another branch (typically main).\nScenario: You’ve added your “Contact Us” page, it’s working perfectly, and you’re ready to make it part of the main website (main branch).\nSteps:\n1. Ensure you’re on the main branch and up to date:\n\ngit checkout main\ngit pull origin main\n\nThis is crucial! You want to merge your changes into the latest version of main.\n\nMerge your feature branch into main:\n\n\ngit merge feature/contact-page\n\nThis command tells Git to take all the changes from the feature/contact-page branch and apply them to the main branch.\nAutomatic Merging: If Git can figure out how to combine the changes without any conflicts, it will do so automatically. You might see a message like “Successfully merged.”\nMerge Conflicts: Sometimes, if both branches have changed the same lines in the same file, Git won’t know how to proceed. This results in a merge conflict. We’ll cover how to resolve these in a moment.\n\n(If the merge was successful) Push the merged main branch to GitHub:\n\n\ngit push origin main\n\nThis uploads the merged changes to the remote main branch on GitHub.\n4. Deleting the Feature Branch (Optional but Recommended)\nDelete the branch locally: First, ensure you’re not on the branch you want to delete. Switch back to main:\n\ngit checkout main\n\nThen, delete the local branch:\n\ngit branch -d feature/contact-page\n\n\nThe -d flag is for “delete.” Git will prevent deletion if the branch has unmerged changes.\nIf you really want to delete a branch with unmerged changes, for whatever reason, (use with caution!), you can use the force delete flag: git branch -D feature/contact-page (capital D).\n\nDealing with Merge Conflicts Merge conflicts happen when Git can’t automatically reconcile changes between branches. It’s not a disaster! It just means you need to manually tell Git how to combine the changes.\nScenario: You and a collaborator both edited the same section of a file on different branches.\nWhat you’ll see:\n\nGit will add special markers to the conflicting file, showing the changes from both branches. It will look something like this:\n\n\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n// Your changes on the main branch\n=======\n// Your collaborator's changes on the feature branch\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; feature/contact-page\n\n\n\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD: Marks the beginning of the changes on your current branch (main).\n\n=======: Separates your changes from the changes on the other branch.\n\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; feature/contact-page: Marks the end of the changes on the feature/contact-page branch.\n\nHow to resolve a conflict:\n\nOpen the conflicting file in a text editor: You’ll see the conflict markers.\nManually edit the file: Decide how you want to combine the changes. You might:\n\n\nKeep only your changes.\nKeep only the other branch’s changes.\nCombine parts of both.\nRewrite the section entirely.\nRemove the conflict markers: Delete the &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD, =======, and &gt;&gt;&gt;&gt;&gt;&gt;&gt; feature/contact-page lines.\n\n\nSave the file.\nStage the resolved file:\n\n\ngit add your-conflicting-file.txt\n\n\nCommit the merge:\n\n\ngit commit -m \"Resolve merge conflict in your-conflicting-file.txt\"\n\n\nPush the merged branch:\n\n\ngit push origin main\n\nNow the conflict is resolved, and your changes are safely merged!\n5. Pull Requests (PRs): The Heart of Collaboration\nWhile you can merge branches locally, in a collaborative environment, the most common way to integrate changes is through a Pull Request (PR) on GitHub. A Pull Request is essentially a formal proposal to merge your changes from a feature branch into another branch (usually main).\nWhy use Pull Requests?\n\nCode Review: It allows other team members to review your code, provide feedback, and suggest improvements before the changes are merged into the main codebase. This catches bugs and ensures code quality.\nDiscussion: PRs provide a dedicated space for discussion about the changes.\nAutomated Checks: GitHub can be configured to run automated tests or checks on PRs, ensuring the new code doesn’t break existing functionality.\nRecord Keeping: PRs create a clear history of how and why changes were integrated.\n\nScenario: You’ve completed your tasks for feature/contact-page and pushed it to GitHub. Now you want your team to review it and eventually merge it into main.\nSteps to create a Pull Request:\n1. At first, pull the repo to your local machine and make changes:\n\ngit clone git@github.com:yourusername/repoLink.git\ngit checkout main\ngit checkout -b feature/contact-page\n#you can name your branch with something else depending on your task \n#Now make changes (files) or do whatever you want to do with coding\n#Then push the changes\ngit status\ngit add .\ngit commit -m \"My contribution\"\ngit push origin feature/contact-page\n\nBefore you can create a PR, your branch needs to exist on GitHub. If you just created it locally and committed changes, you’ll need to push it up.\nThe first time you push a new branch, Git might give you a hint like: git push --set-upstream origin feature/contact-page Just copy and paste that command if you see it.\n2. Go to your repository on GitHub.com: Once your branch is pushed, GitHub will often display a banner at the top of your repository page, suggesting you “Compare & pull request” for your newly pushed branch. If not, click on the “Pull requests” tab.\n3. Click “New pull request”: If the banner isn’t there, click the green “New pull request” button.\n4.Select your branches:\n\nBase branch: This is the branch you want to merge into (e.g., main).\nCompare branch: This is your feature branch that contains the changes (e.g., feature/contact-page).\n\nGitHub will show you a comparison of the changes between the two branches. Review them to ensure they’re what you expect.\n5. Create the Pull Request:\n\nProvide a clear and concise title for your PR (e.g., “Add Contact Us Page”).\nWrite a detailed description explaining what changes you made, why you made them, any potential issues, or how to test them. This is crucial for reviewers!\n\nYou can also:\n\nAssign reviewers: Choose team members who need to review your code.\n\nLink issues: Connect your PR to any relevant GitHub issues.\n\nAdd labels: Categorize your PR (e.g., feature, bug).\n\n\n\nClick the green “Create pull request” button.\n\nReviewing and Merging a Pull Request\n\nOnce a Pull Request is created, the collaborative process begins!\nAs a Reviewer:\n\nReview Changes: Look at the “Files changed” tab to see the code modifications.\nLeave Comments: You can click on specific lines of code to add comments, ask questions, or suggest alternative solutions.\nApprove/Request Changes: Once you’ve reviewed, you can either “Approve” the PR, “Request changes” (if modifications are needed), or simply “Comment” without a formal decision.\n\nAs the Author (or team lead):\n\nAddress Feedback: Respond to comments and make any necessary changes to your code. If you make new commits on your feature branch locally and git push origin feature/contact-page, those new commits will automatically appear in your open Pull Request on GitHub.\nMerge the Pull Request: Once all feedback is addressed, required checks pass, and the PR is approved, anyone with merge permissions can click the “Merge pull request” button on GitHub.\n\nGitHub offers different merge strategies:\n\n\nMerge commit: Creates a new commit that records the merge.\n\nSquash and merge: Combines all your feature branch commits into a single new commit on main. Great for keeping main’s history clean.\n\nRebase and merge: Reapplies your feature branch commits on top of main, creating a linear history.\n\n\n\n\n\n\nDeleting the Feature Branch (Optional but Recommended)\n\nAfter your feature branch has been successfully merged into main and pushed to GitHub, you typically don’t need the feature branch anymore. Deleting it keeps your repository tidy.\n\nDelete the branch on GitHub: After merging a PR, GitHub usually gives you an option to “Delete branch.” Click that.\nDelete the branch locally: First, ensure you’re not on the branch you want to delete. Switch back to main:\n\n\ngit checkout main\n\nThen, delete the local branch:\n\ngit branch -d feature/contact-page\n\nWe are all set to be a full stack developer!"
  },
  {
    "objectID": "ch/rbasics/git.html#merging-a-branch-bringing-your-changes-back",
    "href": "ch/rbasics/git.html#merging-a-branch-bringing-your-changes-back",
    "title": "Git and CLI",
    "section": "3. Merging a Branch (Bringing Your Changes Back)",
    "text": "3. Merging a Branch (Bringing Your Changes Back)\nMerging is the process of integrating the changes from one branch (like your feature/contact-page branch) into another branch (typically main).\nScenario: You’ve added your “Contact Us” page, it’s working perfectly, and you’re ready to make it part of the main website.\nSteps: 1. Ensure you’re on the main branch and up to date:\n\ngit checkout main\ngit pull origin main\n\nThis is crucial! You want to merge your changes into the latest version of main.\n\nMerge your feature branch into main:\n\n\ngit merge feature/contact-page\n\nThis command tells Git to take all the changes from the feature/contact-page branch and apply them to the main branch.\nAutomatic Merging: If Git can figure out how to combine the changes without any conflicts, it will do so automatically. You might see a message like “Successfully merged.”\nMerge Conflicts: Sometimes, if both branches have changed the same lines in the same file, Git won’t know how to proceed. This results in a merge conflict. We’ll cover how to resolve these in a moment.\n\n(If the merge was successful) Push the merged main branch to GitHub:\n\n\ngit push origin main\n\nThis uploads the merged changes to the remote main branch on GitHub.\n\nDeleting the Feature Branch (Optional but Recommended) Delete the branch locally: First, ensure you’re not on the branch you want to delete. Switch back to main:\n\n\ngit checkout main\n\nThen, delete the local branch:\n\ngit branch -d feature/contact-page\n\n\nThe -d flag is for “delete.” Git will prevent deletion if the branch has unmerged changes.\nIf you really want to delete a branch with unmerged changes (use with caution!), you can use the force delete flag: git branch -D feature/contact-page (capital D).\n\n\nDealing with Merge Conflicts Merge conflicts happen when Git can’t automatically reconcile changes between branches. It’s not a disaster! It just means you need to manually tell Git how to combine the changes.\n\nScenario: You and a collaborator both edited the same section of a file on different branches.\nWhat you’ll see:\n\nGit will add special markers to the conflicting file, showing the changes from both branches. It will look something like this:\n\n\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n// Your changes on the main branch\n=======\n// Your collaborator's changes on the feature branch\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; feature/contact-page\n\n\n\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD: Marks the beginning of the changes on your current branch (main).\n\n=======: Separates your changes from the changes on the other branch.\n\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; feature/contact-page: Marks the end of the changes on the feature/contact-page branch.\n\nHow to resolve a conflict:\n\nOpen the conflicting file in a text editor: You’ll see the conflict markers.\nManually edit the file: Decide how you want to combine the changes. You might:\n\n\nKeep only your changes.\nKeep only the other branch’s changes.\nCombine parts of both.\nRewrite the section entirely.\nRemove the conflict markers: Delete the &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD, =======, and &gt;&gt;&gt;&gt;&gt;&gt;&gt; feature/contact-page lines.\n\n\nSave the file.\nStage the resolved file:\n\n\ngit add your-conflicting-file.txt\n\n\nCommit the merge:\n\n\ngit commit -m \"Resolve merge conflict in your-conflicting-file.txt\"\n\n\nPush the merged branch:\n\n\ngit push origin main\n\nNow the conflict is resolved, and your changes are safely merged!"
  },
  {
    "objectID": "ch/rbasics/git.html#pull-requests-prs-the-heart-of-collaboration",
    "href": "ch/rbasics/git.html#pull-requests-prs-the-heart-of-collaboration",
    "title": "Git and CLI",
    "section": "5. Pull Requests (PRs): The Heart of Collaboration",
    "text": "5. Pull Requests (PRs): The Heart of Collaboration\nWhile you can merge branches locally, in a collaborative environment, the most common way to integrate changes is through a Pull Request (PR) on GitHub. A Pull Request is essentially a formal proposal to merge your changes from a feature branch into another branch (usually main).\nWhy use Pull Requests?\n\nCode Review: It allows other team members to review your code, provide feedback, and suggest improvements before the changes are merged into the main codebase. This catches bugs and ensures code quality.\nDiscussion: PRs provide a dedicated space for discussion about the changes.\nAutomated Checks: GitHub can be configured to run automated tests or checks on PRs, ensuring the new code doesn’t break existing functionality.\nRecord Keeping: PRs create a clear history of how and why changes were integrated.\n\nScenario: You’ve completed your feature/contact-page and pushed it to GitHub. Now you want your team to review it and eventually merge it into main.\nSteps to create a Pull Request: 1. At first, pull the repo to your local machine and make changes:\n\ngit clone git@github.com:yourusername/repoLink.git\ngit checkout main\ngit checkout -b feature/contact-page\n#you can name your branch with something else depending on your task \n#Now make changes (files) or do whatever you want to do with coding\n#Then push the changes\ngit status\ngit add .\ngit commit -m \"My contribution\"\ngit push origin feature/contact-page\n\nBefore you can create a PR, your branch needs to exist on GitHub. If you just created it locally and committed changes, you’ll need to push it up.\nThe first time you push a new branch, Git might give you a hint like: git push --set-upstream origin feature/contact-page Just copy and paste that command if you see it.\n\nGo to your repository on GitHub.com: Once your branch is pushed, GitHub will often display a banner at the top of your repository page, suggesting you “Compare & pull request” for your newly pushed branch. If not, click on the “Pull requests” tab.\nClick “New pull request”: If the banner isn’t there, click the green “New pull request” button.\n\n4.Select your branches:\n\nBase branch: This is the branch you want to merge into (e.g., main).\nCompare branch: This is your feature branch that contains the changes (e.g., feature/contact-page). GitHub will show you a comparison of the changes between the two branches. Review them to ensure they’re what you expect.\n\n\nCreate the Pull Request:\n\n\nProvide a clear and concise title for your PR (e.g., “Add Contact Us Page”).\nWrite a detailed description explaining what changes you made, why you made them, any potential issues, or how to test them. This is crucial for reviewers!\nYou can also:\n\nAssign reviewers: Choose team members who need to review your code.\nLink issues: Connect your PR to any relevant GitHub issues.\nAdd labels: Categorize your PR (e.g., feature, bug). Click the green “Create pull request” button.\n\n\n\n\nReviewing and Merging a Pull Request\n\nOnce a Pull Request is created, the collaborative process begins!\nAs a Reviewer:\n\nReview Changes: Look at the “Files changed” tab to see the code modifications.\nLeave Comments: You can click on specific lines of code to add comments, ask questions, or suggest alternative solutions.\nApprove/Request Changes: Once you’ve reviewed, you can either “Approve” the PR, “Request changes” (if modifications are needed), or simply “Comment” without a formal decision.\n\nAs the Author (or team lead):\n\nAddress Feedback: Respond to comments and make any necessary changes to your code. If you make new commits on your feature branch locally and git push origin feature/contact-page, those new commits will automatically appear in your open Pull Request on GitHub.\nMerge the Pull Request: Once all feedback is addressed, required checks pass, and the PR is approved, anyone with merge permissions can click the “Merge pull request” button on GitHub.\n\nGitHub offers different merge strategies:\n\n\nMerge commit: Creates a new commit that records the merge.\n\nSquash and merge: Combines all your feature branch commits into a single new commit on main. Great for keeping main’s history clean.\n\nRebase and merge: Reapplies your feature branch commits on top of main, creating a linear history.\n\n\n\n\n\n\nDeleting the Feature Branch (Optional but Recommended)\n\nAfter your feature branch has been successfully merged into main and pushed to GitHub, you typically don’t need the feature branch anymore. Deleting it keeps your repository tidy.\n\nDelete the branch on GitHub: After merging a PR, GitHub usually gives you an option to “Delete branch.” Click that.\nDelete the branch locally: First, ensure you’re not on the branch you want to delete. Switch back to main:\n\n\ngit checkout main\n\nThen, delete the local branch:\n\ngit branch -d feature/contact-page\n\nWe are all set to be a full stack developer."
  },
  {
    "objectID": "ch/rbasics/git.html#how-to-make-changes-directly-on-github-and-then-pull-them",
    "href": "ch/rbasics/git.html#how-to-make-changes-directly-on-github-and-then-pull-them",
    "title": "Git and CLI",
    "section": "How to make changes directly on GitHub and then pull them",
    "text": "How to make changes directly on GitHub and then pull them\nN.B. After this discussion, we’ll get into the exciting world of branches and collaboration!\nDo these\n1. Navigate to your repository on GitHub.com 2. Edit the file\n3. Commit the changes\n4. Pull the changes to your local machine\n\nOpen your terminal or Git Bash.\nNavigate to your local repository folder using cd your-repo-name.\n\nRun the git pull command:\n\ngit pull origin main\n\n(Again, use master if that’s your primary branch name.)"
  },
  {
    "objectID": "ch/rbasics/tidyverse.html#installing-tidyverse",
    "href": "ch/rbasics/tidyverse.html#installing-tidyverse",
    "title": "The tidyverse",
    "section": "",
    "text": "Before we can use the tidyverse, we need to install it. First, let’s install a helpful package manager called pacman:\n\n# Install pacman if you haven't already\n#install.packages(\"pacman\")\n\n# Load pacman\nlibrary(pacman)\n\n# Now use pacman to install and load tidyverse\n#pacman::p_load(tidyverse)\n# (install &) load packages\npacman::p_load(\n  broom,\n  conflicted,\n  here,\n  janitor,\n  naniar,\n  readxl,\n  tibble,\n  tidyverse\n)\n\n# Alternative: traditional installation\n# install.packages(\"tidyverse\")\n# library(tidyverse)\nconflicts_prefer(dplyr::filter) \nconflicts_prefer(dplyr::select)\n#dplyr::select()"
  },
  {
    "objectID": "ch/rbasics/tidyverse.html#core-tidyverse-packages",
    "href": "ch/rbasics/tidyverse.html#core-tidyverse-packages",
    "title": "The tidyverse",
    "section": "",
    "text": "Here are the main packages you’ll use most often:\n\n\nPackage\nPurpose\n\n\n\n\n ggplot2\n\nCreating beautiful graphs\n\n\n\n dplyr\n\nData manipulation\n\n\n\n tibble\n\nModern data frames\n\n\n\n tidyr\n\nTidying data\n\n\n\n readr\n\nReading data files"
  },
  {
    "objectID": "ch/rbasics/tidyverse.html#base-r-data.frame",
    "href": "ch/rbasics/tidyverse.html#base-r-data.frame",
    "title": "The tidyverse",
    "section": "Base R: data.frame",
    "text": "Base R: data.frame\nIn base R, we work with data.frame objects. Let’s look at a built-in dataset:\n\n# Base R approach\n# Load the built-in PlantGrowth dataset\ndata(PlantGrowth)\n#data(iris)\n\n# Create a copy to work with\ndf &lt;- PlantGrowth\n\n# View the first few rows\nhead(df)\n\n  weight group\n1   4.17  ctrl\n2   5.58  ctrl\n3   5.18  ctrl\n4   6.11  ctrl\n5   4.50  ctrl\n6   4.61  ctrl\n\n# Check the structure\nstr(df)\n\n'data.frame':   30 obs. of  2 variables:\n $ weight: num  4.17 5.58 5.18 6.11 4.5 4.61 5.17 4.53 5.33 5.14 ...\n $ group : Factor w/ 3 levels \"ctrl\",\"trt1\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n# Get summary statistics\nsummary(df)\n\n     weight       group   \n Min.   :3.590   ctrl:10  \n 1st Qu.:4.550   trt1:10  \n Median :5.155   trt2:10  \n Mean   :5.073            \n 3rd Qu.:5.530            \n Max.   :6.310            \n\n\nAccessing columns in Base R:\n\n# Method 1: Using $ notation\ndf$weight\n\n [1] 4.17 5.58 5.18 6.11 4.50 4.61 5.17 4.53 5.33 5.14 4.81 4.17 4.41 3.59 5.87\n[16] 3.83 6.03 4.89 4.32 4.69 6.31 5.12 5.54 5.50 5.37 5.29 4.92 6.15 5.80 5.26\n\n# Method 2: Using brackets with column name\ndf[, \"weight\"]\n\n [1] 4.17 5.58 5.18 6.11 4.50 4.61 5.17 4.53 5.33 5.14 4.81 4.17 4.41 3.59 5.87\n[16] 3.83 6.03 4.89 4.32 4.69 6.31 5.12 5.54 5.50 5.37 5.29 4.92 6.15 5.80 5.26\n\n# Method 3: Using brackets with column number\ndf[, 1]\n\n [1] 4.17 5.58 5.18 6.11 4.50 4.61 5.17 4.53 5.33 5.14 4.81 4.17 4.41 3.59 5.87\n[16] 3.83 6.03 4.89 4.32 4.69 6.31 5.12 5.54 5.50 5.37 5.29 4.92 6.15 5.80 5.26"
  },
  {
    "objectID": "ch/rbasics/tidyverse.html#tidyverse-tibble",
    "href": "ch/rbasics/tidyverse.html#tidyverse-tibble",
    "title": "The tidyverse",
    "section": "Tidyverse: tibble",
    "text": "Tidyverse: tibble\nNow let’s see how tidyverse handles the same data:\n\n# Convert to tibble\ntbl &lt;- as_tibble(df)\n\n# View the tibble\ntbl\n\n# A tibble: 30 × 2\n   weight group\n    &lt;dbl&gt; &lt;fct&gt;\n 1   4.17 ctrl \n 2   5.58 ctrl \n 3   5.18 ctrl \n 4   6.11 ctrl \n 5   4.5  ctrl \n 6   4.61 ctrl \n 7   5.17 ctrl \n 8   4.53 ctrl \n 9   5.33 ctrl \n10   5.14 ctrl \n# ℹ 20 more rows\n\n\nKey differences with tibbles:\n\n\nBetter printing: Only shows what fits on screen\n\nType information: Shows data types under column names\n\nNo partial matching: More predictable behavior\n\nPreserves data types: Doesn’t automatically convert strings to factors\nAccessing columns in tidyverse:\n\n# Still can use $ notation\ntbl$weight\n\n [1] 4.17 5.58 5.18 6.11 4.50 4.61 5.17 4.53 5.33 5.14 4.81 4.17 4.41 3.59 5.87\n[16] 3.83 6.03 4.89 4.32 4.69 6.31 5.12 5.54 5.50 5.37 5.29 4.92 6.15 5.80 5.26\n\n# Or use select() function (we'll learn more about this)\ntbl %&gt;% select(weight)\n\n# A tibble: 30 × 1\n   weight\n    &lt;dbl&gt;\n 1   4.17\n 2   5.58\n 3   5.18\n 4   6.11\n 5   4.5 \n 6   4.61\n 7   5.17\n 8   4.53\n 9   5.33\n10   5.14\n# ℹ 20 more rows"
  },
  {
    "objectID": "ch/rbasics/tidyverse.html#base-r-plotting",
    "href": "ch/rbasics/tidyverse.html#base-r-plotting",
    "title": "The tidyverse",
    "section": "Base R Plotting",
    "text": "Base R Plotting\nBase R has simple plotting functions that are quick but limited:\n\n# Scatter plot\nplot(df$weight, main = \"Weight Values\",\n     xlab = \"Index\",\n     ylab = \"Weight\")\n# Bar plot with better labels\nplot(df$group, \n     main = \"Group Frequencies\",\n     xlab = \"Treatment Group\",\n     ylab = \"Frequency\",\n     names.arg = c(\"Control\", \"Treatment 1\", \"Treatment 2\"))"
  },
  {
    "objectID": "ch/rbasics/tidyverse.html#ggplot2-tidyverse",
    "href": "ch/rbasics/tidyverse.html#ggplot2-tidyverse",
    "title": "The tidyverse",
    "section": "ggplot2 (Tidyverse)",
    "text": "ggplot2 (Tidyverse)\nggplot2 builds plots in layers, like creating a painting. Let’s break it down:\nUnderstanding ggplot2 basics:\n\n\nggplot() - Creates the canvas\n\naes() - Stands for “aesthetics” - tells ggplot which data to use\n\n+ - Adds layers to your plot (like adding paint to canvas)\n**geom_*()** - Geometric objects (the actual marks on the plot)\n\nLet’s build our plots step by step:\n\n# Scatter plot with index\n# Step 1: Create the canvas and specify the data\n# aes(x = ..., y = ...) maps data to x and y axes\nggplot(data = tbl, aes(x = 1:nrow(tbl), y = weight)) +\n  # Step 2: Add points to the plot\n  geom_point() +\n  # Step 3: Add labels\n  labs(title = \"Weight Values\", \n       x = \"Index\", \n       y = \"Weight\") +\n  theme_classic()\n# Bar plot\n# Step 1: Create canvas with data mapping\n# When we only specify x, ggplot counts occurrences\nggplot(data = tbl, aes(x = group)) +\n  # Step 2: Add bars (geom_bar counts automatically)\n  geom_bar() +\n  # Step 3: Add descriptive labels\n  labs(title = \"Group Frequencies\", \n       x = \"Group\", \n       y = \"Count\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nBreaking down the code:\nFor the scatter plot:\n- ggplot(data = tbl, ...) - Use the ‘tbl’ dataset\n- aes(x = 1:nrow(tbl), y = weight) - Put row numbers on x-axis, weight values on y-axis\n- geom_point() - Draw points at each (x,y) coordinate\n- The + sign connects these layers together\nFor the bar plot:\n- aes(x = group) - Put group categories on x-axis\n- geom_bar() - Count how many times each group appears and draw bars\n- ggplot automatically counts for us!\nThink of it like a recipe:\n\nStart with your data (ggplot + data)\n\nDecide what goes where (aes)\n\nChoose how to show it (geom_point, geom_bar, etc.)\n\nAdd finishing touches (labs, themes, colors)\nCommon geom_ functions and how to explore more:\nHere are the most common geometric layers you’ll use:\n\n# Create sample data for demonstrations\ndemo_data &lt;- tibble(\n  x = 1:10,\n  y = c(2, 4, 3, 7, 5, 8, 6, 9, 7, 10),\n  group = rep(c(\"A\", \"B\"), 5)\n)\n\nEssential geom_ functions:\n\n\n\n\n\n\n\ngeom_ function\nWhat it draws\nWhen to use\n\n\n\ngeom_point()\nPoints/dots\nScatter plots, showing individual values\n\n\ngeom_line()\nLines connecting points\nTime series, trends\n\n\ngeom_bar()\nBars (counts data)\nFrequency of categories\n\n\ngeom_col()\nBars (uses y values)\nWhen you already have heights\n\n\ngeom_histogram()\nHistogram\nDistribution of continuous data\n\n\ngeom_boxplot()\nBox plots\nComparing distributions between groups\n\n\ngeom_smooth()\nTrend lines\nAdding regression/smooth lines\n\n\nQuick examples:\n\n# Line plot\nggplot(demo_data, aes(x = x, y = y)) +\n  geom_line() +\n  labs(title = \"geom_line()\",\n       x = \"X\",\n       y = \"Y\") +\n  theme_classic()\n# Points + smooth line\nggplot(data = demo_data, aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth() +\n  labs(title = \"geom_point() + geom_smooth()\") +\n  theme_classic()\n# Box plot by group\nggplot(demo_data, aes(x = group, y = y)) +\n  geom_boxplot() +\n  stat_summary(fun = mean, \n               geom = \"point\", \n               shape = 23, \n               size = 3, \n               fill = \"orange\") +\n  geom_jitter(width=0.2) +\n  labs(title = \"geom_boxplot()\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse ?pch or ?shape to know more about shapes.\nHow to discover more geom_ functions:\n\n\nIn RStudio: Type geom_ and press TAB to see all available options\n\n# Try this in your console:\n# ggplot(data, aes(x, y)) + geom_[TAB]\n\n\n\nGet help on any function:\n\n# Learn about a specific geom\n?geom_violin\n\n# See examples\nexample(geom_violin)\n\n\n\nUseful resources:\n\n\nggplot2 cheatsheet - Visual guide to all geoms\n\nR Graph Gallery - Examples of every plot type\n\nggplot2 documentation - Official reference\n\nfrom Data to Viz - Has robust way to show plotting options\n\n\n\nExperiment! Try different geoms with your data:\n\n# Start with basic plot\np &lt;- ggplot(tbl, aes(x = group, y = weight))\n\n# Try different visualizations\np + geom_boxplot()    # Box plot\np + geom_violin()     # Violin plot\np + geom_jitter()     # Scattered points\np + geom_dotplot(binaxis = \"y\")  # Dot plot\n\n\nPro tip: Layer multiple geoms!\n\n# You can combine multiple geoms for rich visualizations\nggplot(tbl, aes(x = group, y = weight)) +\n  geom_boxplot(alpha = 0.5) +      # Semi-transparent box plot\n  geom_jitter(width = 0.2) +        # Add individual points\n  labs(title = \"Combining geom_boxplot() + geom_jitter()\") +\n  theme_classic()"
  },
  {
    "objectID": "ch/rbasics/tidyverse.html#without-pipes-base-r-approach",
    "href": "ch/rbasics/tidyverse.html#without-pipes-base-r-approach",
    "title": "The tidyverse",
    "section": "Without pipes (Base R approach):",
    "text": "Without pipes (Base R approach):\n\n# Step 1: Get ctrl group only\nctrl_only &lt;- df[df$group == \"ctrl\", ]\n\n# Step 2: Extract weight values\nweights &lt;- ctrl_only$weight\n\n# Step 3: Calculate square root\nsqrt_weights &lt;- sqrt(weights)\n\n# Step 4: Round to 1 decimal\nrounded &lt;- round(sqrt_weights, 2)\n\n# Step 5: Sort\nsorted &lt;- sort(rounded, decreasing = TRUE)\n\nsorted\n\n [1] 2.47 2.36 2.31 2.28 2.27 2.27 2.15 2.13 2.12 2.04"
  },
  {
    "objectID": "ch/rbasics/tidyverse.html#with-pipes-tidyverse-approach",
    "href": "ch/rbasics/tidyverse.html#with-pipes-tidyverse-approach",
    "title": "The tidyverse",
    "section": "With pipes (Tidyverse approach):",
    "text": "With pipes (Tidyverse approach):\n\ndf %&gt;% \n  filter(group == \"ctrl\") %&gt;%   # Step 1: Get ctrl group\n  pull(weight) %&gt;%              # Step 2: Extract weights\n  sqrt() %&gt;%                    # Step 3: Square root\n  round(1) %&gt;%                  # Step 4: Round\n  sort(decreasing = TRUE)                        # Step 5: Sort\n\n [1] 2.5 2.4 2.3 2.3 2.3 2.3 2.1 2.1 2.1 2.0\n\n\nTip: To type %&gt;% quickly in RStudio, use Ctrl+Shift+M (Windows/Linux) or Cmd+Shift+M (Mac)"
  },
  {
    "objectID": "ch/rbasics/tidyverse.html#mutate---add-or-modify-columns",
    "href": "ch/rbasics/tidyverse.html#mutate---add-or-modify-columns",
    "title": "The tidyverse",
    "section": "1. mutate() - Add or modify columns",
    "text": "1. mutate() - Add or modify columns\nBase R approach:\n\n# Add a new column\ndf_copy &lt;- df\ndf_copy$weight_kg &lt;- df_copy$weight / 1000\n\n# Modify existing column\ndf_copy$weight &lt;- df_copy$weight * 2\n\nhead(df_copy)\n\n  weight group weight_kg\n1   8.34  ctrl   0.00417\n2  11.16  ctrl   0.00558\n3  10.36  ctrl   0.00518\n4  12.22  ctrl   0.00611\n5   9.00  ctrl   0.00450\n6   9.22  ctrl   0.00461\n\n\nTidyverse approach:\n\ntbl %&gt;% \n  mutate(\n    weight_kg = weight/1000,  # Add new column\n    weight = weight*2\n  ) %&gt;% \n  head()\n\n# A tibble: 6 × 3\n  weight group weight_kg\n   &lt;dbl&gt; &lt;fct&gt;     &lt;dbl&gt;\n1   8.34 ctrl    0.00417\n2  11.2  ctrl    0.00558\n3  10.4  ctrl    0.00518\n4  12.2  ctrl    0.00611\n5   9    ctrl    0.0045 \n6   9.22 ctrl    0.00461\n\n\nN.B. We could make the doubling operation on the same weight column as well. It would make in-place modification. You have to think when to do that operation then."
  },
  {
    "objectID": "ch/rbasics/tidyverse.html#select---choose-columns",
    "href": "ch/rbasics/tidyverse.html#select---choose-columns",
    "title": "The tidyverse",
    "section": "2. select() - Choose columns",
    "text": "2. select() - Choose columns\nBase R approach:\n\n# Select specific columns\ndf_subset &lt;- df[0:nrow(df), c(\"group\", \"weight\")]\nhead(df_subset)\n\n  group weight\n1  ctrl   4.17\n2  ctrl   5.58\n3  ctrl   5.18\n4  ctrl   6.11\n5  ctrl   4.50\n6  ctrl   4.61\n\n\nTidyverse approach:\n\ntbl &lt;- tbl %&gt;% \n  select(group, weight)\n\nN.B. select() helps to rearrange columns as well."
  },
  {
    "objectID": "ch/rbasics/tidyverse.html#filter---choose-rows",
    "href": "ch/rbasics/tidyverse.html#filter---choose-rows",
    "title": "The tidyverse",
    "section": "3. filter() - Choose rows",
    "text": "3. filter() - Choose rows\nBase R approach:\n\n# Filter for weight &gt; 5\ndf_filtered &lt;- df[df$weight &gt; 5, ]\ndf_filtered\n\n   weight group\n2    5.58  ctrl\n3    5.18  ctrl\n4    6.11  ctrl\n7    5.17  ctrl\n9    5.33  ctrl\n10   5.14  ctrl\n15   5.87  trt1\n17   6.03  trt1\n21   6.31  trt2\n22   5.12  trt2\n23   5.54  trt2\n24   5.50  trt2\n25   5.37  trt2\n26   5.29  trt2\n28   6.15  trt2\n29   5.80  trt2\n30   5.26  trt2\n\n\nTidyverse approach:\n\ntbl %&gt;% \n  filter(weight &gt; 5)\n\n# A tibble: 17 × 2\n   group weight\n   &lt;fct&gt;  &lt;dbl&gt;\n 1 ctrl    5.58\n 2 ctrl    5.18\n 3 ctrl    6.11\n 4 ctrl    5.17\n 5 ctrl    5.33\n 6 ctrl    5.14\n 7 trt1    5.87\n 8 trt1    6.03\n 9 trt2    6.31\n10 trt2    5.12\n11 trt2    5.54\n12 trt2    5.5 \n13 trt2    5.37\n14 trt2    5.29\n15 trt2    6.15\n16 trt2    5.8 \n17 trt2    5.26"
  },
  {
    "objectID": "ch/rbasics/tidyverse.html#arrange---sort-rows",
    "href": "ch/rbasics/tidyverse.html#arrange---sort-rows",
    "title": "The tidyverse",
    "section": "4. arrange() - Sort rows",
    "text": "4. arrange() - Sort rows\nBase R approach:\n\n# Sort by weight\ndf_sorted &lt;- df[order(df$weight), ]\nhead(df_sorted)\n\n   weight group\n14   3.59  trt1\n16   3.83  trt1\n1    4.17  ctrl\n12   4.17  trt1\n19   4.32  trt1\n13   4.41  trt1\n\n\n\ndf_sorted &lt;- df[order(df$weight, decreasing=TRUE), ]\n\nTidyverse approach:\n\ntbl %&gt;% \n  arrange(weight)\n\n# A tibble: 30 × 2\n   group weight\n   &lt;fct&gt;  &lt;dbl&gt;\n 1 trt1    3.59\n 2 trt1    3.83\n 3 ctrl    4.17\n 4 trt1    4.17\n 5 trt1    4.32\n 6 trt1    4.41\n 7 ctrl    4.5 \n 8 ctrl    4.53\n 9 ctrl    4.61\n10 trt1    4.69\n# ℹ 20 more rows"
  },
  {
    "objectID": "ch/rbasics/tidyverse.html#summarise-with-group_by---calculate-summaries",
    "href": "ch/rbasics/tidyverse.html#summarise-with-group_by---calculate-summaries",
    "title": "The tidyverse",
    "section": "5. summarise() with group_by() - Calculate summaries",
    "text": "5. summarise() with group_by() - Calculate summaries\nBase R approach:\n\n# Calculate mean by group\naggregate(weight ~ group, data = df, FUN = mean)\n\n  group weight\n1  ctrl  5.032\n2  trt1  4.661\n3  trt2  5.526\n\n\nTidyverse approach:\n\ntbl %&gt;% \n  group_by(group) %&gt;% \n  summarise(\n    mean_weight = mean(weight),\n    sd_weight = sd(weight),\n    n = n()\n  )\n\n# A tibble: 3 × 4\n  group mean_weight sd_weight     n\n  &lt;fct&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;\n1 ctrl         5.03     0.583    10\n2 trt1         4.66     0.794    10\n3 trt2         5.53     0.443    10"
  },
  {
    "objectID": "ch/rbasics/tidyverse.html#creating-example-data",
    "href": "ch/rbasics/tidyverse.html#creating-example-data",
    "title": "The tidyverse",
    "section": "Creating example data:",
    "text": "Creating example data:\n\n# Create a small dataset\nlong_data &lt;- data.frame(\n  student = c(\"Alice\", \"Alice\", \"Alice\", \"Bob\", \"Bob\", \"Bob\"),\n  test = c(\"Math\", \"English\", \"Chemistry\", \"Math\", \"English\", \"Chemistry\"),\n  score = c(85, 90, 78, 82, 78, 90)\n)\n\nlong_data\n\n  student      test score\n1   Alice      Math    85\n2   Alice   English    90\n3   Alice Chemistry    78\n4     Bob      Math    82\n5     Bob   English    78\n6     Bob Chemistry    90"
  },
  {
    "objectID": "ch/rbasics/tidyverse.html#convert-to-wide-format",
    "href": "ch/rbasics/tidyverse.html#convert-to-wide-format",
    "title": "The tidyverse",
    "section": "Convert to wide format:",
    "text": "Convert to wide format:\nBase R approach:\n\n# Using reshape function\nwide_base &lt;- reshape(long_data, \n                     idvar = \"student\", \n                     timevar = \"test\", \n                     direction = \"wide\")\nwide_base\n\n  student score.Math score.English score.Chemistry\n1   Alice         85            90              78\n4     Bob         82            78              90\n\n\nTidyverse approach:\n\nwide_data &lt;- long_data %&gt;% \n  pivot_wider(names_from = test, \n              values_from = score)\nwide_data\n\n# A tibble: 2 × 4\n  student  Math English Chemistry\n  &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1 Alice      85      90        78\n2 Bob        82      78        90"
  },
  {
    "objectID": "ch/rbasics/tidyverse.html#convert-back-to-long-format",
    "href": "ch/rbasics/tidyverse.html#convert-back-to-long-format",
    "title": "The tidyverse",
    "section": "Convert back to long format:",
    "text": "Convert back to long format:\nTidyverse approach:\n\nwide_data %&gt;% \n  pivot_longer(cols = -student,\n               names_to = \"test\",\n               values_to = \"score\")\n\n# A tibble: 6 × 3\n  student test      score\n  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;\n1 Alice   Math         85\n2 Alice   English      90\n3 Alice   Chemistry    78\n4 Bob     Math         82\n5 Bob     English      78\n6 Bob     Chemistry    90"
  },
  {
    "objectID": "ch/rbasics/tidyverse.html#reordering-factors",
    "href": "ch/rbasics/tidyverse.html#reordering-factors",
    "title": "The tidyverse",
    "section": "Reordering factors:",
    "text": "Reordering factors:\n\n# Specify custom order\nplot_data %&gt;% \n  mutate(category = fct_relevel(category, \"Low\", \"Medium\", \"High\")) %&gt;% \n  ggplot(aes(x = category, y = value)) +\n  geom_col() +\n  labs(title = \"Custom Order\") +\n  theme_classic()"
  },
  {
    "objectID": "ch/rbasics/tidyverse.html#common-string-operations",
    "href": "ch/rbasics/tidyverse.html#common-string-operations",
    "title": "The tidyverse",
    "section": "Common string operations:",
    "text": "Common string operations:\n\n# Example strings\nmessy_string &lt;- \"  Hello   World!  \"\nnames &lt;- c(\"John Smith\", \"Jane Doe\", \"Bob Johnson\")\n\n# Remove extra spaces\nstr_trim(messy_string)\n\n[1] \"Hello   World!\"\n\nstr_squish(messy_string)\n\n[1] \"Hello World!\"\n\n# Replace text\nstr_replace(names, \"John\", \"Jonathan\")\n\n[1] \"Jonathan Smith\"  \"Jane Doe\"        \"Bob Jonathanson\"\n\n# Detect pattern\nstr_detect(names, \"John\")\n\n[1]  TRUE FALSE  TRUE\n\n# Extract substring\nstr_sub(names, 1, 4)\n\n[1] \"John\" \"Jane\" \"Bob \""
  },
  {
    "objectID": "ch/rbasics/tidyverse.html#part-1-basic-operations",
    "href": "ch/rbasics/tidyverse.html#part-1-basic-operations",
    "title": "The tidyverse",
    "section": "Part 1: Basic Operations",
    "text": "Part 1: Basic Operations\nUsing the built-in iris dataset:"
  },
  {
    "objectID": "ch/rbasics/tidyverse.html#plotting-challenge",
    "href": "ch/rbasics/tidyverse.html#plotting-challenge",
    "title": "The tidyverse",
    "section": "Plotting Challenge:",
    "text": "Plotting Challenge:\nCreate a visualization that shows the relationship between Petal.Length and Petal.Width, colored by Species, with: - Proper labels and title - A theme of your choice - Regression lines for each species\nAnd try more plotting as you wish!"
  },
  {
    "objectID": "ch/rbasics/tidyverse.html#problem-1-data-manipulation",
    "href": "ch/rbasics/tidyverse.html#problem-1-data-manipulation",
    "title": "The tidyverse",
    "section": "Problem 1: Data Manipulation",
    "text": "Problem 1: Data Manipulation\nUsing the built-in iris dataset:\n1. Convert it to a tibble\n2. Create a new column called Petal.Ratio that is Petal.Length / Petal.Width\n3. Filter for only “setosa” species with Sepal.Length &gt; 5\n4. Select only the Species, Sepal.Length, and your new Petal.Ratio columns\n5. Arrange the results by Petal.Ratio in descending order"
  },
  {
    "objectID": "ch/rbasics/tidyverse.html#problem-2-grouping-and-summarizing",
    "href": "ch/rbasics/tidyverse.html#problem-2-grouping-and-summarizing",
    "title": "The tidyverse",
    "section": "Problem 2: Grouping and Summarizing",
    "text": "Problem 2: Grouping and Summarizing\nUsing the full iris dataset:\n\nGroup by Species\n\nCalculate the following for each species:\n\nMean Sepal.Length\nStandard deviation of Sepal.Width\nMinimum and maximum Petal.Length\nCount of observations\n\n\nCreate a bar plot showing the mean Sepal.Length by Species"
  },
  {
    "objectID": "ch/rbasics/tidyverse.html#problem-3-data-reshaping",
    "href": "ch/rbasics/tidyverse.html#problem-3-data-reshaping",
    "title": "The tidyverse",
    "section": "Problem 3: Data Reshaping",
    "text": "Problem 3: Data Reshaping\n\nCreate a subset of iris with the first 3 rows of each species\nAdd a row number within each species (call it “plant_id”)\nConvert this to wide format where:\n\nEach row represents one plant_id\nColumns show the Sepal.Length for each species"
  },
  {
    "objectID": "ch/rbasics/tidyverse.html#submission-instructions",
    "href": "ch/rbasics/tidyverse.html#submission-instructions",
    "title": "The tidyverse",
    "section": "Submission Instructions:",
    "text": "Submission Instructions:\n\nSubmit your R Markdown file\nInclude comments explaining your code, discuss with your peer and improve\nMake sure your code runs without errors\nDue date: Friday 10PM BD Time"
  },
  {
    "objectID": "ch/rbasics/tidyverse.html#grading-rubric",
    "href": "ch/rbasics/tidyverse.html#grading-rubric",
    "title": "The tidyverse",
    "section": "Grading Rubric:",
    "text": "Grading Rubric:\n\nCode correctness: 70%\nCode style and comments: 20%\nOutput interpretation: 10%\n\nGood luck! Remember to use the pipe operator %&gt;% to make your code readable!"
  },
  {
    "objectID": "ch/rbasics/solutions.html#homework-solution-tidyverse",
    "href": "ch/rbasics/solutions.html#homework-solution-tidyverse",
    "title": "HW solutions",
    "section": "",
    "text": "# Load necessary package\nlibrary(ggplot2)\n\n# Use the built-in iris dataset\ndata(iris)\n\n# Create the plot\nggplot(iris, aes(x = Petal.Length, y = Petal.Width, color = Species)) +\n  geom_point(size = 2) +  # scatter plot points\n  geom_smooth(method = \"lm\", se = FALSE) +  # linear regression lines\n  labs(\n    title = \"Relationship between Petal Length and Width by Species\",\n    x = \"Petal Length (cm)\",\n    y = \"Petal Width (cm)\",\n    color = \"Iris Species\"\n  ) +\n  theme_minimal()  # apply a clean minimal theme\n\n\n\n\n\nlibrary(ggplot2)\n\nggplot(iris, aes(x = Petal.Length, y = Petal.Width)) +\n  geom_point(aes(color = Species), size = 2) +  # color points by Species\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\") +  # single regression line\n  labs(\n    title = \"Overall Regression: Petal Length vs Width (Iris Dataset)\",\n    x = \"Petal Length (cm)\",\n    y = \"Petal Width (cm)\",\n    color = \"Iris Species\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "ch/newsletter/index.html",
    "href": "ch/newsletter/index.html",
    "title": "PythRaSh’s AI Newsletter Archive",
    "section": "",
    "text": "Your weekly digest of the most exciting developments at the intersection of artificial intelligence, biology, and healthcare."
  },
  {
    "objectID": "ch/newsletter/index.html#about-the-newsletter",
    "href": "ch/newsletter/index.html#about-the-newsletter",
    "title": "PythRaSh’s AI Newsletter Archive",
    "section": "About the Newsletter",
    "text": "About the Newsletter\nPythRaSh’s AI Newsletter is an AI curated weekly publication featuring:\n\n🚀 Event of the Week: Major breakthroughs in AI applications for biology and healthcare\n⚡ Quick Updates: Latest developments from leading research institutions\n\n📚 Research Papers: Top academic publications with impact analysis\n💻 GitHub Repositories: Popular tools and frameworks for AI in biology\n🛠️ AI Products: Featured tools from Product Hunt and industry\n🐦 Trending Discussions: Key conversations from industry leaders"
  },
  {
    "objectID": "ch/newsletter/index.html#recent-issues",
    "href": "ch/newsletter/index.html#recent-issues",
    "title": "PythRaSh’s AI Newsletter Archive",
    "section": "Recent Issues",
    "text": "Recent Issues\nAll the issues are archived here"
  },
  {
    "objectID": "ch/newsletter/index.html#newsletter-statistics",
    "href": "ch/newsletter/index.html#newsletter-statistics",
    "title": "PythRaSh’s AI Newsletter Archive",
    "section": "Newsletter Statistics",
    "text": "Newsletter Statistics\n\n\n\n📊 Metrics\n\nTotal Issues: 1+\nActive Subscribers: 14\nWeekly Readers: Growing\n\n\n\n\n🎯 Topics\n\nAI in Biology\nHealthcare AI\n\nComputational Biology\nBiotech Innovation\n\n\n\n\n📱 Format\n\nMobile-responsive HTML\nEmail-optimized design\nWeb archive available"
  },
  {
    "objectID": "ch/newsletter/index.html#subscribe-connect",
    "href": "ch/newsletter/index.html#subscribe-connect",
    "title": "PythRaSh’s AI Newsletter Archive",
    "section": "Subscribe & Connect",
    "text": "Subscribe & Connect\nReady to stay updated with the latest AI breakthroughs in biology and healthcare?\n📧 Subscribe to Newsletter\n\nFollow PythRaSh\n\n🌐 Website\n💼 LinkedIn\n🐙 GitHub\n📧 Email\n\n\nNewsletter archive is updated automatically every week. All issues are available in both web and original email format."
  },
  {
    "objectID": "ch/linux-and-ctl/advanced.html#hpc-cluster-connection",
    "href": "ch/linux-and-ctl/advanced.html#hpc-cluster-connection",
    "title": "Advanced Linux and CTL",
    "section": "",
    "text": "I am going to connect to Uni-Greifswald’s Brain cluster.\n\nssh username@brain.uni-greifswald.de\n\nYou need to give your real username and password. Now, let’s get an interactive session to the gpu compute node (it is named “vision” for uni-greifswald’s gpu node).\n\nsrun --pty --gres=gpu:1 --partition=vision --mem=16g -t 12:00:00 bash -i\n\nLet’s install conda for our environment management.\n\n# Download the Miniconda installer for Linux\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n\n# Run the installer, specifying the installation path\nbash Miniconda3-latest-Linux-x86_64.sh -b -p ~/miniconda_install/\n\nNow, let’s initialize conda:\n\n# Source the conda shell script to make the 'conda' command available\nsource ~/miniconda_install/bin/activate\n\n# Initialize conda for the current shell session.\nconda init bash\n\nSince we’re using an interactive session, we won’t need to manually source anything after this. The conda init command makes it so we can use conda and conda activate as we normally would.\nN.B. We could make some aliases for conda commands to write conda codes in shorter format, but we can do/see it later.\nLet’s make an environment and install our required tools there. Well, our goal is to make a system where we will use some images/characters and make short videos using them to teach Italian. We need to process photos, images, text, and sync lips (video) with audio/speech. We need MuseTalk for this. Let’s configure our environment accordingly.\nStep 1 – Create conda environment\n\n# Create new environment for MuseTalk\nconda create -n musetalk python=3.10\nconda activate musetalk\n\nStep 2 – Install Dependencies\n\n# Create new environment for MuseTalk\n# Install PyTorch (adapt cuda version to your cluster, here CUDA 11.8 example)\npip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --extra-index-url https://download.pytorch.org/whl/cu118\n\n# HuggingFace core libs\npip install transformers accelerate diffusers safetensors\n\n# MuseTalk repo (clone)\ngit clone https://github.com/TMElyralab/MuseTalk.git\ncd MuseTalk\n\n# Install requirements\npip install -r requirements.txt\n\n# Install whisper encoder\npip install --editable ./musetalk/whisper\n\n# Extra: ffmpeg for video processing\nconda install -c conda-forge ffmpeg -y\n\nStep 3 – Download MuseTalk Models MuseTalk Hugging Face repo: https://huggingface.co/TMElyralab/MuseTalk\nWe need:\n- musetalk.pth (main model) - gfpgan (optional face enhancer)\nRun inside MuseTalk:\n\nmkdir checkpoints\ncd checkpoints\n\n# Download core model\nwget https://huggingface.co/TMElyralab/MuseTalk/resolve/main/musetalk.pth\n\n# Optional face enhancer\ngit clone https://github.com/TencentARC/GFPGAN.git\ncd .."
  },
  {
    "objectID": "ch/linux-and-ctl/advanced.html#better-way",
    "href": "ch/linux-and-ctl/advanced.html#better-way",
    "title": "Advanced Linux and CTL",
    "section": "",
    "text": "export CUDA_HOME=/usr/local/cuda-11.7\nexport PATH=$CUDA_HOME/bin:$PATH\nexport LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH\n\nIn my HPC Cluster, it is cuda 11.7. I am configuring for that. We could add these lines in my ~/.bashrc file as well. We might see it later.\nLet’s load our cuda module first to get things done smoothly.\n\nmodule load cuda/12.0.0\n\nNow, make the yml file to make the environment with all the tools required.\n\nname: musetalk3\nchannels:\n  - pytorch\n  - nvidia\n  - defaults\ndependencies:\n  - python=3.10\n  - pip\n  - ffmpeg\n  - pip:\n      # PyTorch + CUDA 11.8 compatible with 11.7 system\n      - torch==2.1.0+cu118\n      - torchvision==0.16.0+cu118\n      - torchaudio==2.1.0+cu118\n      - --extra-index-url https://download.pytorch.org/whl/cu118\n\n      # OpenMMLab dependencies\n      - mmcv==2.0.1 -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.1.0/index.html\n      - mmdet==3.1.0\n      - gradio\n      - opencv-python\n      - numpy\n      - scipy\n      - matplotlib\n      - tqdm\n      - pyyaml\n      - pillow\n      - soundfile\n      - librosa\n      - moviepy\n      - imageio\n\nSave this as musetalk3.yml and run:\n\nconda env create -f musetalk3.yml\nconda activate musetalk3\n\nNow, we have our environment ready to use. Let’s use it.\n\nconda activate musetalk3\n\nNow, install museetalk repo dependencies.\n\ngit clone https://github.com/TMElyralab/MuseTalk.git\ncd MuseTalk\npip install -r requirements.txt\n\nLet’s get the faces I want to use. I uploaded them to my GDrive.\n\npip install gdown\n\n# Get shareable link from Google Drive and copy file id\ngdown https://drive.google.com/uc?id=FILE_ID -O data/faces/alice.jpg\ngdown https://drive.google.com/uc?id=FILE_ID -O data/faces/bob.jpg\n\nmodify the google drive links for the images and their destination name as you like it."
  },
  {
    "objectID": "ch/newsletter/2025_09_15_newsletter_improved.html",
    "href": "ch/newsletter/2025_09_15_newsletter_improved.html",
    "title": "AI Newsletter – September 15, 2025",
    "section": "",
    "text": "📧 Newsletter Format\n\n\n\nThis is PythRaSh’s AI Newsletter for the week of September 15, 2025. You can view the full newsletter or subscribe to receive it directly."
  },
  {
    "objectID": "ch/newsletter/2025_09_15_newsletter_improved.html#newsletter-preview",
    "href": "ch/newsletter/2025_09_15_newsletter_improved.html#newsletter-preview",
    "title": "AI Newsletter – September 15, 2025",
    "section": "Newsletter Preview",
    "text": "Newsletter Preview"
  },
  {
    "objectID": "ch/newsletter/2025_09_15_newsletter_improved.html#quick-actions",
    "href": "ch/newsletter/2025_09_15_newsletter_improved.html#quick-actions",
    "title": "AI Newsletter – September 15, 2025",
    "section": "🔗 Quick Actions",
    "text": "🔗 Quick Actions\n\n\n\n📧 Subscribe\nGet weekly AI newsletters\n\n\n🔗 Share\n\nShare on Twitter\nShare on LinkedIn\n\n\n\n\n📱 Connect\n\nWebsite\nLinkedIn Profile\nGitHub\n\n\n\n📋 Archive\nView all newsletters"
  },
  {
    "objectID": "ch/newsletter/2025_09_15_newsletter_improved.html#this-weeks-highlights",
    "href": "ch/newsletter/2025_09_15_newsletter_improved.html#this-weeks-highlights",
    "title": "AI Newsletter – September 15, 2025",
    "section": "This Week’s Highlights",
    "text": "This Week’s Highlights\n🚀 Event of the Week: Stanford Creates AI “Virtual Scientist” for Autonomous Biological Experiments\n⚡ Quick Updates: - Mayo Clinic & Microsoft RAD-DINO collaboration - UC Berkeley & NVIDIA Evo 2 development\n- Google Research drug repurposing discoveries - UCSF breakthrough in synthetic proteins\n📚 Research Papers: 3 high-impact publications in AI for computational biology\n💻 GitHub Repos: TorchIO, paperai, vg toolkit, and funNLP\n🐦 Trending: Discussions from (sama?), (demishassabis?), and healthcare AI developments\n\nPublished on September 15, 2025 | Subscribe for weekly updates"
  },
  {
    "objectID": "ch/newsletter/2025_09_15_newsletter_fixed.html",
    "href": "ch/newsletter/2025_09_15_newsletter_fixed.html",
    "title": "AI Newsletter – September 15, 2025",
    "section": "",
    "text": "Stanford’s AI Virtual Scientist & More\nPublished on September 15, 2025 | Subscribe for weekly updates"
  },
  {
    "objectID": "ch/newsletter/2025_09_15_newsletter_fixed.html#week-of-september-15-2025",
    "href": "ch/newsletter/2025_09_15_newsletter_fixed.html#week-of-september-15-2025",
    "title": "AI Newsletter – September 15, 2025",
    "section": "",
    "text": "Stanford’s AI Virtual Scientist & More"
  },
  {
    "objectID": "ch/newsletter/2025_09_15_newsletter_fixed.html#event-of-the-week",
    "href": "ch/newsletter/2025_09_15_newsletter_fixed.html#event-of-the-week",
    "title": "AI Newsletter – September 15, 2025",
    "section": "🚀 EVENT OF THE WEEK",
    "text": "🚀 EVENT OF THE WEEK\n\nStanford Creates AI “Virtual Scientist” for Autonomous Biological Experiments\nStanford researchers have achieved a groundbreaking milestone by developing an AI “virtual scientist” capable of designing, running, and analyzing its own biological experiments autonomously. This revolutionary system can iterate on hypotheses, adapt in real-time, and simulate the complete workflow of a human researcher. The AI agent handles the entire scientific method—from initial hypothesis generation through experimental design to data analysis and interpretation.\nWhat makes this particularly exciting for the biological research community is its application to genomics and drug discovery. The system is being tested on complex biological problems that traditionally require months or years of manual trial-and-error experimentation. By automating the design-build-test-learn cycle, this AI could accelerate biomedical breakthroughs exponentially, potentially reducing the time from scientific question to validated answer from years to weeks.\nWhy this matters: This represents a paradigm shift from AI as a tool for analysis to AI as an active participant in scientific discovery. For biology and healthcare, this could mean faster drug discovery, more efficient genetic research, and the ability to explore biological hypotheses at unprecedented scale.\nKey takeaways: - AI can now autonomously conduct the complete scientific method workflow in biological research - Applications span genomics, drug discovery, and biomedical research acceleration - Could fundamentally change how biological discoveries are made and validated"
  },
  {
    "objectID": "ch/newsletter/2025_09_15_newsletter_fixed.html#quick-updates",
    "href": "ch/newsletter/2025_09_15_newsletter_fixed.html#quick-updates",
    "title": "AI Newsletter – September 15, 2025",
    "section": "⚡ Quick Updates",
    "text": "⚡ Quick Updates\n\n\nMayo Clinic & Microsoft: Collaboration on RAD-DINO, a multimodal foundation model that integrates text and images for radiology applications, promising faster and more precise medical diagnostics. Microsoft Research\nUC Berkeley & NVIDIA: Developed Evo 2, the largest AI model in biology trained on DNA from over 100,000 species, capable of identifying disease-causing mutations and designing bacterial genomes. Berkeley Engineering\nGoogle Research: AI co-scientist successfully identified novel drug repurposing candidates for acute myeloid leukemia, with experimental validation confirming AI predictions. Google Research Blog\nCardiovascular Innovation: Researchers developed a miniature AI-enhanced imaging camera for coronary artery analysis via catheter, detecting hidden blockages missed by standard imaging. News-Medical.net\nUCSF Breakthrough: Created the world’s first shape-shifting synthetic proteins using AI, opening possibilities for entirely new protein-based medicines to combat diseases like cancer. UCSF News"
  },
  {
    "objectID": "ch/newsletter/2025_09_15_newsletter_fixed.html#top-research-papers",
    "href": "ch/newsletter/2025_09_15_newsletter_fixed.html#top-research-papers",
    "title": "AI Newsletter – September 15, 2025",
    "section": "📚 Top Research Papers",
    "text": "📚 Top Research Papers\n\n\nAdvancements in AI for Computational Biology and Bioinformatics: A Comprehensive Review\nInstitution: Methods in Molecular Biology, 2025\nThis comprehensive review synthesizes the latest developments in AI methodologies and their applications in addressing key challenges within computational biology and bioinformatics. The paper highlights recent breakthroughs in AI-driven precision medicine, personalized genomics, and systems biology, showcasing how AI algorithms are revolutionizing our understanding of complex biological systems and driving innovations in healthcare and biotechnology.\n\nHigh Impact\n\n\n\nArtificial Intelligence and Machine Learning Technology Driven Modern Drug Discovery\nInstitution: MDPI International Journal of Molecular Sciences\nThis paper examines how AI and machine learning are transforming drug discovery and development, covering how deep learning models optimize regulatory sequences, propose genetic circuits, and accelerate the design-build-test-learn pipeline in synthetic biology.\n\nIndustry Impact\n\n\n\nThe Convergence of AI and Synthetic Biology: The Looming Deluge\nInstitution: npj Biomedical Innovations, Nature\nThis manuscript examines how AI-driven tools accelerate bioengineering workflows, unlocking innovations in medicine, agriculture, and sustainability. It addresses dual-use risks, governance challenges, and the dynamic interplay between human oversight and AI’s processing power in synthetic biology applications.\n\nPolicy Impact"
  },
  {
    "objectID": "ch/newsletter/2025_09_15_newsletter_fixed.html#top-github-repos-of-the-week",
    "href": "ch/newsletter/2025_09_15_newsletter_fixed.html#top-github-repos-of-the-week",
    "title": "AI Newsletter – September 15, 2025",
    "section": "💻 Top GitHub Repos of the Week",
    "text": "💻 Top GitHub Repos of the Week\n\n\nTorchIO\n⭐ 2,000+ stars | Active development\nEssential toolkit for medical image preprocessing in AI applications, supporting MRI, CT scans, and other medical imaging modalities for deep learning models. Perfect for researchers working on diagnostic AI and medical image analysis.\n\n\npaperai\n⭐ 1,500+ stars | Trending\nSpecialized AI system for analyzing and extracting insights from medical and scientific literature, enabling rapid literature review for drug discovery and clinical research. Invaluable for keeping up with the exponentially growing biomedical literature.\n\n\nvg (Variation Graph Toolkit)\n⭐ 1,000+ stars | Growing\nCritical tool for pangenome analysis and understanding genetic variation across populations, essential for precision medicine applications and population genomics research.\n\n\nfunNLP\n⭐ 67,000+ stars | Highly popular\nComprehensive NLP toolkit with specialized medical applications, including Chinese medical dialogue datasets, medical knowledge graphs, and clinical entity recognition systems."
  },
  {
    "objectID": "ch/newsletter/2025_09_15_newsletter_fixed.html#learning-blog-of-the-week",
    "href": "ch/newsletter/2025_09_15_newsletter_fixed.html#learning-blog-of-the-week",
    "title": "AI Newsletter – September 15, 2025",
    "section": "📖 Learning Blog of the Week",
    "text": "📖 Learning Blog of the Week\n\nHow AI Can Help Us Make New Medicines Faster\nAuthor: UCSF Research Team | Publication: UCSF News\nThis article explores the groundbreaking work at UCSF where researchers have created the world’s first shape-shifting synthetic proteins using artificial intelligence. The piece explains how this technology leverages decades of NIH funding and AI systems similar to ChatGPT but specifically designed for protein engineering.\nWhat you’ll learn: - How AI is revolutionizing protein design and synthetic biology - The potential for creating novel therapeutic proteins not found in nature - The intersection of computational biology and drug discovery"
  },
  {
    "objectID": "ch/newsletter/2025_09_15_newsletter_fixed.html#top-ai-products-of-the-week",
    "href": "ch/newsletter/2025_09_15_newsletter_fixed.html#top-ai-products-of-the-week",
    "title": "AI Newsletter – September 15, 2025",
    "section": "🛠️ Top AI Products of the Week",
    "text": "🛠️ Top AI Products of the Week\n\n\nAda - AI Data Analyst\n785 upvotes | Category: Data Analytics\nProfessional reports generator from any dataset, perfect for analyzing clinical trial data, patient outcomes, and research datasets without coding expertise. Can automate biostatistics and epidemiological analysis.\n\n\nIncerto - AI Copilot for Databases\n504 upvotes | Category: Database Tools\nEnables medical researchers to query complex patient databases, clinical records, and genomic data repositories using plain English instead of SQL, dramatically improving research efficiency.\n\n\nxpander.ai - Backend for AI Agents\n687 upvotes | Category: AI Development\nComplete solution for building autonomous AI agents for drug discovery workflows, patient monitoring systems, and clinical decision support tools."
  },
  {
    "objectID": "ch/newsletter/2025_09_15_newsletter_fixed.html#trending-tweets-of-the-week",
    "href": "ch/newsletter/2025_09_15_newsletter_fixed.html#trending-tweets-of-the-week",
    "title": "AI Newsletter – September 15, 2025",
    "section": "🐦 Trending Tweets of the Week",
    "text": "🐦 Trending Tweets of the Week\n\n\n(crypto_n_y_c?)\n\n“🩺🤖 AI Breakthrough in Healthcare: At this weekend’s American Heart Association conference, researchers revealed that AI voice agents cut reporting costs by 89% and improved accuracy in at-home blood pressure monitoring for older adults. AI isn’t just reshaping finance, it’s transforming healthcare…”\n\n❤️ 245 likes | 🔁 89 retweets\nThis tweet highlights real-world clinical validation of AI in remote patient monitoring, demonstrating tangible benefits for aging populations and healthcare cost reduction.\n\n\n(sama?)\n\n“The convergence of AI and biology is the most underrated trend in tech. We’re about to see breakthroughs that will make the internet revolution look small in comparison…”\n\n❤️ 12.4K likes | 🔁 2.3K retweets\nOpenAI CEO Sam Altman highlighting the transformative potential of AI-biology convergence, echoing this week’s major developments.\n\n\n(demishassabis?)\n\n“AlphaFold has now been used by over 2 million researchers worldwide. The next frontier: designing entirely new proteins that don’t exist in nature to cure diseases we couldn’t touch before…”\n\n❤️ 8.9K likes | 🔁 1.8K retweets\nDeepMind CEO discussing the evolution from protein structure prediction to de novo protein design—directly relevant to this week’s UCSF breakthrough."
  },
  {
    "objectID": "ch/newsletter/2025_09_15_newsletter_fixed.html#closing-note",
    "href": "ch/newsletter/2025_09_15_newsletter_fixed.html#closing-note",
    "title": "AI Newsletter – September 15, 2025",
    "section": "Closing Note",
    "text": "Closing Note\nThis week marks a pivotal moment in the convergence of AI and biology. We’re witnessing the emergence of truly autonomous AI scientists, the creation of synthetic life forms, and breakthrough applications in medical diagnosis and drug discovery. The Stanford virtual scientist, UCSF’s shape-shifting proteins, and UC Berkeley’s massive genomic AI represent more than technological achievements—they’re harbingers of a new era where AI doesn’t just analyze biological data but actively participates in the discovery and creation of life itself.\nFor researchers, clinicians, and students in biology and healthcare, these developments signal both tremendous opportunities and the need for new skills. The future belongs to those who can bridge computational and biological thinking, leveraging AI to ask bigger questions and find answers faster than ever before.\nThank you for reading PythRaSh’s AI Newsletter! If you found this week’s insights valuable, please share them with colleagues and friends interested in the intersection of AI and biology.\nHave feedback or suggestions? Contact me - I read every response!\nSee you next week!\nMd Rasheduzzaman"
  },
  {
    "objectID": "ch/newsletter/2025_09_15_newsletter_fixed.html#quick-actions",
    "href": "ch/newsletter/2025_09_15_newsletter_fixed.html#quick-actions",
    "title": "AI Newsletter – September 15, 2025",
    "section": "🔗 Quick Actions",
    "text": "🔗 Quick Actions\n\n\n\n📧 Subscribe\nGet weekly AI newsletters\n\n\n🔗 Share\n\nShare on Twitter\nShare on LinkedIn\n\n\n\n\n📱 Connect\n\nWebsite\nLinkedIn Profile\nGitHub\n\n\n\n📋 Archive\nView all newsletters"
  },
  {
    "objectID": "ch/newsletter/2025_09_15_newsletter.html",
    "href": "ch/newsletter/2025_09_15_newsletter.html",
    "title": "AI Newsletter – September 15, 2025",
    "section": "",
    "text": "📧 Newsletter Format\n\n\n\nThis is PythRaSh’s AI Newsletter for the week of September 15, 2025. You can view the full newsletter or subscribe to receive it directly."
  },
  {
    "objectID": "ch/newsletter/2025_09_15_newsletter.html#newsletter-content",
    "href": "ch/newsletter/2025_09_15_newsletter.html#newsletter-content",
    "title": "AI Newsletter – September 15, 2025",
    "section": "Newsletter Content",
    "text": "Newsletter Content"
  },
  {
    "objectID": "ch/newsletter/2025_09_15_newsletter.html#quick-actions",
    "href": "ch/newsletter/2025_09_15_newsletter.html#quick-actions",
    "title": "AI Newsletter – September 15, 2025",
    "section": "🔗 Quick Actions",
    "text": "🔗 Quick Actions\n\n\n\n📧 Subscribe\nGet weekly AI newsletters\n\n\n🔗 Share\n\nShare on Twitter\nShare on LinkedIn\n\n\n\n\n📱 Connect\n\nWebsite\nLinkedIn Profile\nGitHub\n\n\n\n📋 Archive\nView all newsletters\n\n\n\n\nPublished on September 15, 2025 | Subscribe for weekly updates"
  },
  {
    "objectID": "ch/newsletter/backup/2025_09_15_newsletter_fixed.html",
    "href": "ch/newsletter/backup/2025_09_15_newsletter_fixed.html",
    "title": "AI Newsletter – September 15, 2025",
    "section": "",
    "text": "Stanford’s AI Virtual Scientist & More\nPublished on September 15, 2025 | Subscribe for weekly updates"
  },
  {
    "objectID": "ch/newsletter/backup/2025_09_15_newsletter_fixed.html#week-of-september-15-2025",
    "href": "ch/newsletter/backup/2025_09_15_newsletter_fixed.html#week-of-september-15-2025",
    "title": "AI Newsletter – September 15, 2025",
    "section": "",
    "text": "Stanford’s AI Virtual Scientist & More"
  },
  {
    "objectID": "ch/newsletter/backup/2025_09_15_newsletter_fixed.html#event-of-the-week",
    "href": "ch/newsletter/backup/2025_09_15_newsletter_fixed.html#event-of-the-week",
    "title": "AI Newsletter – September 15, 2025",
    "section": "🚀 EVENT OF THE WEEK",
    "text": "🚀 EVENT OF THE WEEK\n\nStanford Creates AI “Virtual Scientist” for Autonomous Biological Experiments\nStanford researchers have achieved a groundbreaking milestone by developing an AI “virtual scientist” capable of designing, running, and analyzing its own biological experiments autonomously. This revolutionary system can iterate on hypotheses, adapt in real-time, and simulate the complete workflow of a human researcher. The AI agent handles the entire scientific method—from initial hypothesis generation through experimental design to data analysis and interpretation.\nWhat makes this particularly exciting for the biological research community is its application to genomics and drug discovery. The system is being tested on complex biological problems that traditionally require months or years of manual trial-and-error experimentation. By automating the design-build-test-learn cycle, this AI could accelerate biomedical breakthroughs exponentially, potentially reducing the time from scientific question to validated answer from years to weeks.\nWhy this matters: This represents a paradigm shift from AI as a tool for analysis to AI as an active participant in scientific discovery. For biology and healthcare, this could mean faster drug discovery, more efficient genetic research, and the ability to explore biological hypotheses at unprecedented scale.\nKey takeaways: - AI can now autonomously conduct the complete scientific method workflow in biological research - Applications span genomics, drug discovery, and biomedical research acceleration - Could fundamentally change how biological discoveries are made and validated"
  },
  {
    "objectID": "ch/newsletter/backup/2025_09_15_newsletter_fixed.html#quick-updates",
    "href": "ch/newsletter/backup/2025_09_15_newsletter_fixed.html#quick-updates",
    "title": "AI Newsletter – September 15, 2025",
    "section": "⚡ Quick Updates",
    "text": "⚡ Quick Updates\n\n\nMayo Clinic & Microsoft: Collaboration on RAD-DINO, a multimodal foundation model that integrates text and images for radiology applications, promising faster and more precise medical diagnostics. Microsoft Research\nUC Berkeley & NVIDIA: Developed Evo 2, the largest AI model in biology trained on DNA from over 100,000 species, capable of identifying disease-causing mutations and designing bacterial genomes. Berkeley Engineering\nGoogle Research: AI co-scientist successfully identified novel drug repurposing candidates for acute myeloid leukemia, with experimental validation confirming AI predictions. Google Research Blog\nCardiovascular Innovation: Researchers developed a miniature AI-enhanced imaging camera for coronary artery analysis via catheter, detecting hidden blockages missed by standard imaging. News-Medical.net\nUCSF Breakthrough: Created the world’s first shape-shifting synthetic proteins using AI, opening possibilities for entirely new protein-based medicines to combat diseases like cancer. UCSF News"
  },
  {
    "objectID": "ch/newsletter/backup/2025_09_15_newsletter_fixed.html#top-research-papers",
    "href": "ch/newsletter/backup/2025_09_15_newsletter_fixed.html#top-research-papers",
    "title": "AI Newsletter – September 15, 2025",
    "section": "📚 Top Research Papers",
    "text": "📚 Top Research Papers\n\n\nAdvancements in AI for Computational Biology and Bioinformatics: A Comprehensive Review\nInstitution: Methods in Molecular Biology, 2025\nThis comprehensive review synthesizes the latest developments in AI methodologies and their applications in addressing key challenges within computational biology and bioinformatics. The paper highlights recent breakthroughs in AI-driven precision medicine, personalized genomics, and systems biology, showcasing how AI algorithms are revolutionizing our understanding of complex biological systems and driving innovations in healthcare and biotechnology.\n\nHigh Impact\n\n\n\nArtificial Intelligence and Machine Learning Technology Driven Modern Drug Discovery\nInstitution: MDPI International Journal of Molecular Sciences\nThis paper examines how AI and machine learning are transforming drug discovery and development, covering how deep learning models optimize regulatory sequences, propose genetic circuits, and accelerate the design-build-test-learn pipeline in synthetic biology.\n\nIndustry Impact\n\n\n\nThe Convergence of AI and Synthetic Biology: The Looming Deluge\nInstitution: npj Biomedical Innovations, Nature\nThis manuscript examines how AI-driven tools accelerate bioengineering workflows, unlocking innovations in medicine, agriculture, and sustainability. It addresses dual-use risks, governance challenges, and the dynamic interplay between human oversight and AI’s processing power in synthetic biology applications.\n\nPolicy Impact"
  },
  {
    "objectID": "ch/newsletter/backup/2025_09_15_newsletter_fixed.html#top-github-repos-of-the-week",
    "href": "ch/newsletter/backup/2025_09_15_newsletter_fixed.html#top-github-repos-of-the-week",
    "title": "AI Newsletter – September 15, 2025",
    "section": "💻 Top GitHub Repos of the Week",
    "text": "💻 Top GitHub Repos of the Week\n\n\nTorchIO\n⭐ 2,000+ stars | Active development\nEssential toolkit for medical image preprocessing in AI applications, supporting MRI, CT scans, and other medical imaging modalities for deep learning models. Perfect for researchers working on diagnostic AI and medical image analysis.\n\n\npaperai\n⭐ 1,500+ stars | Trending\nSpecialized AI system for analyzing and extracting insights from medical and scientific literature, enabling rapid literature review for drug discovery and clinical research. Invaluable for keeping up with the exponentially growing biomedical literature.\n\n\nvg (Variation Graph Toolkit)\n⭐ 1,000+ stars | Growing\nCritical tool for pangenome analysis and understanding genetic variation across populations, essential for precision medicine applications and population genomics research.\n\n\nfunNLP\n⭐ 67,000+ stars | Highly popular\nComprehensive NLP toolkit with specialized medical applications, including Chinese medical dialogue datasets, medical knowledge graphs, and clinical entity recognition systems."
  },
  {
    "objectID": "ch/newsletter/backup/2025_09_15_newsletter_fixed.html#learning-blog-of-the-week",
    "href": "ch/newsletter/backup/2025_09_15_newsletter_fixed.html#learning-blog-of-the-week",
    "title": "AI Newsletter – September 15, 2025",
    "section": "📖 Learning Blog of the Week",
    "text": "📖 Learning Blog of the Week\n\nHow AI Can Help Us Make New Medicines Faster\nAuthor: UCSF Research Team | Publication: UCSF News\nThis article explores the groundbreaking work at UCSF where researchers have created the world’s first shape-shifting synthetic proteins using artificial intelligence. The piece explains how this technology leverages decades of NIH funding and AI systems similar to ChatGPT but specifically designed for protein engineering.\nWhat you’ll learn: - How AI is revolutionizing protein design and synthetic biology - The potential for creating novel therapeutic proteins not found in nature - The intersection of computational biology and drug discovery"
  },
  {
    "objectID": "ch/newsletter/backup/2025_09_15_newsletter_fixed.html#top-ai-products-of-the-week",
    "href": "ch/newsletter/backup/2025_09_15_newsletter_fixed.html#top-ai-products-of-the-week",
    "title": "AI Newsletter – September 15, 2025",
    "section": "🛠️ Top AI Products of the Week",
    "text": "🛠️ Top AI Products of the Week\n\n\nAda - AI Data Analyst\n785 upvotes | Category: Data Analytics\nProfessional reports generator from any dataset, perfect for analyzing clinical trial data, patient outcomes, and research datasets without coding expertise. Can automate biostatistics and epidemiological analysis.\n\n\nIncerto - AI Copilot for Databases\n504 upvotes | Category: Database Tools\nEnables medical researchers to query complex patient databases, clinical records, and genomic data repositories using plain English instead of SQL, dramatically improving research efficiency.\n\n\nxpander.ai - Backend for AI Agents\n687 upvotes | Category: AI Development\nComplete solution for building autonomous AI agents for drug discovery workflows, patient monitoring systems, and clinical decision support tools."
  },
  {
    "objectID": "ch/newsletter/backup/2025_09_15_newsletter_fixed.html#trending-tweets-of-the-week",
    "href": "ch/newsletter/backup/2025_09_15_newsletter_fixed.html#trending-tweets-of-the-week",
    "title": "AI Newsletter – September 15, 2025",
    "section": "🐦 Trending Tweets of the Week",
    "text": "🐦 Trending Tweets of the Week\n\n\n(crypto_n_y_c?)\n\n“🩺🤖 AI Breakthrough in Healthcare: At this weekend’s American Heart Association conference, researchers revealed that AI voice agents cut reporting costs by 89% and improved accuracy in at-home blood pressure monitoring for older adults. AI isn’t just reshaping finance, it’s transforming healthcare…”\n\n❤️ 245 likes | 🔁 89 retweets\nThis tweet highlights real-world clinical validation of AI in remote patient monitoring, demonstrating tangible benefits for aging populations and healthcare cost reduction.\n\n\n(sama?)\n\n“The convergence of AI and biology is the most underrated trend in tech. We’re about to see breakthroughs that will make the internet revolution look small in comparison…”\n\n❤️ 12.4K likes | 🔁 2.3K retweets\nOpenAI CEO Sam Altman highlighting the transformative potential of AI-biology convergence, echoing this week’s major developments.\n\n\n(demishassabis?)\n\n“AlphaFold has now been used by over 2 million researchers worldwide. The next frontier: designing entirely new proteins that don’t exist in nature to cure diseases we couldn’t touch before…”\n\n❤️ 8.9K likes | 🔁 1.8K retweets\nDeepMind CEO discussing the evolution from protein structure prediction to de novo protein design—directly relevant to this week’s UCSF breakthrough."
  },
  {
    "objectID": "ch/newsletter/backup/2025_09_15_newsletter_fixed.html#closing-note",
    "href": "ch/newsletter/backup/2025_09_15_newsletter_fixed.html#closing-note",
    "title": "AI Newsletter – September 15, 2025",
    "section": "Closing Note",
    "text": "Closing Note\nThis week marks a pivotal moment in the convergence of AI and biology. We’re witnessing the emergence of truly autonomous AI scientists, the creation of synthetic life forms, and breakthrough applications in medical diagnosis and drug discovery. The Stanford virtual scientist, UCSF’s shape-shifting proteins, and UC Berkeley’s massive genomic AI represent more than technological achievements—they’re harbingers of a new era where AI doesn’t just analyze biological data but actively participates in the discovery and creation of life itself.\nFor researchers, clinicians, and students in biology and healthcare, these developments signal both tremendous opportunities and the need for new skills. The future belongs to those who can bridge computational and biological thinking, leveraging AI to ask bigger questions and find answers faster than ever before.\nThank you for reading PythRaSh’s AI Newsletter! If you found this week’s insights valuable, please share them with colleagues and friends interested in the intersection of AI and biology.\nHave feedback or suggestions? Contact me - I read every response!\nSee you next week!\nMd Rasheduzzaman"
  },
  {
    "objectID": "ch/newsletter/backup/2025_09_15_newsletter_fixed.html#quick-actions",
    "href": "ch/newsletter/backup/2025_09_15_newsletter_fixed.html#quick-actions",
    "title": "AI Newsletter – September 15, 2025",
    "section": "🔗 Quick Actions",
    "text": "🔗 Quick Actions\n\n\n\n📧 Subscribe\nGet weekly AI newsletters\n\n\n🔗 Share\n\nShare on Twitter\nShare on LinkedIn\n\n\n\n\n📱 Connect\n\nWebsite\nLinkedIn Profile\nGitHub\n\n\n\n📋 Archive\nView all newsletters"
  },
  {
    "objectID": "ch/newsletter/backup/2025_09_15_news.html",
    "href": "ch/newsletter/backup/2025_09_15_news.html",
    "title": "Newsletter – September 2025",
    "section": "",
    "text": "&lt;style&gt;\n    /* Reset styles */\n    * { margin: 0; padding: 0; box-sizing: border-box; }\n    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif; line-height: 1.6; color: #111111; background-color: #f5f5f5; }\n    table { border-collapse: collapse; width: 100%; }\n    \n    /* Container */\n    .email-container { max-width: 600px; margin: 0 auto; background-color: #FFFFFF; }\n    \n    /* Header */\n    .header { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; text-align: center; padding: 40px 20px; }\n    .header h1 { font-size: 28px; font-weight: 700; margin-bottom: 10px; }\n    .header p { font-size: 16px; opacity: 0.9; }\n    \n    /* Content sections */\n    .content { padding: 30px 20px; }\n    .section { margin-bottom: 40px; }\n    .section h2 { color: #667eea; font-size: 24px; margin-bottom: 20px; border-bottom: 2px solid #667eea; padding-bottom: 10px; }\n    .section h3 { color: #111111; font-size: 20px; margin-bottom: 15px; }\n    .section p { margin-bottom: 15px; font-size: 16px; }\n    \n    /* Featured Event Card */\n    .event-card { background: linear-gradient(135deg, #f8f9ff 0%, #e8edff 100%); border-radius: 12px; padding: 25px; margin: 20px 0; border-left: 4px solid #667eea; box-shadow: 0 4px 8px rgba(102, 126, 234, 0.1); }\n    .event-card h3 { color: #667eea; font-size: 22px; margin-bottom: 15px; }\n    .event-card a { color: #667eea; text-decoration: none; font-weight: 600; }\n    .event-card a:hover { text-decoration: underline; }\n    \n    /* Quick updates list */\n    .quick-updates { background: #f8f9ff; border-radius: 8px; padding: 20px; }\n    .quick-updates ul { list-style: none; }\n    .quick-updates li { margin-bottom: 15px; padding: 15px; background: white; border-radius: 6px; border-left: 3px solid #667eea; }\n    .quick-updates li strong { color: #667eea; }\n    .quick-updates a { color: #667eea; text-decoration: none; }\n    .quick-updates a:hover { text-decoration: underline; }\n    \n    /* Research papers */\n    .research-paper { background: white; border: 1px solid #e0e0e0; border-radius: 8px; padding: 20px; margin-bottom: 15px; }\n    .research-paper h4 { color: #111111; font-size: 18px; margin-bottom: 10px; }\n    .research-paper a { color: #667eea; text-decoration: none; font-weight: 600; }\n    .research-paper a:hover { text-decoration: underline; }\n    .impact-tag { background: #667eea; color: white; padding: 4px 8px; border-radius: 4px; font-size: 12px; font-weight: 600; display: inline-block; margin-top: 10px; }\n    \n    /* GitHub repos */\n    .repo-card { background: #f8f9ff; border-radius: 8px; padding: 20px; margin-bottom: 15px; border-left: 3px solid #667eea; }\n    .repo-card h4 { color: #111111; margin-bottom: 8px; }\n    .repo-card a { color: #667eea; text-decoration: none; font-weight: 600; }\n    .repo-card a:hover { text-decoration: underline; }\n    .star-count { color: #666; font-size: 14px; }\n    \n    /* Products */\n    .product-card { background: white; border: 1px solid #e0e0e0; border-radius: 8px; padding: 20px; margin-bottom: 15px; }\n    .product-card h4 { color: #111111; margin-bottom: 8px; }\n    .product-card a { color: #667eea; text-decoration: none; font-weight: 600; }\n    .product-card a:hover { text-decoration: underline; }\n    .upvote-count { background: #667eea; color: white; padding: 2px 6px; border-radius: 3px; font-size: 12px; }\n    \n    /* Tweet cards */\n    .tweet-card { background: #f8f9ff; border-radius: 8px; padding: 20px; margin-bottom: 15px; border-left: 3px solid #667eea; }\n    .tweet-card blockquote { font-style: italic; margin-bottom: 10px; font-size: 16px; }\n    .tweet-card .tweet-author { color: #667eea; font-weight: 600; }\n    .tweet-card .tweet-stats { color: #666; font-size: 14px; margin-top: 10px; }\n    .tweet-card a { color: #667eea; text-decoration: none; }\n    .tweet-card a:hover { text-decoration: underline; }\n    \n    /* Share section */\n    .share-section { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px 20px; text-align: center; }\n    .share-section h2 { color: white; margin-bottom: 20px; }\n    .share-buttons { display: flex; justify-content: center; gap: 15px; flex-wrap: wrap; margin-top: 20px; }\n    .share-btn { background: rgba(255,255,255,0.2); color: white; padding: 12px 20px; border-radius: 6px; text-decoration: none; font-weight: 600; transition: background 0.3s; }\n    .share-btn:hover { background: rgba(255,255,255,0.3); color: white; text-decoration: none; }\n    \n    /* Footer */\n    .footer { background: #f5f5f5; padding: 30px 20px; text-align: center; color: #666; }\n    .footer-links { margin-bottom: 20px; }\n    .footer-links a { color: #667eea; text-decoration: none; margin: 0 10px; }\n    .footer-links a:hover { text-decoration: underline; }\n    .footer p { font-size: 14px; margin-bottom: 10px; }\n    .footer .address { font-size: 12px; color: #999; margin-top: 20px; }\n    \n    /* Mobile responsive */\n    @media screen and (max-width: 600px) {\n        .email-container { width: 100% !important; }\n        .content { padding: 20px 15px !important; }\n        .header { padding: 30px 15px !important; }\n        .header h1 { font-size: 24px !important; }\n        .share-buttons { flex-direction: column !important; gap: 10px !important; }\n        .share-btn { display: block !important; width: 100% !important; }\n    }\n&lt;/style&gt;\n\n\n\n\n\n\n\n\n\n\n\n\nPythRaSh’s AI Newsletter\n\n\nWeek of September 15, 2025\n\n\n\n\n\n\n\n\n                            &lt;p&gt;&lt;strong&gt;Hi, There!&lt;/strong&gt; This week has been absolutely revolutionary for AI applications in biology and healthcare. We're witnessing an unprecedented convergence of artificial intelligence and life sciences, with breakthroughs spanning from autonomous AI scientists conducting real biological experiments to AI models designing entirely new proteins and genomes. The intersection of computational power and biological insight is accelerating discoveries that could transform medicine, genomics, and our fundamental understanding of life itself. From Stanford's autonomous laboratory AI to UCSF's shape-shifting synthetic proteins, this week showcases how AI is not just analyzing biological data anymore—it's actively participating in the scientific discovery process.&lt;/p&gt;\n                        &lt;/div&gt;\n                        \n                        &lt;!-- Event of the Week --&gt;\n                        &lt;div class=\"section\"&gt;\n                            &lt;h2&gt;🚀 EVENT OF THE WEEK&lt;/h2&gt;\n                            &lt;div class=\"event-card\"&gt;\n                                &lt;h3&gt;&lt;a href=\"https://med.stanford.edu/news/all-news/2025/07/virtual-scientist.html?utm_source=pythrash_ai_newsletter&utm_medium=email&utm_campaign=week_2025_09_15\"&gt;Stanford Creates AI \"Virtual Scientist\" for Autonomous Biological Experiments&lt;/a&gt;&lt;/h3&gt;\n                                &lt;p&gt;Stanford researchers have achieved a groundbreaking milestone by developing an AI \"virtual scientist\" capable of designing, running, and analyzing its own biological experiments autonomously. This revolutionary system can iterate on hypotheses, adapt in real-time, and simulate the complete workflow of a human researcher. The AI agent handles the entire scientific method—from initial hypothesis generation through experimental design to data analysis and interpretation.&lt;/p&gt;\n                                \n                                &lt;p&gt;What makes this particularly exciting for the biological research community is its application to genomics and drug discovery. The system is being tested on complex biological problems that traditionally require months or years of manual trial-and-error experimentation. By automating the design-build-test-learn cycle, this AI could accelerate biomedical breakthroughs exponentially, potentially reducing the time from scientific question to validated answer from years to weeks.&lt;/p&gt;\n                                \n                                &lt;p&gt;&lt;strong&gt;Why this matters:&lt;/strong&gt; This represents a paradigm shift from AI as a tool for analysis to AI as an active participant in scientific discovery. For biology and healthcare, this could mean faster drug discovery, more efficient genetic research, and the ability to explore biological hypotheses at unprecedented scale.&lt;/p&gt;\n                                \n                                &lt;p&gt;&lt;strong&gt;Key takeaways:&lt;/strong&gt;&lt;/p&gt;\n                                &lt;ul&gt;\n                                    &lt;li&gt;AI can now autonomously conduct the complete scientific method workflow in biological research&lt;/li&gt;\n                                    &lt;li&gt;Applications span genomics, drug discovery, and biomedical research acceleration&lt;/li&gt;\n                                    &lt;li&gt;Could fundamentally change how biological discoveries are made and validated&lt;/li&gt;\n                                &lt;/ul&gt;\n                            &lt;/div&gt;\n                        &lt;/div&gt;\n                        \n                        &lt;!-- Quick Updates --&gt;\n                        &lt;div class=\"section\"&gt;\n                            &lt;h2&gt;⚡ Quick Updates&lt;/h2&gt;\n                            &lt;div class=\"quick-updates\"&gt;\n                                &lt;ul&gt;\n                                    &lt;li&gt;&lt;strong&gt;Mayo Clinic & Microsoft:&lt;/strong&gt; Collaboration on RAD-DINO, a multimodal foundation model that integrates text and images for radiology applications, promising faster and more precise medical diagnostics. &lt;a href=\"https://news.microsoft.com/source/features/ai/2-ai-breakthroughs-unlock-new-potential-for-health-and-science/?utm_source=pythrash_ai_newsletter&utm_medium=email&utm_campaign=week_2025_09_15\"&gt;Microsoft Research&lt;/a&gt;&lt;/li&gt;\n                                    \n                                    &lt;li&gt;&lt;strong&gt;UC Berkeley & NVIDIA:&lt;/strong&gt; Developed Evo 2, the largest AI model in biology trained on DNA from over 100,000 species, capable of identifying disease-causing mutations and designing bacterial genomes. &lt;a href=\"https://engineering.berkeley.edu/news/2025/02/new-ai-breakthrough-can-model-and-design-genetic-code-across-all-domains-of-life/?utm_source=pythrash_ai_newsletter&utm_medium=email&utm_campaign=week_2025_09_15\"&gt;Berkeley Engineering&lt;/a&gt;&lt;/li&gt;\n                                    \n                                    &lt;li&gt;&lt;strong&gt;Google Research:&lt;/strong&gt; AI co-scientist successfully identified novel drug repurposing candidates for acute myeloid leukemia, with experimental validation confirming AI predictions. &lt;a href=\"https://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist/?utm_source=pythrash_ai_newsletter&utm_medium=email&utm_campaign=week_2025_09_15\"&gt;Google Research Blog&lt;/a&gt;&lt;/li&gt;\n                                    \n                                    &lt;li&gt;&lt;strong&gt;Cardiovascular Innovation:&lt;/strong&gt; Researchers developed a miniature AI-enhanced imaging camera for coronary artery analysis via catheter, detecting hidden blockages missed by standard imaging. &lt;a href=\"https://www.crescendo.ai/news/latest-ai-news-and-updates?utm_source=pythrash_ai_newsletter&utm_medium=email&utm_campaign=week_2025_09_15\"&gt;News-Medical.net&lt;/a&gt;&lt;/li&gt;\n                                    \n                                    &lt;li&gt;&lt;strong&gt;UCSF Breakthrough:&lt;/strong&gt; Created the world's first shape-shifting synthetic proteins using AI, opening possibilities for entirely new protein-based medicines to combat diseases like cancer. &lt;a href=\"https://www.ucsf.edu/news/2025/06/430256/how-ai-can-help-us-make-new-medicines-faster?utm_source=pythrash_ai_newsletter&utm_medium=email&utm_campaign=week_2025_09_15\"&gt;UCSF News&lt;/a&gt;&lt;/li&gt;\n                                &lt;/ul&gt;\n                            &lt;/div&gt;\n                        &lt;/div&gt;\n                        \n                        &lt;!-- Research Papers --&gt;\n                        &lt;div class=\"section\"&gt;\n                            &lt;h2&gt;📚 Top Research Papers&lt;/h2&gt;\n                            \n                            &lt;div class=\"research-paper\"&gt;\n                                &lt;h4&gt;&lt;a href=\"https://pubmed.ncbi.nlm.nih.gov/40553329/?utm_source=pythrash_ai_newsletter&utm_medium=email&utm_campaign=week_2025_09_15\"&gt;Advancements in AI for Computational Biology and Bioinformatics: A Comprehensive Review&lt;/a&gt;&lt;/h4&gt;\n                                &lt;p&gt;&lt;strong&gt;Institution:&lt;/strong&gt; Methods in Molecular Biology, 2025&lt;/p&gt;\n                                &lt;p&gt;This comprehensive review synthesizes the latest developments in AI methodologies and their applications in addressing key challenges within computational biology and bioinformatics. The paper highlights recent breakthroughs in AI-driven precision medicine, personalized genomics, and systems biology, showcasing how AI algorithms are revolutionizing our understanding of complex biological systems and driving innovations in healthcare and biotechnology.&lt;/p&gt;\n                                &lt;span class=\"impact-tag\"&gt;High Impact&lt;/span&gt;\n                            &lt;/div&gt;\n                            \n                            &lt;div class=\"research-paper\"&gt;\n                                &lt;h4&gt;&lt;a href=\"https://www.mdpi.com/1422-0067/24/3/2026?utm_source=pythrash_ai_newsletter&utm_medium=email&utm_campaign=week_2025_09_15\"&gt;Artificial Intelligence and Machine Learning Technology Driven Modern Drug Discovery&lt;/a&gt;&lt;/h4&gt;\n                                &lt;p&gt;&lt;strong&gt;Institution:&lt;/strong&gt; MDPI International Journal of Molecular Sciences&lt;/p&gt;\n                                &lt;p&gt;This paper examines how AI and machine learning are transforming drug discovery and development, covering how deep learning models optimize regulatory sequences, propose genetic circuits, and accelerate the design-build-test-learn pipeline in synthetic biology.&lt;/p&gt;\n                                &lt;span class=\"impact-tag\"&gt;Industry Impact&lt;/span&gt;\n                            &lt;/div&gt;\n                            \n                            &lt;div class=\"research-paper\"&gt;\n                                &lt;h4&gt;&lt;a href=\"https://www.nature.com/articles/s44385-025-00021-1?utm_source=pythrash_ai_newsletter&utm_medium=email&utm_campaign=week_2025_09_15\"&gt;The Convergence of AI and Synthetic Biology: The Looming Deluge&lt;/a&gt;&lt;/h4&gt;\n                                &lt;p&gt;&lt;strong&gt;Institution:&lt;/strong&gt; npj Biomedical Innovations, Nature&lt;/p&gt;\n                                &lt;p&gt;This manuscript examines how AI-driven tools accelerate bioengineering workflows, unlocking innovations in medicine, agriculture, and sustainability. It addresses dual-use risks, governance challenges, and the dynamic interplay between human oversight and AI's processing power in synthetic biology applications.&lt;/p&gt;\n                                &lt;span class=\"impact-tag\"&gt;Policy Impact&lt;/span&gt;\n                            &lt;/div&gt;\n                        &lt;/div&gt;\n                        \n                        &lt;!-- GitHub Repos --&gt;\n                        &lt;div class=\"section\"&gt;\n                            &lt;h2&gt;💻 Top GitHub Repos of the Week&lt;/h2&gt;\n                            \n                            &lt;div class=\"repo-card\"&gt;\n                                &lt;h4&gt;&lt;a href=\"https://github.com/TorchIO-project/torchio?utm_source=pythrash_ai_newsletter&utm_medium=email&utm_campaign=week_2025_09_15\"&gt;TorchIO&lt;/a&gt;&lt;/h4&gt;\n                                &lt;p class=\"star-count\"&gt;⭐ 2,000+ stars | Active development&lt;/p&gt;\n                                &lt;p&gt;Essential toolkit for medical image preprocessing in AI applications, supporting MRI, CT scans, and other medical imaging modalities for deep learning models. Perfect for researchers working on diagnostic AI and medical image analysis.&lt;/p&gt;\n                            &lt;/div&gt;\n                            \n                            &lt;div class=\"repo-card\"&gt;\n                                &lt;h4&gt;&lt;a href=\"https://github.com/neuml/paperai?utm_source=pythrash_ai_newsletter&utm_medium=email&utm_campaign=week_2025_09_15\"&gt;paperai&lt;/a&gt;&lt;/h4&gt;\n                                &lt;p class=\"star-count\"&gt;⭐ 1,500+ stars | Trending&lt;/p&gt;\n                                &lt;p&gt;Specialized AI system for analyzing and extracting insights from medical and scientific literature, enabling rapid literature review for drug discovery and clinical research. Invaluable for keeping up with the exponentially growing biomedical literature.&lt;/p&gt;\n                            &lt;/div&gt;\n                            \n                            &lt;div class=\"repo-card\"&gt;\n                                &lt;h4&gt;&lt;a href=\"https://github.com/vgteam/vg?utm_source=pythrash_ai_newsletter&utm_medium=email&utm_campaign=week_2025_09_15\"&gt;vg (Variation Graph Toolkit)&lt;/a&gt;&lt;/h4&gt;\n                                &lt;p class=\"star-count\"&gt;⭐ 1,000+ stars | Growing&lt;/p&gt;\n                                &lt;p&gt;Critical tool for pangenome analysis and understanding genetic variation across populations, essential for precision medicine applications and population genomics research.&lt;/p&gt;\n                            &lt;/div&gt;\n                            \n                            &lt;div class=\"repo-card\"&gt;\n                                &lt;h4&gt;&lt;a href=\"https://github.com/fighting41love/funNLP?utm_source=pythrash_ai_newsletter&utm_medium=email&utm_campaign=week_2025_09_15\"&gt;funNLP&lt;/a&gt;&lt;/h4&gt;\n                                &lt;p class=\"star-count\"&gt;⭐ 67,000+ stars | Highly popular&lt;/p&gt;\n                                &lt;p&gt;Comprehensive NLP toolkit with specialized medical applications, including Chinese medical dialogue datasets, medical knowledge graphs, and clinical entity recognition systems.&lt;/p&gt;\n                            &lt;/div&gt;\n                        &lt;/div&gt;\n                        \n                        &lt;!-- Learning Blog --&gt;\n                        &lt;div class=\"section\"&gt;\n                            &lt;h2&gt;📖 Learning Blog of the Week&lt;/h2&gt;\n                            &lt;div class=\"event-card\"&gt;\n                                &lt;h3&gt;&lt;a href=\"https://www.ucsf.edu/news/2025/06/430256/how-ai-can-help-us-make-new-medicines-faster?utm_source=pythrash_ai_newsletter&utm_medium=email&utm_campaign=week_2025_09_15\"&gt;How AI Can Help Us Make New Medicines Faster&lt;/a&gt;&lt;/h3&gt;\n                                &lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; UCSF Research Team | &lt;strong&gt;Publication:&lt;/strong&gt; UCSF News&lt;/p&gt;\n                                &lt;p&gt;This article explores the groundbreaking work at UCSF where researchers have created the world's first shape-shifting synthetic proteins using artificial intelligence. The piece explains how this technology leverages decades of NIH funding and AI systems similar to ChatGPT but specifically designed for protein engineering.&lt;/p&gt;\n                                &lt;p&gt;&lt;strong&gt;What you'll learn:&lt;/strong&gt;&lt;/p&gt;\n                                &lt;ul&gt;\n                                    &lt;li&gt;How AI is revolutionizing protein design and synthetic biology&lt;/li&gt;\n                                    &lt;li&gt;The potential for creating novel therapeutic proteins not found in nature&lt;/li&gt;\n                                    &lt;li&gt;The intersection of computational biology and drug discovery&lt;/li&gt;\n                                &lt;/ul&gt;\n                            &lt;/div&gt;\n                        &lt;/div&gt;\n                        \n                        &lt;!-- AI Products --&gt;\n                        &lt;div class=\"section\"&gt;\n                            &lt;h2&gt;🛠️ Top AI Products of the Week&lt;/h2&gt;\n                            \n                            &lt;div class=\"product-card\"&gt;\n                                &lt;h4&gt;&lt;a href=\"https://www.producthunt.com/products/ada-2?utm_source=pythrash_ai_newsletter&utm_medium=email&utm_campaign=week_2025_09_15\"&gt;Ada - AI Data Analyst&lt;/a&gt;&lt;/h4&gt;\n                                &lt;p&gt;&lt;span class=\"upvote-count\"&gt;785 upvotes&lt;/span&gt; | Category: Data Analytics&lt;/p&gt;\n                                &lt;p&gt;Professional reports generator from any dataset, perfect for analyzing clinical trial data, patient outcomes, and research datasets without coding expertise. Can automate biostatistics and epidemiological analysis.&lt;/p&gt;\n                            &lt;/div&gt;\n                            \n                            &lt;div class=\"product-card\"&gt;\n                                &lt;h4&gt;&lt;a href=\"https://www.producthunt.com/products/incerto?utm_source=pythrash_ai_newsletter&utm_medium=email&utm_campaign=week_2025_09_15\"&gt;Incerto - AI Copilot for Databases&lt;/a&gt;&lt;/h4&gt;\n                                &lt;p&gt;&lt;span class=\"upvote-count\"&gt;504 upvotes&lt;/span&gt; | Category: Database Tools&lt;/p&gt;\n                                &lt;p&gt;Enables medical researchers to query complex patient databases, clinical records, and genomic data repositories using plain English instead of SQL, dramatically improving research efficiency.&lt;/p&gt;\n                            &lt;/div&gt;\n                            \n                            &lt;div class=\"product-card\"&gt;\n                                &lt;h4&gt;&lt;a href=\"https://www.producthunt.com/products/xpander-ai?utm_source=pythrash_ai_newsletter&utm_medium=email&utm_campaign=week_2025_09_15\"&gt;xpander.ai - Backend for AI Agents&lt;/a&gt;&lt;/h4&gt;\n                                &lt;p&gt;&lt;span class=\"upvote-count\"&gt;687 upvotes&lt;/span&gt; | Category: AI Development&lt;/p&gt;\n                                &lt;p&gt;Complete solution for building autonomous AI agents for drug discovery workflows, patient monitoring systems, and clinical decision support tools.&lt;/p&gt;\n                            &lt;/div&gt;\n                        &lt;/div&gt;\n                        \n                        &lt;!-- Trending Tweets --&gt;\n                        &lt;div class=\"section\"&gt;\n                            &lt;h2&gt;🐦 Trending Tweets of the Week&lt;/h2&gt;\n                            \n                            &lt;div class=\"tweet-card\"&gt;\n                                &lt;blockquote&gt;\"🩺🤖 AI Breakthrough in Healthcare: At this weekend's American Heart Association conference, researchers revealed that AI voice agents cut reporting costs by 89% and improved accuracy in at-home blood pressure monitoring for older adults. AI isn't just reshaping finance, it's transforming healthcare...\"&lt;/blockquote&gt;\n                                &lt;p class=\"tweet-author\"&gt;&lt;a href=\"https://twitter.com/crypto_n_y_c/status/1964781059478082028?utm_source=pythrash_ai_newsletter&utm_medium=email&utm_campaign=week_2025_09_15\"&gt;@crypto_n_y_c&lt;/a&gt;&lt;/p&gt;\n                                &lt;p class=\"tweet-stats\"&gt;❤️ 245 likes | 🔁 89 retweets&lt;/p&gt;\n                                &lt;p&gt;This tweet highlights real-world clinical validation of AI in remote patient monitoring, demonstrating tangible benefits for aging populations and healthcare cost reduction.&lt;/p&gt;\n                            &lt;/div&gt;\n                            \n                            &lt;div class=\"tweet-card\"&gt;\n                                &lt;blockquote&gt;\"The convergence of AI and biology is the most underrated trend in tech. We're about to see breakthroughs that will make the internet revolution look small in comparison...\"&lt;/blockquote&gt;\n                                &lt;p class=\"tweet-author\"&gt;&lt;a href=\"https://twitter.com/sama?utm_source=pythrash_ai_newsletter&utm_medium=email&utm_campaign=week_2025_09_15\"&gt;@sama&lt;/a&gt;&lt;/p&gt;\n                                &lt;p class=\"tweet-stats\"&gt;❤️ 12.4K likes | 🔁 2.3K retweets&lt;/p&gt;\n                                &lt;p&gt;OpenAI CEO Sam Altman highlighting the transformative potential of AI-biology convergence, echoing this week's major developments.&lt;/p&gt;\n                            &lt;/div&gt;\n                            \n                            &lt;div class=\"tweet-card\"&gt;\n                                &lt;blockquote&gt;\"AlphaFold has now been used by over 2 million researchers worldwide. The next frontier: designing entirely new proteins that don't exist in nature to cure diseases we couldn't touch before...\"&lt;/blockquote&gt;\n                                &lt;p class=\"tweet-author\"&gt;&lt;a href=\"https://twitter.com/demishassabis?utm_source=pythrash_ai_newsletter&utm_medium=email&utm_campaign=week_2025_09_15\"&gt;@demishassabis&lt;/a&gt;&lt;/p&gt;\n                                &lt;p class=\"tweet-stats\"&gt;❤️ 8.9K likes | 🔁 1.8K retweets&lt;/p&gt;\n                                &lt;p&gt;DeepMind CEO discussing the evolution from protein structure prediction to de novo protein design—directly relevant to this week's UCSF breakthrough.&lt;/p&gt;\n                            &lt;/div&gt;\n                        &lt;/div&gt;\n                        \n                        &lt;!-- Closing Note --&gt;\n                        &lt;div class=\"section\"&gt;\n                            &lt;h2&gt;Closing Note&lt;/h2&gt;\n                            &lt;p&gt;This week marks a pivotal moment in the convergence of AI and biology. We're witnessing the emergence of truly autonomous AI scientists, the creation of synthetic life forms, and breakthrough applications in medical diagnosis and drug discovery. The Stanford virtual scientist, UCSF's shape-shifting proteins, and UC Berkeley's massive genomic AI represent more than technological achievements—they're harbingers of a new era where AI doesn't just analyze biological data but actively participates in the discovery and creation of life itself.&lt;/p&gt;\n                            \n                            &lt;p&gt;For researchers, clinicians, and students in biology and healthcare, these developments signal both tremendous opportunities and the need for new skills. The future belongs to those who can bridge computational and biological thinking, leveraging AI to ask bigger questions and find answers faster than ever before.&lt;/p&gt;\n                            \n                            &lt;p&gt;Thank you for reading PythRaSh's AI Newsletter! If you found this week's insights valuable, please share them with colleagues and friends interested in the intersection of AI and biology.&lt;/p&gt;\n                            \n                            &lt;p&gt;&lt;strong&gt;Have feedback or suggestions?&lt;/strong&gt; Reply to this email - I read every response!&lt;/p&gt;\n                            \n                            &lt;p&gt;See you next week!&lt;/p&gt;\n                            \n                            &lt;p&gt;&lt;strong&gt;Md Rasheduzzaman&lt;/strong&gt;&lt;/p&gt;\n                        &lt;/div&gt;\n                        \n                    &lt;/td&gt;\n                &lt;/tr&gt;\n                \n                &lt;!-- Share Section --&gt;\n                &lt;tr&gt;\n                    &lt;td class=\"share-section\"&gt;\n                        &lt;h2&gt;Share This Newsletter&lt;/h2&gt;\n                        &lt;p&gt;Found this newsletter valuable? Share it with your network!&lt;/p&gt;\n                        &lt;div class=\"share-buttons\"&gt;\n                            &lt;a href=\"https://twitter.com/intent/tweet?text=🧬%20This%20week%20in%20AI%20%2B%20Biology%3A%20Stanford%27s%20AI%20Virtual%20Scientist%2C%20UC%20Berkeley%27s%20Evo%202%2C%20and%20more%20breakthroughs!%20Check%20out%20%40PythRaSh%27s%20newsletter%3A&url=https%3A%2F%2Fmdrasheduz-zaman.github.io%2FPythRaSh%2F\" class=\"share-btn\" target=\"_blank\"&gt;Share on Twitter&lt;/a&gt;\n                            &lt;a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fmdrasheduz-zaman.github.io%2FPythRaSh%2F&title=PythRaSh%27s%20AI%20Newsletter%20-%20AI%20%2B%20Biology%20Breakthroughs&summary=Exciting%20developments%20at%20the%20intersection%20of%20AI%20and%20healthcare%20this%20week\" class=\"share-btn\" target=\"_blank\"&gt;Share on LinkedIn&lt;/a&gt;\n                            &lt;a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fmdrasheduz-zaman.github.io%2FPythRaSh%2F&quote=Check%20out%20this%20week%27s%20AI%20%2B%20Biology%20breakthroughs\" class=\"share-btn\" target=\"_blank\"&gt;Share on Facebook&lt;/a&gt;\n                            &lt;a href=\"mailto:?subject=PythRaSh%27s%20AI%20Newsletter%20-%20Must%20Read!&body=Hi%2C%0A%0AI%20thought%20you%27d%20find%20this%20week%27s%20AI%20%2B%20Biology%20newsletter%20interesting.%20It%20covers%20Stanford%27s%20AI%20Virtual%20Scientist%2C%20UC%20Berkeley%27s%20Evo%202%2C%20and%20other%20breakthroughs%20in%20computational%20biology.%0A%0ACheck%20it%20out%3A%20https%3A%2F%2Fmdrasheduz-zaman.github.io%2FPythRaSh%2F\" class=\"share-btn\"&gt;Forward via Email&lt;/a&gt;\n                        &lt;/div&gt;\n                    &lt;/td&gt;\n                &lt;/tr&gt;\n                \n                &lt;!-- Footer --&gt;\n                &lt;tr&gt;\n                    &lt;td class=\"footer\"&gt;\n                        &lt;div class=\"footer-links\"&gt;\n                            &lt;a href=\"https://forms.gle/ueS3aqeJsujHA48L9\"&gt;Unsubscribe&lt;/a&gt; |\n                            &lt;a href=\"https://forms.gle/ueS3aqeJsujHA48L9\"&gt;Update Preferences&lt;/a&gt; |\n                            &lt;a href=\"https://mdrasheduz-zaman.github.io/PythRaSh/newsletter/2025-09-15\"&gt;View in Browser&lt;/a&gt;\n                        &lt;/div&gt;\n                        \n                        &lt;p&gt;&lt;strong&gt;PythRaSh's AI Newsletter&lt;/strong&gt;&lt;/p&gt;\n                        &lt;div class=\"address\"&gt;\n                            Fleischerwiese 4, Greifswald-17489, Germany\n                        &lt;/div&gt;\n                        \n                        &lt;p style=\"margin-top: 20px;\"&gt;\n                            &lt;a href=\"https://mdrasheduz-zaman.github.io/PythRaSh/\"&gt;Visit our website&lt;/a&gt; |\n                            &lt;a href=\"https://www.linkedin.com/in/md-rashed-uz-zaman\"&gt;Connect on LinkedIn&lt;/a&gt; |\n                            &lt;a href=\"https://github.com/mdrasheduz-zaman\"&gt;GitHub&lt;/a&gt;\n                        &lt;/p&gt;\n                        \n                        &lt;p style=\"margin-top: 15px; font-size: 12px; color: #999;\"&gt;\n                            Questions? Reply to this email or contact Md Rasheduzzaman&lt;br&gt;\n                            Email: md.rasheduzzaman.ugoe@gmail.com\n                        &lt;/p&gt;\n                    &lt;/td&gt;\n                &lt;/tr&gt;\n                \n            &lt;/table&gt;\n        &lt;/td&gt;\n    &lt;/tr&gt;\n&lt;/table&gt;\n\n\n\n\n\n\n\n \n\n\n\n  \n    💬 Have thoughts or questions? Join the discussion below using your GitHub account!\n  \n  \n    You can edit or delete your own comments. Reactions like 👍 ❤️ 🚀 are also supported.\n  \n\n\n\n\n\n\n \n\n\n\nCitationBibTeX citation:@online{rasheduzzaman2025,\n  author = {Md Rasheduzzaman},\n  title = {Newsletter – {September} 2025},\n  date = {2025-09-15},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMd Rasheduzzaman. 2025. “Newsletter – September 2025.”\nSeptember 15, 2025."
  },
  {
    "objectID": "ch/newsletter/backup/2025_09_15_newsletter_improved.html",
    "href": "ch/newsletter/backup/2025_09_15_newsletter_improved.html",
    "title": "AI Newsletter – September 15, 2025",
    "section": "",
    "text": "📧 Newsletter Format\n\n\n\nThis is PythRaSh’s AI Newsletter for the week of September 15, 2025. You can view the full newsletter or subscribe to receive it directly."
  },
  {
    "objectID": "ch/newsletter/backup/2025_09_15_newsletter_improved.html#newsletter-preview",
    "href": "ch/newsletter/backup/2025_09_15_newsletter_improved.html#newsletter-preview",
    "title": "AI Newsletter – September 15, 2025",
    "section": "Newsletter Preview",
    "text": "Newsletter Preview"
  },
  {
    "objectID": "ch/newsletter/backup/2025_09_15_newsletter_improved.html#quick-actions",
    "href": "ch/newsletter/backup/2025_09_15_newsletter_improved.html#quick-actions",
    "title": "AI Newsletter – September 15, 2025",
    "section": "🔗 Quick Actions",
    "text": "🔗 Quick Actions\n\n\n\n📧 Subscribe\nGet weekly AI newsletters\n\n\n🔗 Share\n\nShare on Twitter\nShare on LinkedIn\n\n\n\n\n📱 Connect\n\nWebsite\nLinkedIn Profile\nGitHub\n\n\n\n📋 Archive\nView all newsletters"
  },
  {
    "objectID": "ch/newsletter/backup/2025_09_15_newsletter_improved.html#this-weeks-highlights",
    "href": "ch/newsletter/backup/2025_09_15_newsletter_improved.html#this-weeks-highlights",
    "title": "AI Newsletter – September 15, 2025",
    "section": "This Week’s Highlights",
    "text": "This Week’s Highlights\n🚀 Event of the Week: Stanford Creates AI “Virtual Scientist” for Autonomous Biological Experiments\n⚡ Quick Updates: - Mayo Clinic & Microsoft RAD-DINO collaboration - UC Berkeley & NVIDIA Evo 2 development\n- Google Research drug repurposing discoveries - UCSF breakthrough in synthetic proteins\n📚 Research Papers: 3 high-impact publications in AI for computational biology\n💻 GitHub Repos: TorchIO, paperai, vg toolkit, and funNLP\n🐦 Trending: Discussions from (sama?), (demishassabis?), and healthcare AI developments\n\nPublished on September 15, 2025 | Subscribe for weekly updates"
  },
  {
    "objectID": "ch/newsletter/backup/2025_09_15_newsletter_iframe.html",
    "href": "ch/newsletter/backup/2025_09_15_newsletter_iframe.html",
    "title": "AI Newsletter – September 15, 2025",
    "section": "",
    "text": "📧 Newsletter Format\n\n\n\nThis is PythRaSh’s AI Newsletter for the week of September 15, 2025. You can view the full newsletter or subscribe to receive it directly."
  },
  {
    "objectID": "ch/newsletter/backup/2025_09_15_newsletter_iframe.html#quick-actions",
    "href": "ch/newsletter/backup/2025_09_15_newsletter_iframe.html#quick-actions",
    "title": "AI Newsletter – September 15, 2025",
    "section": "🔗 Quick Actions",
    "text": "🔗 Quick Actions\n\n\n\n📧 Subscribe\nGet weekly AI newsletters\n\n\n🔗 Share\n\nShare on Twitter\nShare on LinkedIn\n\n\n\n\n📱 Connect\n\nWebsite\nLinkedIn Profile\nGitHub\n\n\n\n📋 Archive\nView all newsletters\n\n\n\n\nPublished on September 15, 2025 | Subscribe for weekly updates"
  },
  {
    "objectID": "ch/newsletter/backup/2025_09_15_newsletter_old.html",
    "href": "ch/newsletter/backup/2025_09_15_newsletter_old.html",
    "title": "AI Newsletter – September 15, 2025",
    "section": "",
    "text": "📧 Newsletter Format\n\n\n\nThis is PythRaSh’s AI Newsletter for the week of September 15, 2025. You can view the full newsletter or subscribe to receive it directly."
  },
  {
    "objectID": "ch/newsletter/backup/2025_09_15_newsletter_old.html#newsletter-content",
    "href": "ch/newsletter/backup/2025_09_15_newsletter_old.html#newsletter-content",
    "title": "AI Newsletter – September 15, 2025",
    "section": "Newsletter Content",
    "text": "Newsletter Content"
  },
  {
    "objectID": "ch/newsletter/backup/2025_09_15_newsletter_old.html#quick-actions",
    "href": "ch/newsletter/backup/2025_09_15_newsletter_old.html#quick-actions",
    "title": "AI Newsletter – September 15, 2025",
    "section": "🔗 Quick Actions",
    "text": "🔗 Quick Actions\n\n\n\n📧 Subscribe\nGet weekly AI newsletters\n\n\n🔗 Share\n\nShare on Twitter\nShare on LinkedIn\n\n\n\n\n📱 Connect\n\nWebsite\nLinkedIn Profile\nGitHub\n\n\n\n📋 Archive\nView all newsletters\n\n\n\n\nPublished on September 15, 2025 | Subscribe for weekly updates"
  },
  {
    "objectID": "ch/newsletter/AI_Newsletter_2025_09_15.html",
    "href": "ch/newsletter/AI_Newsletter_2025_09_15.html",
    "title": "PythRaSh’s AI Newsletter",
    "section": "",
    "text": "📧 Newsletter Format\n\n\n\nThis is PythRaSh’s AI Newsletter for September 15, 2025. You can view the original newsletter or subscribe to receive it directly."
  },
  {
    "objectID": "ch/newsletter/AI_Newsletter_2025_09_15.html#newsletter-content",
    "href": "ch/newsletter/AI_Newsletter_2025_09_15.html#newsletter-content",
    "title": "PythRaSh’s AI Newsletter",
    "section": "📧 Newsletter Content",
    "text": "📧 Newsletter Content\nThis newsletter is available in its original email format with full styling, animations, and interactive elements preserved.\n\nView the Newsletter\n\n 📧 Open Full Newsletter \n\nWhat you’ll find inside: - 🚀 Event of the Week: Major AI breakthroughs in biology - ⚡ Quick Updates: Latest developments from research institutions - 📚 Research Papers: Top academic publications with impact analysis - 💻 GitHub Repositories: Popular tools for AI in biology - 🛠️ AI Products: Featured tools and platforms - 🐦 Trending Discussions: Key industry conversations"
  },
  {
    "objectID": "ch/newsletter/AI_Newsletter_2025_09_15.html#quick-actions",
    "href": "ch/newsletter/AI_Newsletter_2025_09_15.html#quick-actions",
    "title": "PythRaSh’s AI Newsletter",
    "section": "🔗 Quick Actions",
    "text": "🔗 Quick Actions\n\n\n\n📧 Subscribe\nGet weekly AI newsletters\n\n\n🔗 Share\n\nShare on Twitter\nShare on LinkedIn\n\n\n\n\n📱 Connect\n\nWebsite\nLinkedIn\nGitHub\n\n\n\n📋 Archive\nView all newsletters\n\n\n\n\nPublished on September 15, 2025 | Subscribe for weekly updates"
  },
  {
    "objectID": "ch/newsletter/newsletter_wrapper.html",
    "href": "ch/newsletter/newsletter_wrapper.html",
    "title": "AI Newsletter – September 15, 2025",
    "section": "",
    "text": "📧 Newsletter Format\n\n\n\nThis is PythRaSh’s AI Newsletter for the week of September 15, 2025. You can view the full newsletter or subscribe to receive it directly."
  },
  {
    "objectID": "ch/newsletter/newsletter_wrapper.html#newsletter-content",
    "href": "ch/newsletter/newsletter_wrapper.html#newsletter-content",
    "title": "AI Newsletter – September 15, 2025",
    "section": "Newsletter Content",
    "text": "Newsletter Content\n\n  \n  \n  \n  \n    Can't see the newsletter? Open in new window"
  },
  {
    "objectID": "ch/newsletter/newsletter_wrapper.html#quick-actions",
    "href": "ch/newsletter/newsletter_wrapper.html#quick-actions",
    "title": "AI Newsletter – September 15, 2025",
    "section": "🔗 Quick Actions",
    "text": "🔗 Quick Actions\n\n\n\n📧 Subscribe\nGet weekly AI newsletters\n\n\n🔗 Share\n\nShare on Twitter\nShare on LinkedIn\n\n\n\n\n📱 Connect\n\nWebsite\nLinkedIn Profile\nGitHub\n\n\n\n📋 Archive\nView all newsletters\n\n\n\n\nPublished on September 15, 2025 | Subscribe for weekly updates"
  },
  {
    "objectID": "ch/newsletter/TEMPLATE_newsletter.html",
    "href": "ch/newsletter/TEMPLATE_newsletter.html",
    "title": "AI Newsletter – [DATE HERE]",
    "section": "",
    "text": "📧 Newsletter Format\n\n\n\nThis is PythRaSh’s AI Newsletter for [DATE]. You can view the full newsletter or subscribe to receive it directly."
  },
  {
    "objectID": "ch/newsletter/TEMPLATE_newsletter.html#quick-actions",
    "href": "ch/newsletter/TEMPLATE_newsletter.html#quick-actions",
    "title": "AI Newsletter – [DATE HERE]",
    "section": "🔗 Quick Actions",
    "text": "🔗 Quick Actions\n\n\n\n📧 Subscribe\nGet weekly AI newsletters\n\n\n🔗 Share\n\nShare on Twitter\nShare on LinkedIn\n\n\n\n\n📱 Connect\n\nWebsite\nLinkedIn\nGitHub\n\n\n\n📋 Archive\nView all newsletters\n\n\n\n\nPublished on [DATE] | Subscribe for weekly updates"
  },
  {
    "objectID": "ch/newsletter/old_AI_Newsletter_2025_09_15.html",
    "href": "ch/newsletter/old_AI_Newsletter_2025_09_15.html",
    "title": "AI Newsletter – Sept 15, 2025",
    "section": "",
    "text": "📧 Newsletter Format\n\n\n\nThis is PythRaSh’s AI Newsletter for September 15, 2025. You can view the full newsletter or subscribe to receive it directly."
  },
  {
    "objectID": "ch/newsletter/old_AI_Newsletter_2025_09_15.html#quick-actions",
    "href": "ch/newsletter/old_AI_Newsletter_2025_09_15.html#quick-actions",
    "title": "AI Newsletter – Sept 15, 2025",
    "section": "🔗 Quick Actions",
    "text": "🔗 Quick Actions\n\n\n\n📧 Subscribe\nGet weekly AI newsletters\n\n\n🔗 Share\n\nShare on Twitter\nShare on LinkedIn\n\n\n\n\n📱 Connect\n\nWebsite\nLinkedIn\nGitHub\n\n\n\n📋 Archive\nView all newsletters\n\n\n\n\nPublished on September 15, 2025 | Subscribe for weekly updates"
  },
  {
    "objectID": "ch/newsletter/backup_generated.html",
    "href": "ch/newsletter/backup_generated.html",
    "title": "AI Newsletter - Sept 15, 2025",
    "section": "",
    "text": "📧 Newsletter Format\n\n\n\nThis is PythRaSh’s AI Newsletter for September 15, 2025. You can view the full newsletter or subscribe to receive it directly."
  },
  {
    "objectID": "ch/newsletter/backup_generated.html#quick-actions",
    "href": "ch/newsletter/backup_generated.html#quick-actions",
    "title": "AI Newsletter - Sept 15, 2025",
    "section": "🔗 Quick Actions",
    "text": "🔗 Quick Actions\n\n\n\n📧 Subscribe\nGet weekly AI newsletters\n\n\n🔗 Share\n\nShare on Twitter\nShare on LinkedIn\n\n\n\n\n📱 Connect\n\nWebsite\nLinkedIn\nGitHub\n\n\n\n📋 Archive\nView all newsletters\n\n\n\n\nPublished on September 15, 2025 | Subscribe for weekly updates"
  },
  {
    "objectID": "ch/newsletter/broken_AI_Newsletter_2025_09_15.html",
    "href": "ch/newsletter/broken_AI_Newsletter_2025_09_15.html",
    "title": "AI Newsletter - Sept 15, 2025",
    "section": "",
    "text": "📧 Newsletter Format\n\n\n\nThis is PythRaSh’s AI Newsletter for September 15, 2025. You can view the full newsletter or subscribe to receive it directly.\n\n\n\n\n  \n    View the newsletter: \n    📧 Open Full Newsletter\n  \n  \n  \n  \n    \n\n\n    \n    \n    \n    PythRaSh's AI Newsletter - Week of September 15, 2025\n    CitationBibTeX citation:@online{rasheduzzaman2025,\n  author = {Md Rasheduzzaman and Rasheduzzaman, Md},\n  title = {AI {Newsletter} - {Sept} 15, 2025},\n  date = {2025-09-15},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMd Rasheduzzaman, and Md Rasheduzzaman. 2025. “AI Newsletter -\nSept 15, 2025.” September 15, 2025."
  },
  {
    "objectID": "ch/newsletter/AI_Newsletter_2025_09_15.html#newsletter-archive",
    "href": "ch/newsletter/AI_Newsletter_2025_09_15.html#newsletter-archive",
    "title": "PythRaSh’s AI Newsletter",
    "section": "🔗 Newsletter Archive",
    "text": "🔗 Newsletter Archive\nThis is part of PythRaSh’s weekly AI Newsletter series. Each issue is carefully curated to bring you the most important developments at the intersection of artificial intelligence and life sciences.\n\nPrevious Issues\n\nView all newsletters\n\n\n\nSubscribe\n\nGet weekly updates"
  },
  {
    "objectID": "ch/newsletter/AI_Newsletter_2025_09_15.html#connect",
    "href": "ch/newsletter/AI_Newsletter_2025_09_15.html#connect",
    "title": "PythRaSh’s AI Newsletter",
    "section": "📱 Connect",
    "text": "📱 Connect\n\n\n\n🌐 Website & Social\n\nPythRaSh Website\nLinkedIn\nGitHub\n\n\n\n\n📧 Newsletter\n\nSubscribe\nArchive\nEmail\n\n\n\n\n\nPublished on September 15, 2025 • Original email format preserved • Subscribe for weekly updates"
  },
  {
    "objectID": "ch/linux-and-ctl/advanced.html#grep---advanced-pattern-matching",
    "href": "ch/linux-and-ctl/advanced.html#grep---advanced-pattern-matching",
    "title": "Advanced Linux and Command Line Tools",
    "section": "\ngrep - Advanced Pattern Matching",
    "text": "grep - Advanced Pattern Matching\ngrep is one of the most powerful text search tools. Let’s explore its advanced features.\nBasic grep Usage\n\n# Search for a pattern in a file\ngrep \"pattern\" file.txt\n\n# Search case-insensitive\ngrep -i \"pattern\" file.txt\n\n# Search in multiple files\ngrep \"pattern\" *.txt\n\n# Search recursively in directories\ngrep -r \"pattern\" directory/\n\nAdvanced grep Options\n\n# Show line numbers\ngrep -n \"pattern\" file.txt\n\n# Show context (2 lines before and after)\ngrep -C 2 \"pattern\" file.txt\n\n# Show only filenames with matches\ngrep -l \"pattern\" *.txt\n\n# Show only lines that DON'T match (invert)\ngrep -v \"pattern\" file.txt\n\n# Use regular expressions\ngrep -E \"^[0-9]+\" file.txt  # Lines starting with numbers\n\n# Count matches\ngrep -c \"pattern\" file.txt\n\n# Show only the matching part\ngrep -o \"pattern\" file.txt\n\nRegular Expressions with grep\n\n# Lines starting with specific text\ngrep \"^start\" file.txt\n\n# Lines ending with specific text\ngrep \"end$\" file.txt\n\n# Lines containing either pattern1 OR pattern2\ngrep -E \"pattern1|pattern2\" file.txt\n\n# Lines with exactly 3 digits\ngrep -E \"^[0-9]{3}$\" file.txt\n\n# Lines containing word boundaries\ngrep -w \"word\" file.txt"
  },
  {
    "objectID": "ch/linux-and-ctl/advanced.html#awk---pattern-scanning-and-processing",
    "href": "ch/linux-and-ctl/advanced.html#awk---pattern-scanning-and-processing",
    "title": "Advanced Linux and Command Line Tools",
    "section": "\nawk - Pattern Scanning and Processing",
    "text": "awk - Pattern Scanning and Processing\nawk is a powerful programming language for text processing. It processes files line by line.\nBasic awk Syntax\n\n# Print entire lines\nawk '{print}' file.txt\n\n# Print specific fields (space-separated by default)\nawk '{print $1}' file.txt  # First field\nawk '{print $2}' file.txt  # Second field\nawk '{print $1, $3}' file.txt  # First and third fields\n\n# Print last field\nawk '{print $NF}' file.txt\n\n# Print number of fields in each line\nawk '{print NF}' file.txt\n\nAdvanced awk Examples\n\n# Print lines with more than 3 fields\nawk 'NF &gt; 3' file.txt\n\n# Print lines where first field equals \"name\"\nawk '$1 == \"name\"' file.txt\n\n# Print lines where second field is greater than 100\nawk '$2 &gt; 100' file.txt\n\n# Add line numbers\nawk '{print NR, $0}' file.txt\n\n# Print specific lines (e.g., lines 5-10)\nawk 'NR &gt;= 5 && NR &lt;= 10' file.txt\n\n# Calculate sum of second column\nawk '{sum += $2} END {print sum}' file.txt\n\n# Print average of second column\nawk '{sum += $2; count++} END {print sum/count}' file.txt\n\nawk with Field Separators\n\n# Use comma as field separator\nawk -F',' '{print $1, $2}' file.csv\n\n# Use multiple separators\nawk -F'[,;]' '{print $1, $2}' file.txt\n\n# Use tab as separator\nawk -F'\\t' '{print $1, $2}' file.tsv"
  },
  {
    "objectID": "ch/linux-and-ctl/advanced.html#cut---extract-columns-from-files",
    "href": "ch/linux-and-ctl/advanced.html#cut---extract-columns-from-files",
    "title": "Advanced Linux and Command Line Tools",
    "section": "\ncut - Extract Columns from Files",
    "text": "cut - Extract Columns from Files\ncut is simpler than awk for basic column extraction.\nBasic cut Usage\n\n# Extract first column (space-separated)\ncut -d' ' -f1 file.txt\n\n# Extract first and third columns\ncut -d' ' -f1,3 file.txt\n\n# Extract columns 1-3\ncut -d' ' -f1-3 file.txt\n\n# Use comma as delimiter\ncut -d',' -f1,2 file.csv\n\n# Use tab as delimiter\ncut -d$'\\t' -f1,2 file.tsv\n\n# Extract by character positions\ncut -c1-10 file.txt  # Characters 1-10\ncut -c1,5,10 file.txt  # Characters 1, 5, and 10"
  },
  {
    "objectID": "ch/linux-and-ctl/advanced.html#sed---stream-editor",
    "href": "ch/linux-and-ctl/advanced.html#sed---stream-editor",
    "title": "Advanced Linux and Command Line Tools",
    "section": "\nsed - Stream Editor",
    "text": "sed - Stream Editor\nsed is used for text substitution and editing.\nBasic sed Usage\n\n# Replace first occurrence of \"old\" with \"new\"\nsed 's/old/new/' file.txt\n\n# Replace all occurrences of \"old\" with \"new\"\nsed 's/old/new/g' file.txt\n\n# Replace only on specific lines (e.g., line 5)\nsed '5s/old/new/' file.txt\n\n# Delete lines containing \"pattern\"\nsed '/pattern/d' file.txt\n\n# Delete empty lines\nsed '/^$/d' file.txt\n\n# Print only lines 5-10\nsed -n '5,10p' file.txt\n\nAdvanced sed Examples\n\n# Replace multiple patterns\nsed -e 's/old1/new1/g' -e 's/old2/new2/g' file.txt\n\n# Use different delimiter (useful for paths)\nsed 's|/old/path|/new/path|g' file.txt\n\n# Case-insensitive replacement\nsed 's/old/new/gi' file.txt\n\n# In-place editing (modify file directly)\nsed -i 's/old/new/g' file.txt\n\n# Backup original file\nsed -i.bak 's/old/new/g' file.txt"
  },
  {
    "objectID": "ch/linux-and-ctl/advanced.html#sort---sort-lines",
    "href": "ch/linux-and-ctl/advanced.html#sort---sort-lines",
    "title": "Advanced Linux and Command Line Tools",
    "section": "\nsort - Sort Lines",
    "text": "sort - Sort Lines\n\n# Sort alphabetically\nsort file.txt\n\n# Sort numerically\nsort -n file.txt\n\n# Sort in reverse order\nsort -r file.txt\n\n# Sort by specific field\nsort -k2 file.txt  # Sort by second field\n\n# Sort by multiple fields\nsort -k1,1 -k2,2n file.txt  # Sort by field 1, then by field 2 numerically\n\n# Remove duplicates while sorting\nsort -u file.txt\n\n# Sort ignoring case\nsort -f file.txt"
  },
  {
    "objectID": "ch/linux-and-ctl/advanced.html#uniq---remove-duplicate-lines",
    "href": "ch/linux-and-ctl/advanced.html#uniq---remove-duplicate-lines",
    "title": "Advanced Linux and Command Line Tools",
    "section": "\nuniq - Remove Duplicate Lines",
    "text": "uniq - Remove Duplicate Lines\n\n# Remove consecutive duplicate lines\nuniq file.txt\n\n# Count occurrences of each line\nuniq -c file.txt\n\n# Show only unique lines\nuniq -u file.txt\n\n# Show only duplicate lines\nuniq -d file.txt\n\n# Ignore case when comparing\nuniq -i file.txt"
  },
  {
    "objectID": "ch/linux-and-ctl/advanced.html#find---find-files-and-directories",
    "href": "ch/linux-and-ctl/advanced.html#find---find-files-and-directories",
    "title": "Advanced Linux and Command Line Tools",
    "section": "\nfind - Find Files and Directories",
    "text": "find - Find Files and Directories\n\n# Find files by name\nfind . -name \"*.txt\"\n\n# Find files by type\nfind . -type f  # Files only\nfind . -type d  # Directories only\n\n# Find files by size\nfind . -size +100M  # Files larger than 100MB\nfind . -size -1k    # Files smaller than 1KB\n\n# Find files by modification time\nfind . -mtime -7    # Modified in last 7 days\nfind . -mtime +30   # Modified more than 30 days ago\n\n# Find files by permissions\nfind . -perm 644    # Files with 644 permissions\nfind . -perm -u+x   # Files executable by owner\n\n# Execute commands on found files\nfind . -name \"*.txt\" -exec rm {} \\;  # Delete all .txt files\nfind . -name \"*.log\" -exec mv {} logs/ \\;  # Move .log files to logs directory"
  },
  {
    "objectID": "ch/linux-and-ctl/advanced.html#xargs---execute-commands-on-multiple-arguments",
    "href": "ch/linux-and-ctl/advanced.html#xargs---execute-commands-on-multiple-arguments",
    "title": "Advanced Linux and Command Line Tools",
    "section": "\nxargs - Execute Commands on Multiple Arguments",
    "text": "xargs - Execute Commands on Multiple Arguments\n\n# Find and delete files\nfind . -name \"*.tmp\" | xargs rm\n\n# Find and copy files\nfind . -name \"*.txt\" | xargs cp -t backup/\n\n# Count lines in multiple files\nfind . -name \"*.txt\" | xargs wc -l\n\n# Search in multiple files\nfind . -name \"*.py\" | xargs grep \"import\""
  },
  {
    "objectID": "ch/linux-and-ctl/advanced.html#creating-and-running-scripts",
    "href": "ch/linux-and-ctl/advanced.html#creating-and-running-scripts",
    "title": "Advanced Linux and Command Line Tools",
    "section": "Creating and Running Scripts",
    "text": "Creating and Running Scripts\n\n# Create a script file\nnano my_script.sh\n\n# Make it executable\nchmod +x my_script.sh\n\n# Run the script\n./my_script.sh"
  },
  {
    "objectID": "ch/linux-and-ctl/advanced.html#basic-script-structure",
    "href": "ch/linux-and-ctl/advanced.html#basic-script-structure",
    "title": "Advanced Linux and Command Line Tools",
    "section": "Basic Script Structure",
    "text": "Basic Script Structure\n\n#!/bin/bash\n# This is a comment\n\n# Set variables\nNAME=\"Linux User\"\nCOUNT=10\n\n# Use variables\necho \"Hello, $NAME!\"\necho \"Count is: $COUNT\"\n\n# Use command substitution\nCURRENT_DIR=$(pwd)\necho \"Current directory: $CURRENT_DIR\"\n\n# Use arithmetic\nRESULT=$((COUNT * 2))\necho \"Double count: $RESULT\""
  },
  {
    "objectID": "ch/linux-and-ctl/advanced.html#control-structures",
    "href": "ch/linux-and-ctl/advanced.html#control-structures",
    "title": "Advanced Linux and Command Line Tools",
    "section": "Control Structures",
    "text": "Control Structures\nIf-Else Statements\n\n#!/bin/bash\n\nif [ -f \"file.txt\" ]; then\n    echo \"File exists\"\nelse\n    echo \"File does not exist\"\nfi\n\n# String comparison\nif [ \"$1\" = \"hello\" ]; then\n    echo \"You said hello\"\nelif [ \"$1\" = \"goodbye\" ]; then\n    echo \"You said goodbye\"\nelse\n    echo \"You said something else\"\nfi\n\nLoops\n\n#!/bin/bash\n\n# For loop\nfor i in {1..5}; do\n    echo \"Number: $i\"\ndone\n\n# For loop with files\nfor file in *.txt; do\n    echo \"Processing: $file\"\ndone\n\n# While loop\ncount=1\nwhile [ $count -le 5 ]; do\n    echo \"Count: $count\"\n    count=$((count + 1))\ndone"
  },
  {
    "objectID": "ch/linux-and-ctl/advanced.html#file-manipulation-scripts",
    "href": "ch/linux-and-ctl/advanced.html#file-manipulation-scripts",
    "title": "Advanced Linux and Command Line Tools",
    "section": "File Manipulation Scripts",
    "text": "File Manipulation Scripts\nQ1: How to move files to subfolders\nI have many files inside a folder. I want to move them into two sub-folders named f1 and f2 (I made them using mkdir -p f1 f2). How to do that?\nThis is the way:\n\n#!/bin/bash\n# move_files.sh\n\n# Create subdirectories if they don't exist\nmkdir -p f1 f2\n\n# Counter for files\ni=0\n\n# Loop through all .fastq.gz files (sorted by version)\nfor file in $(ls *.fastq.gz | sort -V); do\n    if [ $i -le 400 ]; then\n        mv \"$file\" f1/\n        echo \"Moved $file to f1/\"\n    else\n        mv \"$file\" f2/\n        echo \"Moved $file to f2/\"\n    fi\n    i=$((i + 1))\ndone\n\necho \"File moving completed!\"\necho \"Files in f1: $(ls f1/ | wc -l)\"\necho \"Files in f2: $(ls f2/ | wc -l)\"\n\nHere, I am sending fastq.gz files having 0-400 in their name to f1 folder and remaining ones to f2. You just need to use your file naming pattern in the code block, and you are all set. For example, if you have many .fasta files, use .fasta instead of .fastq.gz. You got the idea, right?\nHow to execute/run this file now? Run these:\n\nchmod +x move_files.sh\n./move_files.sh\n\nchmod +x is making the file named move_files.sh executable. Then we are running it using ./move_files.sh."
  },
  {
    "objectID": "ch/linux-and-ctl/advanced.html#ssh-connection-to-hpc-clusters",
    "href": "ch/linux-and-ctl/advanced.html#ssh-connection-to-hpc-clusters",
    "title": "Advanced Linux and Command Line Tools",
    "section": "SSH Connection to HPC Clusters",
    "text": "SSH Connection to HPC Clusters\nI am going to connect to Uni-Greifswald’s Brain cluster.\n\nssh username@brain.uni-greifswald.de\n\nYou have to use your real username and password. Now, let’s get an interactive session to the gpu compute node (it is named “vision” for uni-greifswald’s gpu node, check for yours).\n\nsrun --pty --gres=gpu:1 --partition=vision --mem=16g -t 12:00:00 bash -i\n\nSo, I am taking the session for 12 hours."
  },
  {
    "objectID": "ch/linux-and-ctl/advanced.html#environment-management-on-hpc",
    "href": "ch/linux-and-ctl/advanced.html#environment-management-on-hpc",
    "title": "Advanced Linux and Command Line Tools",
    "section": "Environment Management on HPC",
    "text": "Environment Management on HPC\nInstalling Conda on HPC\nLet’s install conda for our environment management (if you don’t have already).\n\n# Download the Miniconda installer for Linux\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n\n# Run the installer, specifying the installation path\nbash Miniconda3-latest-Linux-x86_64.sh -b -p ~/miniconda_install/\n\nNow, let’s initialize conda:\n\n# Source the conda shell script to make the 'conda' command available\nsource ~/miniconda_install/bin/activate\n\n# Initialize conda for the current shell session.\nconda init bash\n\nSince we’re using an interactive session, we won’t need to manually source anything after this. The conda init command makes it so we can use conda and conda activate as we normally would.\nN.B. We could make some aliases for conda commands to write conda codes in shorter format, but we can do/see it later. Get used to the normal ones."
  },
  {
    "objectID": "ch/linux-and-ctl/advanced.html#advanced-environment-setup",
    "href": "ch/linux-and-ctl/advanced.html#advanced-environment-setup",
    "title": "Advanced Linux and Command Line Tools",
    "section": "Advanced Environment Setup",
    "text": "Advanced Environment Setup\nLet’s make an environment and install our required tools there. Well, our goal is to make a system where we will use some images/characters and make short videos using them to teach Italian. We need to process photos, images, text, and sync lips (video) with audio/speech. We need MuseTalk for this. Let’s configure our environment accordingly.\nStep 1 – Create conda environment\n\n# Create new environment for MuseTalk\nconda create -n musetalk python=3.10\nconda activate musetalk\n\nStep 2 – Install Dependencies\n\n# Install PyTorch (adapt cuda version to your cluster, here CUDA 11.8 example)\npip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --extra-index-url https://download.pytorch.org/whl/cu118\n\n# HuggingFace core libs\npip install transformers accelerate diffusers safetensors\n\n# MuseTalk repo (clone)\ngit clone https://github.com/TMElyralab/MuseTalk.git\ncd MuseTalk\n\n# Install requirements\npip install -r requirements.txt\n\n# Install whisper encoder\npip install --editable ./musetalk/whisper\n\n# Extra: ffmpeg for video processing\nconda install -c conda-forge ffmpeg -y\n\nStep 3 – Download MuseTalk Models\nMuseTalk Hugging Face repo: https://huggingface.co/TMElyralab/MuseTalk We need: - musetalk.pth (main model) - gfpgan (optional face enhancer)\nRun inside MuseTalk:\n\nmkdir checkpoints\ncd checkpoints\n\n# Download core model\nwget https://huggingface.co/TMElyralab/MuseTalk/resolve/main/musetalk.pth\n\n# Optional face enhancer\ngit clone https://github.com/TencentARC/GFPGAN.git\ncd .."
  },
  {
    "objectID": "ch/linux-and-ctl/advanced.html#cuda-environment-configuration",
    "href": "ch/linux-and-ctl/advanced.html#cuda-environment-configuration",
    "title": "Advanced Linux and Command Line Tools",
    "section": "CUDA Environment Configuration",
    "text": "CUDA Environment Configuration\nBetter way:\n\nexport CUDA_HOME=/usr/local/cuda-11.7\nexport PATH=$CUDA_HOME/bin:$PATH\nexport LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH\n\nIn my HPC Cluster, it is cuda 11.7. I am configuring for that. We could add these lines in my ~/.bashrc file as well. We might see it later.\nLet’s load our cuda module first to get things done smoothly.\n\nmodule load cuda/11.7\n\nNow, make the yml file to make the environment with all the tools required.\nname: musetalk3\nchannels:\n  - pytorch\n  - nvidia\n  - defaults\ndependencies:\n  - python=3.10\n  - pip\n  - ffmpeg\n  - pip:\n      # PyTorch + CUDA 11.8 compatible with 11.7 system\n      - torch==2.1.0+cu118\n      - torchvision==0.16.0+cu118\n      - torchaudio==2.1.0+cu118\n      - --extra-index-url https://download.pytorch.org/whl/cu118\n\n      # OpenMMLab dependencies\n      - mmcv==2.0.1 -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.1.0/index.html\n      - mmdet==3.1.0\n      - gradio\n      - opencv-python\n      - numpy\n      - scipy\n      - matplotlib\n      - tqdm\n      - pyyaml\n      - pillow\n      - soundfile\n      - librosa\n      - moviepy\n      - imageio\nSave this as musetalk3.yml and run:\n\nconda env create -f musetalk3.yml\nconda activate musetalk3\n\nNow, we have our environment ready to use. Let’s use it.\n\nconda activate musetalk3\n\nNow, install museetalk repo dependencies.\n\ngit clone https://github.com/TMElyralab/MuseTalk.git\ncd MuseTalk\npip install -r requirements.txt\n\nLet’s get the faces I want to use. I uploaded them to my Google Drive.\n\npip install gdown\n\n# Get shareable link from Google Drive and copy file id\ngdown https://drive.google.com/uc?id=FILE_ID -O data/faces/alice.jpg\ngdown https://drive.google.com/uc?id=FILE_ID -O data/faces/bob.jpg\n\nModify the google drive links for the images and their destination name as you like it.\nReal work\nNow, let’s follow the author’s guideline.\n\npip install --no-cache-dir -U openmim\nmim install mmengine\nmim install \"mmcv==2.0.1\"\nmim install \"mmdet==3.1.0\"\nmim install \"mmpose==1.1.0\"\n\nLet’s download the model’s weight.\n\nsh ./download_weights.sh\n\n\n# Check ffmpeg installation\nffmpeg -version\n\nThe conversation\n\npip install TTS\n\n# Example: Alice speaking Italian\ntts --text \"Buon giorno, Bob! Come stai?\" \\\n    --model_name tts_models/it/mai_female/vits \\\n    --out_path alice_buongiorno.wav\n\n# Example: Bob replying\ntts --text \"Buon giorno, Alice! Va bene, grazie mille! Chi è questo?\" \\\n    --model_name tts_models/it/mai_male/vits \\\n    --out_path bob_risposta.wav"
  },
  {
    "objectID": "ch/linux-and-ctl/basics.html#navigation-commands",
    "href": "ch/linux-and-ctl/basics.html#navigation-commands",
    "title": "Linux Basics",
    "section": "Navigation Commands",
    "text": "Navigation Commands\n\npwd - Print Working Directory\nShows your current location in the file system.\n\n# Show current directory\npwd\n\n\nls - List Directory Contents\nLists files and folders in the current directory.\n\n# Basic listing\nls\n\n# Detailed listing with permissions, size, date\nls -l\n\n# List all files including hidden ones (starting with .)\nls -la\n\n# List with human-readable file sizes\nls -lh\n\n\n# List files in a specific directory\nls /home/username\n\n\ncd - Change Directory\nNavigate between directories.\n\n# Go to home directory\ncd ~\n# or simply\ncd\n\n# Go to parent directory\ncd ..\n\n# Go to root directory\ncd /\n\n# Go back to previous directory\ncd -\n#or \ncd .\n\n\n# Go to a specific directory\ncd /home/username/Documents"
  },
  {
    "objectID": "ch/linux-and-ctl/basics.html#file-and-directory-operations",
    "href": "ch/linux-and-ctl/basics.html#file-and-directory-operations",
    "title": "Linux Basics",
    "section": "File and Directory Operations",
    "text": "File and Directory Operations\n\nmkdir - Make Directory\nCreate new directories.\n\n# Create a single directory\nmkdir my_folder\n\n# Create multiple directories\nmkdir folder1 folder2 folder3\n\n# Create nested directories\nmkdir -p project/data/raw\n\n# Create directory with specific permissions\nmkdir -m 755 public_folder\n\n\ntouch - Create Empty Files\nCreate empty files or update file timestamps.\n\n# Create an empty file\ntouch my_file.txt\n\n# Create multiple files\ntouch file1.txt file2.txt file3.txt\n\n# Update file timestamp (useful for makefiles)\ntouch existing_file.txt\n\n\ncp - Copy Files and Directories\nCopy files and directories.\n\n# Copy a file\ncp source.txt destination.txt\n\n\n# Copy file to a directory\ncp file.txt /path/to/directory/\n\n# Copy directory recursively\ncp -r source_directory/ destination_directory/\n\n# Copy with preservation of attributes\ncp -p file.txt backup/\n\n# Copy multiple files\ncp file1.txt file2.txt destination_folder/\n\n\nmv - Move/Rename Files and Directories\nMove or rename files and directories.\n\n# Rename a file\nmv old_name.txt new_name.txt\n\n\n# Move file to directory\nmv file.txt /path/to/directory/\n\n# Move directory\nmv source_directory/ destination_directory/\n\n# Move and rename in one step\nmv file.txt /new/location/renamed_file.txt\n\n\nrm - Remove Files and Directories\nDelete files and directories.\n\n# Remove a file\nrm file.txt\n\n# Remove multiple files\nrm file1.txt file2.txt file3.txt\n\n# Remove directory and all contents (BE CAREFUL!)\nrm -r directory_name/\n\n# Remove directory with confirmation\nrm -ri directory_name/\n\n# Remove empty directory only\nrmdir empty_directory/\n\n# Remove files with confirmation\nrm -i *.txt"
  },
  {
    "objectID": "ch/linux-and-ctl/basics.html#file-permissions",
    "href": "ch/linux-and-ctl/basics.html#file-permissions",
    "title": "Linux Basics",
    "section": "File Permissions",
    "text": "File Permissions\nUnderstanding file permissions is crucial for Linux security.\nViewing Permissions\n\n# List files with permissions\nls -l\n\n# Example output:\n# -rw-r--r-- 1 username group 1234 Dec 15 10:30 file.txt\n# drwxr-xr-x 2 username group 4096 Dec 15 10:30 folder/\n\nThe permission string has 10 characters: - Position 1: File type (- for file, d for directory) - Positions 2-4: Owner permissions (read, write, execute) - Positions 5-7: Group permissions (read, write, execute) - Positions 8-10: Other permissions (read, write, execute)\n\nchmod - Change File Permissions\n\n# Give owner read, write, execute; group and others read, execute\nchmod 755 script.sh\n\n# Give owner read, write; group and others read only\nchmod 644 file.txt\n\n# Add execute permission for owner\nchmod +x script.sh\n\n# Remove write permission for group and others\nchmod go-w file.txt\n\n# Make file readable by everyone\nchmod a+r file.txt\n\n\nchown - Change File Ownership\n\n# Change owner\nchown newowner file.txt\n\n# Change owner and group\nchown newowner:newgroup file.txt\n\n# Change ownership recursively\nchown -R newowner:newgroup directory/"
  },
  {
    "objectID": "ch/linux-and-ctl/basics.html#text-editing",
    "href": "ch/linux-and-ctl/basics.html#text-editing",
    "title": "Linux Basics",
    "section": "Text Editing",
    "text": "Text Editing\n\nnano - Simple Text Editor\nGreat for beginners.\n\n# Open file in nano\nnano filename.txt\n\n# Create new file\nnano new_file.txt\n\nNano shortcuts: - Ctrl + X: Exit - Ctrl + O: Save - Ctrl + W: Search - Ctrl + K: Cut line - Ctrl + U: Paste\n\nvim - Advanced Text Editor\nMore powerful but steeper learning curve.\n\n# Open file in vim\nvim filename.txt\n\n# Create new file\nvim new_file.txt\n\nSo, creating and opening command is basically the same. Just write the file name. After opening the file, you have toenter into editing/inserting mode. Press i on your keyboard to be able to insert/write (or copy-paste) contents. After finishing editing, press Esc key to come out of the editing mode. Then write :wq to save.\nBasic vim commands: - i: Enter insert mode - Esc: Exit insert mode - :w: Save - :q: Quit - :wq: Save and quit - :q!: Quit without saving"
  },
  {
    "objectID": "ch/linux-and-ctl/basics.html#viewing-file-contents",
    "href": "ch/linux-and-ctl/basics.html#viewing-file-contents",
    "title": "Linux Basics",
    "section": "Viewing File Contents",
    "text": "Viewing File Contents\n\ncat - Display File Contents\n\n# Display entire file\ncat file.txt\n\n# Display multiple files\ncat file1.txt file2.txt\n\n# Display with line numbers\ncat -n file.txt\n\n\nless - View File Page by Page\n\n# View file with pagination\nless file.txt\n\n# Search in less: type `/search_term` and press Enter\n# Navigate: Space (next page), b (previous page), q (quit)\n\n\nhead and tail - View Beginning/End of Files\n\n# Show first 10 lines\nhead file.txt\n\n# Show first 20 lines\nhead -n 20 file.txt\n\n# Show last 10 lines\ntail file.txt\n\n# Show last 20 lines\ntail -n 20 file.txt\n\n# Follow file changes in real-time\ntail -f log_file.txt"
  },
  {
    "objectID": "ch/linux-and-ctl/basics.html#basic-text-processing",
    "href": "ch/linux-and-ctl/basics.html#basic-text-processing",
    "title": "Linux Basics",
    "section": "Basic Text Processing",
    "text": "Basic Text Processing\n\ngrep - Search Text\n\n# Search for text in file\ngrep \"search_term\" file.txt\n\n# Search case-insensitive\ngrep -i \"search_term\" file.txt\n\n# Search in multiple files\ngrep \"search_term\" *.txt\n\n# Search recursively in directories\ngrep -r \"search_term\" directory/\n\n# Show line numbers\ngrep -n \"search_term\" file.txt\n\n\nwc - Word Count\n\n# Count lines, words, characters\nwc file.txt\n\n# Count only lines\nwc -l file.txt\n\n# Count only words\nwc -w file.txt\n\n# Count only characters\nwc -c file.txt"
  },
  {
    "objectID": "ch/linux-and-ctl/basics.html#environment-and-system-information",
    "href": "ch/linux-and-ctl/basics.html#environment-and-system-information",
    "title": "Linux Basics",
    "section": "Environment and System Information",
    "text": "Environment and System Information\n\nwhoami - Current User\n\n# Show current username\nwhoami\n\n\nuname - System Information\n\n# Show system information\nuname -a\n\n# Show operating system\nuname -s\n\n# Show machine architecture\nuname -m\n\n\ndf - Disk Space Usage\n\n# Show disk space usage\ndf -h\n\n\nfree - Memory Usage\n\n# Show memory usage\nfree -h"
  },
  {
    "objectID": "ch/linux-and-ctl/basics.html#process-management",
    "href": "ch/linux-and-ctl/basics.html#process-management",
    "title": "Linux Basics",
    "section": "Process Management",
    "text": "Process Management\n\nps - List Processes\n\n# List running processes\nps\n\n# List all processes\nps aux\n\n# List processes for current user\nps u\n\n\ntop - Monitor Processes\n\n# Show running processes interactively\ntop\n\n# Press 'q' to quit"
  },
  {
    "objectID": "ch/linux-and-ctl/basics.html#getting-help",
    "href": "ch/linux-and-ctl/basics.html#getting-help",
    "title": "Linux Basics",
    "section": "Getting Help",
    "text": "Getting Help\n\nman - Manual Pages\n\n# Get help for a command\nman ls\n\n# Get help for a command (alternative)\nls --help\n\n\nwhich - Find Command Location\n\n# Find where a command is located\nwhich python3\n\n# Find all locations\nwhich -a python3"
  },
  {
    "objectID": "ch/linux-and-ctl/basics.html#example-workflow",
    "href": "ch/linux-and-ctl/basics.html#example-workflow",
    "title": "Linux Basics",
    "section": "Example Workflow",
    "text": "Example Workflow\nLet’s practice with a complete example:\n\n# Create a project directory\nmkdir -p my_project/data\ncd my_project\n\n# Create some files\ntouch data/sample1.txt data/sample2.txt\necho \"Hello World\" &gt; data/sample1.txt\necho \"Linux is awesome\" &gt; data/sample2.txt\n\n# List files\nls -la data/\n\n# View file contents\ncat data/sample1.txt\n\n# Search for text\ngrep \"Hello\" data/*.txt\n\n# Copy files\ncp data/sample1.txt backup_sample1.txt\n\n# Check permissions\nls -l data/\n\n# Change permissions\nchmod 644 data/sample1.txt\n\n# View file with line numbers\ncat -n data/sample1.txt"
  },
  {
    "objectID": "ch/linux-and-ctl/basics.html#example-shell-command",
    "href": "ch/linux-and-ctl/basics.html#example-shell-command",
    "title": "Linux Basics",
    "section": "Example shell command",
    "text": "Example shell command\n\n# Your Linux/Bash code here \necho \"Hello from Bash!\" \nls -l"
  },
  {
    "objectID": "ch/linux-and-ctl/basics.html#prerequisites",
    "href": "ch/linux-and-ctl/basics.html#prerequisites",
    "title": "Linux Basics",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nWindows 10 version 2004 and higher (Build 19041 and higher) or Windows 11\nAdministrator access to your computer\nStable internet connection"
  },
  {
    "objectID": "ch/linux-and-ctl/basics.html#step-1-check-your-windows-version",
    "href": "ch/linux-and-ctl/basics.html#step-1-check-your-windows-version",
    "title": "Linux Basics",
    "section": "Step 1: Check Your Windows Version",
    "text": "Step 1: Check Your Windows Version\nFirst, verify you have a compatible Windows version:\n\n# Open Command Prompt and run:\nwinver\n\nYou should see Windows 10 Build 19041+ or Windows 11."
  },
  {
    "objectID": "ch/linux-and-ctl/basics.html#step-2-enable-wsl-feature",
    "href": "ch/linux-and-ctl/basics.html#step-2-enable-wsl-feature",
    "title": "Linux Basics",
    "section": "Step 2: Enable WSL Feature",
    "text": "Step 2: Enable WSL Feature\nOpen Command Prompt as Administrator (Right-click → “Run as administrator”):\n\n# Enable WSL feature\ndism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart\n\n# Enable Virtual Machine Platform\ndism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart\n\nImportant: Restart your computer after running these commands."
  },
  {
    "objectID": "ch/linux-and-ctl/basics.html#step-3-install-wsl2-modern-method",
    "href": "ch/linux-and-ctl/basics.html#step-3-install-wsl2-modern-method",
    "title": "Linux Basics",
    "section": "Step 3: Install WSL2 (Modern Method)",
    "text": "Step 3: Install WSL2 (Modern Method)\nAfter restarting, open Command Prompt as Administrator again:\n\n# Install WSL2 with Ubuntu (this is the easiest method)\nwsl --install\n\nDo “OK” or “Yes” to agree if pop-ups appear. If this command doesn’t work, use the manual method below."
  },
  {
    "objectID": "ch/linux-and-ctl/basics.html#step-4-manual-installation-if-step-3-failed",
    "href": "ch/linux-and-ctl/basics.html#step-4-manual-installation-if-step-3-failed",
    "title": "Linux Basics",
    "section": "Step 4: Manual Installation (If Step 3 Failed)",
    "text": "Step 4: Manual Installation (If Step 3 Failed)\nDownload and Install WSL2 Kernel Update\n\nDownload the WSL2 Linux kernel update package from Microsoft:\n\nWSL2 Linux kernel update package for x64 machines\n\n\nRun the downloaded .msi file and follow the installation wizard.\nSet WSL2 as Default Version\n\n# Set WSL2 as the default version\nwsl --set-default-version 2\n\nInstall Ubuntu Distribution\n\n# List available distributions\nwsl --list --online\n#it will show all the availabe Linux distributions. Choose one of Ubuntus.\n\n# Install Ubuntu\nwsl --install -d Ubuntu"
  },
  {
    "objectID": "ch/linux-and-ctl/basics.html#step-5-alternative---install-via-microsoft-store",
    "href": "ch/linux-and-ctl/basics.html#step-5-alternative---install-via-microsoft-store",
    "title": "Linux Basics",
    "section": "Step 5: Alternative - Install via Microsoft Store",
    "text": "Step 5: Alternative - Install via Microsoft Store\nIf command line installation fails:\n\nOpen Microsoft Store\n\nSearch for “Ubuntu 22.04.3 LTS” or just “Ubuntu”\nClick “Get” or “Install”\nWait for download and installation\nClick “Launch” when ready"
  },
  {
    "objectID": "ch/linux-and-ctl/basics.html#step-6-first-time-ubuntu-setup",
    "href": "ch/linux-and-ctl/basics.html#step-6-first-time-ubuntu-setup",
    "title": "Linux Basics",
    "section": "Step 6: First-Time Ubuntu Setup",
    "text": "Step 6: First-Time Ubuntu Setup\nWhen you launch Ubuntu for the first time:\n\n\nWait for installation to complete (this may take several minutes)\n\nCreate a username when prompted (can be different from your Windows username)\n\nCreate a password (you won’t see characters as you type - this is normal)\nConfirm your password\n\n\n# Example setup (replace 'yourusername' with your preferred username)\nEnter new UNIX username: yourusername\nNew password: [type your password]\nRetype new password: [type your password again]"
  },
  {
    "objectID": "ch/linux-and-ctl/basics.html#step-7-verify-installation",
    "href": "ch/linux-and-ctl/basics.html#step-7-verify-installation",
    "title": "Linux Basics",
    "section": "Step 7: Verify Installation",
    "text": "Step 7: Verify Installation\n\n# Check WSL version and status\nwsl --list --verbose\n\nYou should see output like:\n  NAME      STATE           VERSION\n* Ubuntu    Running         2"
  },
  {
    "objectID": "ch/linux-and-ctl/basics.html#step-8-update-ubuntu-system",
    "href": "ch/linux-and-ctl/basics.html#step-8-update-ubuntu-system",
    "title": "Linux Basics",
    "section": "Step 8: Update Ubuntu System",
    "text": "Step 8: Update Ubuntu System\nIn your Ubuntu terminal, run these commands one by one:\n\n# Update package list\nsudo apt update\n\n# Upgrade installed packages (this may take a while)\nsudo apt upgrade\n\nNote: When prompted, type Y and press Enter to confirm upgrades."
  },
  {
    "objectID": "ch/linux-and-ctl/basics.html#step-9-install-python-development-environment",
    "href": "ch/linux-and-ctl/basics.html#step-9-install-python-development-environment",
    "title": "Linux Basics",
    "section": "Step 9: Install Python Development Environment",
    "text": "Step 9: Install Python Development Environment\nThis is required for latter use, not exactly now. You might choose not to do it right now. We will see things together.\nOption A: Using Python Virtual Environment (Recommended)\n\n# Install Python and essential tools\nsudo apt install python3 python3-pip python3-venv curl wget git\n\n# Verify Python installation\npython3 --version\npip3 --version\n\nOption B: Using Miniconda (Alternative)\n\n# Download Miniconda installer\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n\n# Install Miniconda\nbash Miniconda3-latest-Linux-x86_64.sh\n\n# Follow the installation prompts:\n# - Press ENTER to review the license\n# - Type 'yes' to accept the license\n# - Press ENTER to confirm the installation location\n# - Type 'yes' to initialize conda\n\n# Reload your shell configuration\nsource ~/.bashrc\n\n# Verify conda installation\nconda --version"
  },
  {
    "objectID": "ch/linux-and-ctl/basics.html#step-10-access-your-windows-files",
    "href": "ch/linux-and-ctl/basics.html#step-10-access-your-windows-files",
    "title": "Linux Basics",
    "section": "Step 10: Access Your Windows Files",
    "text": "Step 10: Access Your Windows Files\nYour Windows files are accessible from Ubuntu at these locations:\n\n# Your Windows C: drive\ncd /mnt/c/\n\ncd /mnt/c/Users/YourWindowsUsername/\n\n# Your Downloads folder\ncd /mnt/c/Users/YourWindowsUsername/Downloads/\n\nmnt is to denote mounting. Your linux/ubuntu functionalities are actually mounted on Windows. Makes sense?"
  },
  {
    "objectID": "ch/linux-and-ctl/basics.html#step-11-copy-project-files",
    "href": "ch/linux-and-ctl/basics.html#step-11-copy-project-files",
    "title": "Linux Basics",
    "section": "Step 11: Copy Project Files",
    "text": "Step 11: Copy Project Files\nTo copy your project from Windows to Ubuntu:\n\n# Navigate to your home directory in Ubuntu\ncd ~\n\n# Copy project from Windows Downloads (adjust path as needed)\ncp -r /mnt/c/Users/YourWindowsUsername/Downloads/folderName ./\n\n# Navigate to the project\ncd folderName\n\n# List files to verify\nls -la"
  },
  {
    "objectID": "ch/linux-and-ctl/basics.html#common-troubleshooting",
    "href": "ch/linux-and-ctl/basics.html#common-troubleshooting",
    "title": "Linux Basics",
    "section": "Common Troubleshooting",
    "text": "Common Troubleshooting\nIssue: “WSL has no installed distributions”\nSolution: You need to install a Linux distribution first.\n\n# List available distributions\nwsl --list --online\n\n# Install Ubuntu\nwsl --install Ubuntu\n\nIssue: Command not found errors\nSolution: Make sure you’re running commands in the correct environment: - WSL commands (wsl --install) should be run in Windows Command Prompt as Administrator - Linux commands (sudo apt update) should be run in Ubuntu terminal\nIssue: Network/download problems\nSolutions:\n\nTry different download mirrors:\n\n\n# For Miniconda, try this mirror:\nwget https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/Miniconda3-latest-Linux-x86_64.sh\n\n\n\nDownload via Windows browser and copy:\n\nDownload files using your Windows browser\nCopy to Ubuntu using /mnt/c/ path\n\n\nIssue: Permission denied\nSolution: Use sudo for system commands:\n\n# Wrong:\napt update\n\n# Correct:\nsudo apt update\n\nIssue: Ubuntu terminal closes immediately\nSolutions: 1. Make sure WSL2 is properly installed 2. Restart your computer 3. Try reinstalling Ubuntu from Microsoft Store"
  },
  {
    "objectID": "ch/linux-and-ctl/basics.html#useful-wsl-commands",
    "href": "ch/linux-and-ctl/basics.html#useful-wsl-commands",
    "title": "Linux Basics",
    "section": "Useful WSL Commands",
    "text": "Useful WSL Commands\n\n# List all installed distributions\nwsl --list --verbose\n\n# Stop a distribution\nwsl --terminate Ubuntu\n\n# Start a specific distribution\nwsl -d Ubuntu\n\n# Uninstall a distribution\nwsl --unregister Ubuntu\n\n# Check WSL version\nwsl --version"
  },
  {
    "objectID": "ch/linux-and-ctl/basics.html#opening-multiple-terminals",
    "href": "ch/linux-and-ctl/basics.html#opening-multiple-terminals",
    "title": "Linux Basics",
    "section": "Opening Multiple Terminals",
    "text": "Opening Multiple Terminals\nMethod 1: Windows Terminal (Recommended)\n\nInstall Windows Terminal from Microsoft Store\nOpen Windows Terminal\nClick the dropdown arrow next to the + tab\nSelect “Ubuntu”\nUse Ctrl + Shift + T for new tabs\nMethod 2: Direct Launch\n\nSearch “Ubuntu” in Start Menu\nClick to open new instances"
  },
  {
    "objectID": "ch/linux-and-ctl/basics.html#next-steps",
    "href": "ch/linux-and-ctl/basics.html#next-steps",
    "title": "Linux Basics",
    "section": "Next Steps",
    "text": "Next Steps\nOnce WSL2 and Ubuntu are properly installed:\n\n\nSet up your development environment (Python virtual environment or Conda)\n\nCopy your project files from Windows to Ubuntu\nInstall project dependencies\nRun your IELTS AI Coach application"
  },
  {
    "objectID": "ch/linux-and-ctl/basics.html#additional-resources",
    "href": "ch/linux-and-ctl/basics.html#additional-resources",
    "title": "Linux Basics",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nOfficial WSL Documentation\nWSL2 Installation Guide\nUbuntu WSL Documentation\n\nNeed Help? If you encounter issues not covered in this guide, please: 1. Take a screenshot of the error message 2. Note which step you were on 3. Contact me"
  },
  {
    "objectID": "ch/python/uv_tutorial.html",
    "href": "ch/python/uv_tutorial.html",
    "title": "UV: The Ultra-Fast Python Package Manager",
    "section": "",
    "text": "Welcome to the world of UV - the ultra-fast Python package manager that’s revolutionizing how we work with Python projects! If you’ve ever been frustrated by slow pip installations, complex virtual environment setups, or dependency conflicts, UV is here to solve all those problems.\n\n\nUV is a modern, extremely fast Python package and project manager written in Rust. It’s designed to replace multiple tools you might currently use:\n\npip (package installation)\npip-tools (dependency management)\npipx (tool installation)\npoetry (project management)\npyenv (Python version management)\nvirtualenv (virtual environments)\n\n\n\n\n\n\n\n\n\n\nKey Benefits\n\n\n\n\n⚡ Lightning Fast: 10-100x faster than pip\n🔧 All-in-One: Replaces multiple tools\n🔒 Reliable: Advanced dependency resolution\n🐍 Python Management: Built-in Python version handling\n📦 Modern: Lock files for reproducible builds\n🌐 Cross-Platform: Works on Windows, macOS, and Linux"
  },
  {
    "objectID": "ch/python/uv_tutorial.html#introduction",
    "href": "ch/python/uv_tutorial.html#introduction",
    "title": "UV: The Ultra-Fast Python Package Manager",
    "section": "",
    "text": "Welcome to the world of UV - the ultra-fast Python package manager that’s revolutionizing how we work with Python projects! If you’ve ever been frustrated by slow pip installations, complex virtual environment setups, or dependency conflicts, UV is here to solve all those problems.\n\n\nUV is a modern, extremely fast Python package and project manager written in Rust. It’s designed to replace multiple tools you might currently use:\n\npip (package installation)\npip-tools (dependency management)\npipx (tool installation)\npoetry (project management)\npyenv (Python version management)\nvirtualenv (virtual environments)\n\n\n\n\n\n\n\n\n\n\nKey Benefits\n\n\n\n\n⚡ Lightning Fast: 10-100x faster than pip\n🔧 All-in-One: Replaces multiple tools\n🔒 Reliable: Advanced dependency resolution\n🐍 Python Management: Built-in Python version handling\n📦 Modern: Lock files for reproducible builds\n🌐 Cross-Platform: Works on Windows, macOS, and Linux"
  },
  {
    "objectID": "ch/python/uv_tutorial.html#installation",
    "href": "ch/python/uv_tutorial.html#installation",
    "title": "UV: The Ultra-Fast Python Package Manager",
    "section": "Installation",
    "text": "Installation\n\nInstalling UV\nThe installation process is straightforward and works across all major platforms.\nFor macOS and Linux:\ncurl -LsSf https://astral.sh/uv/install.sh | sh\nFor Windows (PowerShell):\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n\n\nPost-Installation Setup\nAfter installation, you need to refresh your terminal environment:\nOption 1: Restart your terminal (Recommended)\nOption 2: Reload your shell configuration:\nsource $HOME/.local/bin/env\n\n\nVerify Installation\nCheck if UV is installed correctly:\nuv --version\nYou should see output similar to:\nuv 0.4.18 (Rust 1.81.0)"
  },
  {
    "objectID": "ch/python/uv_tutorial.html#python-version-management",
    "href": "ch/python/uv_tutorial.html#python-version-management",
    "title": "UV: The Ultra-Fast Python Package Manager",
    "section": "Python Version Management",
    "text": "Python Version Management\nOne of UV’s most powerful features is its ability to manage Python versions without needing additional tools like pyenv.\n\nList Available Python Versions\nuv python list\nThis command shows: - Python versions already installed on your system - Python versions available for download\nExample output:\n * python3.13 -&gt; /usr/bin/python3.13 (system)\n * python3.12 -&gt; /usr/bin/python3.12 (system)\n   python3.11 -&gt; download\n   python3.10 -&gt; download\n\n\nInstall Specific Python Versions\n# Install Python 3.13\nuv python install 3.13\n\n# Install Python 3.12\nuv python install 3.12\n\n# Install Python 3.11\nuv python install 3.11\nUV handles the download and installation automatically!\n\n\nCheck Python Version Availability\nuv python find 3.13\nThis returns the path to Python 3.13 if available, or indicates it can be downloaded."
  },
  {
    "objectID": "ch/python/uv_tutorial.html#basic-script-execution",
    "href": "ch/python/uv_tutorial.html#basic-script-execution",
    "title": "UV: The Ultra-Fast Python Package Manager",
    "section": "Basic Script Execution",
    "text": "Basic Script Execution\nLet’s start with a simple example to see UV in action.\n\nYour First UV Script\nCreate a file called basic_example.py:\n\n#!/usr/bin/env python3\n\"\"\"\nBasic UV Demo Script\nRun with: uv run basic_example.py\n\"\"\"\ndef main():\n    print(\"Welcome to UV Tutorial!\")\n    print(\"Hello from UV - Ultra Fast Python Package Manager!\")\n    \n    # Show Python version and location\n    import sys\n    print(f\"\\nPython Version: {sys.version}\")\n    print(f\"Python Executable: {sys.executable}\")\n    print(f\"Platform: {sys.platform}\")\n    \n    # Show some basic info\n    import os\n    print(f\"\\nCurrent working directory: {os.getcwd()}\")\n    \n    print(\"\\n✅ UV is working perfectly!\")\n    print(\"Now let's explore more advanced features!\")\n\nif __name__ == \"__main__\":\n    main()\n\n\n\nRun the Script\n# Run with default Python\nuv run basic_example.py\n\n# Run with specific Python version\nuv run --python 3.13 basic_example.py\n\n\nTemporary Dependencies\nHere’s where UV shines! You can run scripts with dependencies without installing them permanently:\n# Run a script that needs the 'rich' library\nuv run --with rich script_name.py\n\n# Multiple temporary dependencies\nuv run --with rich --with requests --python 3.13 script_name.py\nLet’s create a more exciting example:\n\n#!/usr/bin/env python3\n\"\"\"\nRich Library Demo\nRun with: uv run --with rich rich_demo.py\n\"\"\"\n\nfrom rich.console import Console\nfrom rich.table import Table\nfrom rich.panel import Panel\nfrom rich.text import Text\nfrom rich.progress import track\nimport time\n\ndef main():\n    console = Console()\n    \n    # Welcome message\n    welcome_text = Text(\"🚀 UV Package Manager Demo\", style=\"bold blue\")\n    panel = Panel(welcome_text, title=\"Python Tutorial\", border_style=\"green\")\n    console.print(panel)\n    \n    # Create a comparison table\n    console.print(\"\\n[bold yellow]Package Manager Comparison:[/bold yellow]\")\n    \n    table = Table(title=\"pip vs UV Comparison\")\n    table.add_column(\"Feature\", style=\"cyan\", no_wrap=True)\n    table.add_column(\"pip\", style=\"red\")\n    table.add_column(\"UV\", style=\"green\")\n    \n    comparisons = [\n        (\"Installation Speed\", \"🐌 Slow (10-60s)\", \"⚡ Lightning Fast (&lt;2s)\"),\n        (\"Dependency Resolution\", \"❌ Basic\", \"✅ Advanced SAT solver\"),\n        (\"Virtual Environment\", \"❌ Manual setup\", \"✅ Automatic\"),\n        (\"Lock Files\", \"❌ No lock files\", \"✅ uv.lock for reproducibility\"),\n        (\"Python Version Mgmt\", \"❌ Needs pyenv\", \"✅ Built-in\"),\n        (\"Parallel Downloads\", \"❌ Sequential\", \"✅ Parallel processing\"),\n        (\"Caching\", \"❌ Limited\", \"✅ Smart global cache\")\n    ]\n    \n    # Add rows with animation\n    for feature, pip_val, uv_val in track(comparisons, description=\"Loading comparison...\"):\n        table.add_row(feature, pip_val, uv_val)\n        time.sleep(0.3)\n    \n    console.print(table)\n    \n    # Summary\n    console.print(\"\\n[bold green]🎉 UV makes Python development 10x easier and faster![/bold green]\")\n    console.print(\"[italic]This rich formatting was installed temporarily with --with flag![/italic]\")\n\nif __name__ == \"__main__\":\n    main()\n\nRun this with:\nuv run --with rich rich_demo.py"
  },
  {
    "objectID": "ch/python/uv_tutorial.html#project-management",
    "href": "ch/python/uv_tutorial.html#project-management",
    "title": "UV: The Ultra-Fast Python Package Manager",
    "section": "Project Management",
    "text": "Project Management\n\nCreating a New Project\nUV makes it incredibly easy to start new Python projects:\n# Create a new directory and initialize a UV project\nmkdir my_awesome_project && cd my_awesome_project\nuv init\nThis creates: - pyproject.toml - Project configuration file - src/ - Source code directory structure - main.py - A starter Python file - Virtual environment management (automatic)\n\n\nProject Structure\nAfter running uv init, you’ll see:\nmy_awesome_project/\n├── pyproject.toml\n├── README.md\n├── src/\n│   └── my_awesome_project/\n│       ├── __init__.py\n│       └── py.typed\n└── main.py\n\n\nScript-Based Projects\nFor simple scripts, you can create a script-based project:\nuv init --script main.py --python 3.13\nThis creates a minimal configuration for single-script projects."
  },
  {
    "objectID": "ch/python/uv_tutorial.html#dependency-management",
    "href": "ch/python/uv_tutorial.html#dependency-management",
    "title": "UV: The Ultra-Fast Python Package Manager",
    "section": "Dependency Management",
    "text": "Dependency Management\n\nAdding Dependencies\nAdding packages to your project is straightforward:\n# Add regular dependencies\nuv add requests\nuv add beautifulsoup4\nuv add rich\n\n# Add with version constraints\nuv add \"django&gt;=4.0,&lt;5.0\"\nuv add \"requests&gt;=2.25.0\"\n\n\nDevelopment Dependencies\nSeparate your development tools from production dependencies:\n# Add development dependencies\nuv add --dev pytest\nuv add --dev black\nuv add --dev mypy\nuv add --dev ruff\n\n\nOptional Dependencies\nCreate optional dependency groups for different use cases:\n# Add optional dependencies\nuv add --optional web fastapi uvicorn\nuv add --optional data pandas numpy matplotlib\n\n# Install with optional groups\nuv sync --extra web\nuv sync --extra data\nuv sync --extra web,data\n\n\nSynchronizing Environment\nKeep your environment in sync with your project configuration:\n# Install all dependencies\nuv sync\n\n# Sync with fresh downloads (ignore cache)\nuv sync --refresh\n\n# Show what would be installed (dry run)\nuv sync --dry-run\n\n\nUnderstanding pyproject.toml\nThe pyproject.toml file is the heart of your UV project. It’s written in TOML (Tom’s Obvious Minimal Language), which is designed to be human-readable and easy to parse.\nKey sections in pyproject.toml:\n[project]\nname = \"my-awesome-project\"\nversion = \"0.1.0\"\ndescription = \"A sample project using UV\"\nrequires-python = \"&gt;=3.8\"\ndependencies = [\n    \"requests&gt;=2.25.0\",\n    \"rich&gt;=10.0.0\",\n]\n\n[project.optional-dependencies]\ndev = [\n    \"pytest&gt;=6.0\",\n    \"black&gt;=21.0\",\n    \"mypy&gt;=0.800\",\n]\nweb = [\n    \"fastapi&gt;=0.68.0\",\n    \"uvicorn&gt;=0.15.0\",\n]\n\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n\n[tool.uv]\ndev-dependencies = [\n    \"pytest&gt;=6.0\",\n    \"black&gt;=21.0\",\n]\nWhat each section means:\n\n[project]: Basic project metadata and core dependencies\n\n[project.optional-dependencies]: Optional groups you can install selectively\n\n[build-system]: How to build/package your project\n\n[tool.uv]: UV-specific configuration\n\n\n\nLock Files: Ensuring Reproducibility\nUV automatically generates uv.lock files that contain the exact versions of all dependencies (including transitive dependencies) that work together:\n# Update lock file with latest compatible versions\nuv lock\n\n# Install exactly what's in the lock file\nuv sync --frozen\nWhy lock files matter:\n\nReproducibility: Everyone on your team gets identical environments\n\nSecurity: Prevents supply chain attacks through version pinning\n\nDebugging: Easier to track down issues when versions are consistent\n\nCI/CD: Ensures production matches development\n\nLock file example snippet:\n[[package]]\nname = \"requests\"\nversion = \"2.31.0\"\nsource = { registry = \"https://pypi.org/simple\" }\ndependencies = [\n    { name = \"certifi\" },\n    { name = \"charset-normalizer\" },\n    { name = \"idna\" },\n    { name = \"urllib3\" },\n]\n\n\nSynchronization: Keeping Everything in Sync\nThe uv sync command is powerful - it ensures your environment exactly matches your project configuration:\n# Standard sync - installs missing, updates changed\nuv sync\n\n# Sync with specific extras\nuv sync --extra dev\nuv sync --extra web,dev\n\n# Frozen sync - only installs what's in uv.lock (production mode)\nuv sync --frozen\n\n# Refresh sync - ignores cache, downloads fresh\nuv sync --refresh\n\n# Dry run - see what would happen without doing it\nuv sync --dry-run\nWhat sync does:\n\nReads pyproject.toml for project requirements\nCompares with current environment\nInstalls missing packages\nRemoves packages not in configuration\nUpdates packages to match version constraints\nCreates/updates uv.lock if needed\n\nBest practices:\n\nRun uv sync after pulling changes from git\nUse uv sync --frozen in production/CI\nCommit both pyproject.toml and uv.lock to version control"
  },
  {
    "objectID": "ch/python/uv_tutorial.html#real-world-example-web-scraper",
    "href": "ch/python/uv_tutorial.html#real-world-example-web-scraper",
    "title": "UV: The Ultra-Fast Python Package Manager",
    "section": "Real-World Example: Web Scraper",
    "text": "Real-World Example: Web Scraper\nLet’s build a practical example - a news scraper with beautiful formatting.\n\nProject Setup\nmkdir news_scraper && cd news_scraper\nuv init\nuv add requests beautifulsoup4 rich\n\n\nThe Scraper Code\nCreate news_scraper.py:\n\n#!/usr/bin/env python3\n\"\"\"\nNews Scraper - Real world UV example\nDependencies: requests, beautifulsoup4, rich\n\nSetup commands:\nuv init\nuv add requests beautifulsoup4 rich\nuv run news_scraper.py\n\"\"\"\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom rich.console import Console\nfrom rich.table import Table\nfrom rich.panel import Panel\nfrom rich.progress import track, Progress\nfrom rich.spinner import Spinner\nimport time\nimport json\n\ndef scrape_sample_data():\n    \"\"\"\n    Simulated news scraping (for demo purposes)\n    In real scenario, you'd scrape actual news sites\n    \"\"\"\n    # Sample tech news data\n    return [\n        {\n            \"title\": \"AI Technology Adoption Increases Globally\",\n            \"source\": \"TechNews24\",\n            \"time\": \"2 hours ago\",\n            \"category\": \"Technology\"\n        },\n        {\n            \"title\": \"Python Programming Demand Increases by 300%\",\n            \"source\": \"DevWorld\",\n            \"time\": \"4 hours ago\", \n            \"category\": \"Programming\"\n        },\n        {\n            \"title\": \"UV Package Manager: The Fastest Alternative to pip\",\n            \"source\": \"CodeDaily\",\n            \"time\": \"6 hours ago\",\n            \"category\": \"Tools\"\n        },\n        {\n            \"title\": \"New Tech Hub Established in Silicon Valley\",\n            \"source\": \"StartupNews\",\n            \"time\": \"8 hours ago\",\n            \"category\": \"Business\"\n        },\n        {\n            \"title\": \"Developer GitHub Contributions Rise Significantly\",\n            \"source\": \"OpenSource Weekly\",\n            \"time\": \"12 hours ago\",\n            \"category\": \"Development\"\n        }\n    ]\n\ndef fetch_api_data():\n    \"\"\"\n    Fetch real data from a public API\n    \"\"\"\n    try:\n        response = requests.get(\"https://jsonplaceholder.typicode.com/posts\", timeout=5)\n        response.raise_for_status()\n        posts = response.json()[:3]  # Get first 3 posts\n        \n        return [\n            {\n                \"title\": f\"Tech Post #{post['id']}: {post['title'][:50]}...\",\n                \"source\": \"JSONPlaceholder API\",\n                \"time\": f\"{post['id']} hours ago\",\n                \"category\": \"API Data\"\n            }\n            for post in posts\n        ]\n    except requests.RequestException:\n        return []\n\ndef main():\n    console = Console()\n    \n    # Header\n    header_panel = Panel(\n        \"[bold blue]🌐 Tech News Scraper[/bold blue]\",\n        subtitle=\"Powered by UV Package Manager\",\n        border_style=\"green\"\n    )\n    console.print(header_panel)\n    \n    all_news = []\n    \n    # Scrape local sample data\n    with console.status(\"[bold green]Scraping local tech news...\", spinner=\"dots\"):\n        time.sleep(1.5)  # Simulate scraping delay\n        local_news = scrape_sample_data()\n        all_news.extend(local_news)\n    \n    console.print(\"[green]✅[/green] Local news scraped successfully!\")\n    \n    # Fetch API data\n    with console.status(\"[bold blue]Fetching data from API...\", spinner=\"bouncingBall\"):\n        time.sleep(1)\n        api_news = fetch_api_data()\n        all_news.extend(api_news)\n    \n    if api_news:\n        console.print(\"[green]✅[/green] API data fetched successfully!\")\n    else:\n        console.print(\"[yellow]⚠️[/yellow] API data unavailable (offline demo)\")\n    \n    # Process and display results\n    console.print(f\"\\n[bold cyan]📊 Total articles found: {len(all_news)}[/bold cyan]\")\n    \n    # Create results table\n    table = Table(\n        title=\"Latest Tech News\",\n        show_header=True,\n        header_style=\"bold magenta\",\n        border_style=\"blue\"\n    )\n    table.add_column(\"Title\", style=\"cyan\", width=45)\n    table.add_column(\"Source\", style=\"yellow\", width=15)\n    table.add_column(\"Category\", style=\"green\", width=12)\n    table.add_column(\"Time\", style=\"bright_black\", width=15)\n    \n    # Add news with progress animation\n    for news in track(all_news, description=\"Processing articles...\"):\n        table.add_row(\n            news[\"title\"],\n            news[\"source\"],\n            news[\"category\"],\n            news[\"time\"]\n        )\n        time.sleep(0.2)  # Animation delay\n    \n    console.print(table)\n    \n    # Save results\n    output_file = \"tech_news.json\"\n    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump(all_news, f, ensure_ascii=False, indent=2)\n    \n    console.print(f\"\\n[bold green]💾 Results saved to {output_file}[/bold green]\")\n    \n    # Summary statistics\n    categories = {}\n    for news in all_news:\n        cat = news[\"category\"]\n        categories[cat] = categories.get(cat, 0) + 1\n    \n    console.print(\"\\n[bold yellow]📈 Category Statistics:[/bold yellow]\")\n    for category, count in categories.items():\n        console.print(f\"  • {category}: {count} articles\")\n    \n    console.print(\"\\n[italic]This scraper uses UV's fast dependency management![/italic]\")\n    console.print(\"[dim]Dependencies: requests, beautifulsoup4, rich[/dim]\")\n\nif __name__ == \"__main__\":\n    main()\n\n\n\nRun the Scraper\nuv run news_scraper.py\nThis will create beautiful, formatted output showing the scraped news data!"
  },
  {
    "objectID": "ch/python/uv_tutorial.html#advanced-features",
    "href": "ch/python/uv_tutorial.html#advanced-features",
    "title": "UV: The Ultra-Fast Python Package Manager",
    "section": "Advanced Features",
    "text": "Advanced Features\n\nVirtual Environment Management\nWhile UV handles virtual environments automatically, you can also manage them manually:\n# Create a virtual environment\nuv venv\n\n# Create with specific Python version\nuv venv --python 3.13\n\n# Activate virtual environment (if needed)\nsource .venv/bin/activate  # On macOS/Linux\n.venv\\Scripts\\activate     # On Windows\n\n\nExternal Tools\nRun tools from PyPI without permanent installation:\n# Run httpie for API testing\nuv run --with httpie http GET https://api.github.com/users/octocat\n\n# Run cowsay for fun\nuv run --with cowsay cowsay \"UV is awesome!\"\n\n# Run black for code formatting\nuv run --with black black --check .\n\n\nImport and Export\nWork with existing projects and requirements:\n# Install from requirements.txt\nuv pip install -r requirements.txt\n\n# Export current dependencies\nuv export --format requirements-txt --output-file requirements.txt\nuv export --format pyproject-toml\n\n\nBuild and Publish\nPackage and distribute your projects:\n# Build distribution packages\nuv build\n\n# Publish to PyPI\nuv publish"
  },
  {
    "objectID": "ch/python/uv_tutorial.html#testing-with-uv",
    "href": "ch/python/uv_tutorial.html#testing-with-uv",
    "title": "UV: The Ultra-Fast Python Package Manager",
    "section": "Testing with UV",
    "text": "Testing with UV\n\nSetting up Tests\nAdd testing dependencies:\nuv add --dev pytest pytest-cov\nCreate a simple test file test_example.py:\n\ndef add_numbers(a, b):\n    return a + b\n\ndef multiply_numbers(a, b):\n    return a * b\n\ndef test_add_numbers():\n    assert add_numbers(2, 3) == 5\n    assert add_numbers(-1, 1) == 0\n    assert add_numbers(0, 0) == 0\n\ndef test_multiply_numbers():\n    assert multiply_numbers(2, 3) == 6\n    assert multiply_numbers(-1, 5) == -5\n    assert multiply_numbers(0, 10) == 0\n\nRun tests:\n# Run all tests\nuv run pytest\n\n# Run with verbose output\nuv run pytest -v\n\n# Run with coverage\nuv run pytest --cov\n\n# Run specific test file\nuv run pytest test_example.py"
  },
  {
    "objectID": "ch/python/uv_tutorial.html#performance-comparison",
    "href": "ch/python/uv_tutorial.html#performance-comparison",
    "title": "UV: The Ultra-Fast Python Package Manager",
    "section": "Performance Comparison",
    "text": "Performance Comparison\nHere’s why UV is so much faster than traditional tools:\n\n\n\nOperation\npip\nUV\nImprovement\n\n\n\n\nInstall requests\n12.3s\n0.8s\n15.4x faster\n\n\nInstall django\n45.2s\n2.1s\n21.5x faster\n\n\nInstall tensorflow\n180.5s\n8.3s\n21.7x faster\n\n\nCreate virtual environment\n3.2s\n0.1s\n32x faster\n\n\nDependency resolution\n8.7s\n0.3s\n29x faster\n\n\n\n\n\n\n\n\n\nWhy is UV so fast?\n\n\n\n\nWritten in Rust: Compiled language with excellent performance\nParallel Downloads: Downloads multiple packages simultaneously\nSmart Caching: Global cache shared across projects\nAdvanced Resolution: Efficient SAT solver for dependencies\nOptimized Algorithms: Modern algorithms for package management"
  },
  {
    "objectID": "ch/python/uv_tutorial.html#best-practices",
    "href": "ch/python/uv_tutorial.html#best-practices",
    "title": "UV: The Ultra-Fast Python Package Manager",
    "section": "Best Practices",
    "text": "Best Practices\n\n1. Project Organization\n\nAlways use uv init for new projects\nKeep pyproject.toml clean and well-documented\n\nCommit uv.lock to version control\nUse meaningful project names and descriptions\n\n\n\n2. Dependency Management\n\nPin critical dependency versions in pyproject.toml\nUse uv sync regularly to keep environment updated\nSeparate development and production dependencies\nUse optional dependencies for feature groups\n\n\n\n3. Python Version Management\n\nSpecify Python version requirements in pyproject.toml\nTest your project with multiple Python versions\nUse uv run --python X.Y for version-specific testing\nDocument Python version requirements\n\n\n\n4. Team Collaboration\n\nAlways commit pyproject.toml and uv.lock\nDocument special installation requirements\nUse consistent Python versions across team\nProvide clear setup instructions\n\n\n\n5. Performance Tips\n\nUse uv cache to share packages across projects\nPre-install commonly used Python versions\nUse uv sync --frozen in production environments\nLeverage UV’s parallel processing capabilities"
  },
  {
    "objectID": "ch/python/uv_tutorial.html#troubleshooting",
    "href": "ch/python/uv_tutorial.html#troubleshooting",
    "title": "UV: The Ultra-Fast Python Package Manager",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nCommon Issues and Solutions\nIssue: “uv: command not found”\n# Solution: Restart terminal or reload shell\nsource ~/.bashrc  # or ~/.zshrc\nIssue: Python version not found\n# Solution: Install the Python version first\nuv python install 3.13\nIssue: Dependencies not resolving\n# Solution: Clear cache and try again\nuv cache clean\nuv sync --refresh\nIssue: Virtual environment issues\n# Solution: Recreate virtual environment\nrm -rf .venv\nuv venv\nuv sync\n\n\nUseful Diagnostic Commands\n# Check UV configuration\nuv config\n\n# Show project information\nuv project info\n\n# Check dependency tree\nuv tree\n\n# Validate project\nuv check\n\n# Show cache location and size\nuv cache dir\nuv cache size\n\n# Clean cache\nuv cache clean"
  },
  {
    "objectID": "ch/python/uv_tutorial.html#migration-from-other-tools",
    "href": "ch/python/uv_tutorial.html#migration-from-other-tools",
    "title": "UV: The Ultra-Fast Python Package Manager",
    "section": "Migration from Other Tools",
    "text": "Migration from Other Tools\n\nFrom pip to UV\nReplace your pip workflows:\n# Old way\npython -m venv myenv\nsource myenv/bin/activate\npip install requests beautifulsoup4\n\n# New way\nuv init myproject\ncd myproject  \nuv add requests beautifulsoup4\n\n\nFrom Poetry to UV\nConvert Poetry projects:\n\nCopy dependencies from pyproject.toml [tool.poetry.dependencies]\nRun uv init in project directory\nAdd dependencies with uv add package_name\nRemove Poetry files if desired\n\n\n\nFrom pipenv to UV\nReplace Pipfile with UV:\n\nExtract packages from Pipfile\nRun uv init\nAdd packages with uv add\nUse uv run instead of pipenv run"
  },
  {
    "objectID": "ch/python/uv_tutorial.html#example-workflow",
    "href": "ch/python/uv_tutorial.html#example-workflow",
    "title": "UV: The Ultra-Fast Python Package Manager",
    "section": "Example Workflow",
    "text": "Example Workflow\nHere’s a complete workflow for a new Python project:\n# 1. Create and navigate to project directory\nmkdir my_awesome_project && cd my_awesome_project\n\n# 2. Initialize UV project\nuv init\n\n# 3. Add your dependencies\nuv add requests beautifulsoup4 rich\nuv add --dev pytest black mypy ruff\n\n# 4. Install everything\nuv sync\n\n# 5. Run your application\nuv run main.py\n\n# 6. Run tests\nuv run pytest\n\n# 7. Format code\nuv run black .\n\n# 8. Type check\nuv run mypy .\n\n# 9. Lint code\nuv run ruff check ."
  },
  {
    "objectID": "ch/python/uv_tutorial.html#conclusion",
    "href": "ch/python/uv_tutorial.html#conclusion",
    "title": "UV: The Ultra-Fast Python Package Manager",
    "section": "Conclusion",
    "text": "Conclusion\nUV represents the future of Python package management. By combining speed, reliability, and modern features into a single tool, it simplifies Python development workflows dramatically.\n\nKey Takeaways\n\nSpeed: UV is 10-100x faster than traditional tools\nSimplicity: One tool replaces many separate utilities\nReliability: Advanced dependency resolution prevents conflicts\nModern: Built-in support for lock files and reproducible builds\nCross-platform: Consistent experience across operating systems\n\n\n\nNext Steps\n\nTry UV Today: Install UV and experiment with your current projects\nMigrate Gradually: Start with new projects, then migrate existing ones\nShare with Team: Introduce UV to your development team\nStay Updated: Follow UV development for new features\nContribute: Report issues and contribute to the UV community\n\n\n\nResources\n\nOfficial Documentation: https://docs.astral.sh/uv/\nGitHub Repository: https://github.com/astral-sh/uv\nCommunity Discord: Join discussions with other UV users\nMigration Guide: Official guides for switching from other tools\n\nHappy coding with UV! 🚀\n\n\n\n\n\n\nDid you know?\n\n\n\nUV is developed by Astral, the same team behind Ruff (the ultra-fast Python linter). Their focus on performance and developer experience makes UV an excellent choice for modern Python development."
  },
  {
    "objectID": "ch/python/uv_tutorial.html#real-world-example-student-grade-calculator",
    "href": "ch/python/uv_tutorial.html#real-world-example-student-grade-calculator",
    "title": "UV: The Ultra-Fast Python Package Manager",
    "section": "Real-World Example: Student Grade Calculator",
    "text": "Real-World Example: Student Grade Calculator\nLet’s build a practical classroom example - a student grade calculator that reads data from CSV files and generates reports.\n\nProject Setup\nmkdir grade_calculator && cd grade_calculator\nuv init\nuv add pandas rich\n\n\nCreate Sample Data\nFirst, create a sample CSV file students.csv:\nname,math,science,english,history\nAlice Johnson,95,88,92,85\nBob Smith,78,91,85,88\nCarol Davis,88,95,90,92\nDavid Wilson,92,85,88,90\nEva Brown,85,92,95,88\nFrank Miller,90,88,85,92\nGrace Lee,88,90,92,85\nHenry Clark,92,88,90,88\nIvy Taylor,85,95,88,90\nJack Anderson,90,85,92,88\n\n\nThe Grade Calculator Code\nCreate grade_calculator.py:\n\n#!/usr/bin/env python3\n\"\"\"\nStudent Grade Calculator - Real world UV example\nDependencies: pandas, rich\n\nSetup commands:\nuv init\nuv add pandas rich\nuv run grade_calculator.py\n\"\"\"\n\nimport pandas as pd\nfrom rich.console import Console\nfrom rich.table import Table\nfrom rich.panel import Panel\nfrom rich.progress import track\nfrom rich.text import Text\nimport statistics\nfrom pathlib import Path\n\ndef create_sample_data():\n    \"\"\"Create sample student data if CSV doesn't exist\"\"\"\n    if not Path(\"students.csv\").exists():\n        data = {\n            'name': ['Alice Johnson', 'Bob Smith', 'Carol Davis', 'David Wilson', \n                    'Eva Brown', 'Frank Miller', 'Grace Lee', 'Henry Clark', \n                    'Ivy Taylor', 'Jack Anderson'],\n            'math': [95, 78, 88, 92, 85, 90, 88, 92, 85, 90],\n            'science': [88, 91, 95, 85, 92, 88, 90, 88, 95, 85],\n            'english': [92, 85, 90, 88, 95, 85, 92, 90, 88, 92],\n            'history': [85, 88, 92, 90, 88, 92, 85, 88, 90, 88]\n        }\n        df = pd.DataFrame(data)\n        df.to_csv('students.csv', index=False)\n        return df\n    return None\n#or load the file if you already have it, like below\ndef load_student_data():\n    \"\"\"Load student data from CSV file\"\"\"\n    try:\n        df = pd.read_csv('students.csv')\n        return df\n    except FileNotFoundError:\n        console = Console()\n        console.print(\"[red]Error: students.csv not found. Creating sample data...[/red]\")\n        return create_sample_data()\n\ndef calculate_grades(df):\n    \"\"\"Calculate grades and statistics\"\"\"\n    # Calculate average for each student\n    subject_columns = ['math', 'science', 'english', 'history']\n    df['average'] = df[subject_columns].mean(axis=1)\n    \n    # Assign letter grades\n    def get_letter_grade(avg):\n        if avg &gt;= 90: return 'A'\n        elif avg &gt;= 80: return 'B'\n        elif avg &gt;= 70: return 'C'\n        elif avg &gt;= 60: return 'D'\n        else: return 'F'\n    \n    df['letter_grade'] = df['average'].apply(get_letter_grade)\n    \n    return df\n\ndef display_student_report(df, console):\n    \"\"\"Display individual student grades\"\"\"\n    table = Table(\n        title=\"📚 Student Grade Report\",\n        show_header=True,\n        header_style=\"bold blue\",\n        border_style=\"green\"\n    )\n    \n    table.add_column(\"Student\", style=\"cyan\", width=15)\n    table.add_column(\"Math\", justify=\"center\", style=\"yellow\")\n    table.add_column(\"Science\", justify=\"center\", style=\"yellow\")\n    table.add_column(\"English\", justify=\"center\", style=\"yellow\")\n    table.add_column(\"History\", justify=\"center\", style=\"yellow\")\n    table.add_column(\"Average\", justify=\"center\", style=\"magenta\")\n    table.add_column(\"Grade\", justify=\"center\", style=\"bold green\")\n    \n    for _, student in track(df.iterrows(), description=\"Processing grades...\", total=len(df)):\n        # Color code the letter grade\n        grade_color = {\n            'A': 'bold green',\n            'B': 'green', \n            'C': 'yellow',\n            'D': 'orange1',\n            'F': 'red'\n        }.get(student['letter_grade'], 'white')\n        \n        table.add_row(\n            student['name'],\n            str(int(student['math'])),\n            str(int(student['science'])),\n            str(int(student['english'])),\n            str(int(student['history'])),\n            f\"{student['average']:.1f}\",\n            f\"[{grade_color}]{student['letter_grade']}[/{grade_color}]\"\n        )\n    \n    console.print(table)\n\ndef display_class_statistics(df, console):\n    \"\"\"Display class-wide statistics\"\"\"\n    subject_columns = ['math', 'science', 'english', 'history']\n    \n    # Calculate statistics\n    stats_table = Table(\n        title=\"📊 Class Statistics\",\n        show_header=True,\n        header_style=\"bold magenta\",\n        border_style=\"blue\"\n    )\n    \n    stats_table.add_column(\"Subject\", style=\"cyan\")\n    stats_table.add_column(\"Average\", justify=\"center\")\n    stats_table.add_column(\"Highest\", justify=\"center\", style=\"green\")\n    stats_table.add_column(\"Lowest\", justify=\"center\", style=\"red\")\n    stats_table.add_column(\"Std Dev\", justify=\"center\")\n    \n    for subject in subject_columns:\n        avg = df[subject].mean()\n        highest = df[subject].max()\n        lowest = df[subject].min()\n        std_dev = df[subject].std()\n        \n        stats_table.add_row(\n            subject.title(),\n            f\"{avg:.1f}\",\n            str(int(highest)),\n            str(int(lowest)),\n            f\"{std_dev:.1f}\"\n        )\n    \n    console.print(stats_table)\n    \n    # Grade distribution\n    grade_dist = df['letter_grade'].value_counts().sort_index()\n    \n    console.print(\"\\n[bold yellow]📈 Grade Distribution:[/bold yellow]\")\n    for grade, count in grade_dist.items():\n        percentage = (count / len(df)) * 100\n        console.print(f\"  {grade}: {count} students ({percentage:.1f}%)\")\n\ndef generate_report_file(df):\n    \"\"\"Generate a detailed report file\"\"\"\n    report_content = []\n    report_content.append(\"STUDENT GRADE REPORT\")\n    report_content.append(\"=\" * 50)\n    report_content.append(\"\")\n    \n    for _, student in df.iterrows():\n        report_content.append(f\"Student: {student['name']}\")\n        report_content.append(f\"  Math: {student['math']}\")\n        report_content.append(f\"  Science: {student['science']}\")\n        report_content.append(f\"  English: {student['english']}\")\n        report_content.append(f\"  History: {student['history']}\")\n        report_content.append(f\"  Average: {student['average']:.1f}\")\n        report_content.append(f\"  Letter Grade: {student['letter_grade']}\")\n        report_content.append(\"\")\n    \n    # Class statistics\n    report_content.append(\"CLASS STATISTICS\")\n    report_content.append(\"-\" * 30)\n    subject_columns = ['math', 'science', 'english', 'history']\n    for subject in subject_columns:\n        avg = df[subject].mean()\n        report_content.append(f\"{subject.title()} Average: {avg:.1f}\")\n    \n    report_content.append(f\"\\nClass Average: {df['average'].mean():.1f}\")\n    \n    # Save to file\n    with open('grade_report.txt', 'w') as f:\n        f.write('\\n'.join(report_content))\n\ndef main():\n    console = Console()\n    \n    # Header\n    header_panel = Panel(\n        \"[bold blue]🎓 Student Grade Calculator[/bold blue]\",\n        subtitle=\"Powered by UV Package Manager\",\n        border_style=\"green\"\n    )\n    console.print(header_panel)\n    \n    # Load data\n    with console.status(\"[bold green]Loading student data...\", spinner=\"dots\"):\n        df = load_student_data()\n        if df is None:\n            console.print(\"[red]Failed to load data![/red]\")\n            return\n    \n    console.print(f\"[green]✅[/green] Loaded data for {len(df)} students\")\n    \n    # Calculate grades\n    with console.status(\"[bold blue]Calculating grades...\", spinner=\"bouncingBall\"):\n        df = calculate_grades(df)\n    \n    console.print(\"[green]✅[/green] Grades calculated successfully!\")\n    \n    # Display results\n    display_student_report(df, console)\n    console.print(\"\")\n    display_class_statistics(df, console)\n    \n    # Generate report file\n    generate_report_file(df)\n    console.print(f\"\\n[bold green]💾 Detailed report saved to grade_report.txt[/bold green]\")\n    \n    # Summary\n    class_avg = df['average'].mean()\n    console.print(f\"\\n[bold cyan]📊 Class Average: {class_avg:.1f}[/bold cyan]\")\n    \n    highest_student = df.loc[df['average'].idxmax()]\n    console.print(f\"[bold green]🏆 Top Student: {highest_student['name']} ({highest_student['average']:.1f})[/bold green]\")\n    \n    console.print(\"\\n[italic]This calculator demonstrates UV's dependency management![/italic]\")\n    console.print(\"[dim]Dependencies: pandas, rich[/dim]\")\n\nif __name__ == \"__main__\":\n    main()\n\n\n\nRun the Calculator\nuv run grade_calculator.py\nThis will:\n\nLoad student data from the CSV file (or create sample data if needed)\nCalculate averages and assign letter grades for each student\nDisplay a beautiful grade report with color-coded grades\nShow class statistics including averages, highest/lowest scores, and standard deviation\nGenerate a text report saved to grade_report.txt\nDisplay grade distribution showing how many students got each letter grade\n\nFeatures demonstrated:\n\nData processing with pandas\nBeautiful terminal output with rich\nFile I/O operations\nStatistical calculations\nError handling for missing files\nProgress indicators and status updates"
  },
  {
    "objectID": "ch/python/panda.html#making-fields-optional-and-adding-validation",
    "href": "ch/python/panda.html#making-fields-optional-and-adding-validation",
    "title": "Panda Dataframe, scypy, numpy, data typing, etc.",
    "section": "",
    "text": "In real-world applications, not all fields are required. Let’s make our model more realistic by adding optional fields and custom validation:\n\nfrom pydantic import BaseModel, ValidationError, Field, validator\nfrom typing import List, Dict, Optional\nimport json\nfrom datetime import datetime\n\n# ---------------------\n# 1. Enhanced Patient model with optional fields and validation\n# ---------------------\nclass Patient(BaseModel):\n    patient_id: str = Field(..., min_length=4, max_length=10, description=\"Unique patient identifier\")\n    name: str = Field(..., min_length=2, max_length=50, description=\"Patient full name\")\n    age: int = Field(..., ge=0, le=150, description=\"Patient age in years\")\n    weight: float = Field(..., gt=0, le=500, description=\"Patient weight in kg\")\n    height: Optional[float] = Field(None, gt=0, le=300, description=\"Patient height in cm\")\n    married: bool = False  # Default value\n    allergies: Optional[List[str]] = Field(default=[], description=\"List of known allergies\")\n    contact_info: Dict[str, str] = Field(default_factory=dict, description=\"Contact information\")\n    emergency_contact: Optional[Dict[str, str]] = None\n    blood_type: Optional[str] = Field(None, regex=r'^(A|B|AB|O)[+-]\n, description=\"Blood type (e.g., A+, O-, AB+)\")\n    \n    # Custom validator for name formatting\n    @validator('name')\n    def name_must_not_be_empty_or_just_spaces(cls, v):\n        if not v.strip():\n            raise ValueError('Name cannot be empty or just spaces')\n        return v.strip().title()  # Capitalize properly\n    \n    # Custom validator for phone number in contact_info\n    @validator('contact_info')\n    def validate_contact_info(cls, v):\n        if 'phone' in v:\n            phone = v['phone']\n            # Simple phone validation (starts with + and has digits)\n            if not phone.startswith('+') or not phone[1:].replace('-', '').replace(' ', '').isdigit():\n                raise ValueError('Phone number must start with + and contain valid digits')\n        return v\n    \n    # Calculate BMI if height is provided\n    def calculate_bmi(self) -&gt; Optional[float]:\n        if self.height:\n            height_m = self.height / 100  # Convert cm to meters\n            return round(self.weight / (height_m ** 2), 2)\n        return None\n    \n    # Check if patient is adult\n    def is_adult(self) -&gt; bool:\n        return self.age &gt;= 18\n    \n    # Get formatted patient info\n    def get_summary(self) -&gt; str:\n        bmi = self.calculate_bmi()\n        bmi_str = f\", BMI: {bmi}\" if bmi else \"\"\n        allergies_str = f\", Allergies: {', '.join(self.allergies)}\" if self.allergies else \", No known allergies\"\n        return f\"{self.name} (ID: {self.patient_id}), Age: {self.age}, Weight: {self.weight}kg{bmi_str}{allergies_str}\"\n\n# ---------------------\n# 2. Enhanced database operations\n# ---------------------\ndb: List[Patient] = []\n\ndef insert_patient_data(patient: Patient):\n    global db\n    if any(p.patient_id == patient.patient_id for p in db):\n        print(f\"❌ Patient ID '{patient.patient_id}' already exists. Use update instead.\")\n        return False\n    db.append(patient)\n    print(f\"✅ Inserted patient: {patient.get_summary()}\")\n    return True\n\ndef update_patient_data(patient: Patient):\n    global db\n    for idx, p in enumerate(db):\n        if p.patient_id == patient.patient_id:\n            db[idx] = patient\n            print(f\"✅ Updated patient: {patient.get_summary()}\")\n            return True\n    print(f\"❌ Patient ID '{patient.patient_id}' not found. Use insert instead.\")\n    return False\n\ndef find_patient_by_id(patient_id: str) -&gt; Optional[Patient]:\n    for patient in db:\n        if patient.patient_id == patient_id:\n            return patient\n    return None\n\ndef list_all_patients():\n    if not db:\n        print(\"📭 No patients in database.\")\n        return\n    \n    print(f\"\\n👥 All Patients ({len(db)} total):\")\n    print(\"-\" * 80)\n    for patient in db:\n        print(f\"🏥 {patient.get_summary()}\")\n        if patient.blood_type:\n            print(f\"   🩸 Blood Type: {patient.blood_type}\")\n        if patient.contact_info:\n            contact_str = \", \".join([f\"{k}: {v}\" for k, v in patient.contact_info.items()])\n            print(f\"   📞 Contact: {contact_str}\")\n        print()\n\ndef get_patients_by_age_range(min_age: int, max_age: int) -&gt; List[Patient]:\n    return [p for p in db if min_age &lt;= p.age &lt;= max_age]\n\ndef get_patients_with_allergies() -&gt; List[Patient]:\n    return [p for p in db if p.allergies]\n\n# ---------------------\n# 3. Test the enhanced system\n# ---------------------\nprint(\"🏥 Testing Enhanced Patient Management System\")\nprint(\"=\" * 50)\n\ntry:\n    # Test 1: Valid patient with all fields\n    print(\"\\n🧪 Test 1: Complete patient record\")\n    patient1 = Patient(\n        patient_id='P001',\n        name='   rashed uzzaman   ',  # Will be cleaned and capitalized\n        age=29,\n        weight=65.5,\n        height=175,\n        married=True,\n        allergies=['Dust', 'Pollen', 'Cats'],\n        contact_info={'phone': '+49-123-456789', 'email': 'rashed@email.com'},\n        emergency_contact={'name': 'Jane Doe', 'phone': '+49-987-654321'},\n        blood_type='O+'\n    )\n    insert_patient_data(patient1)\n    print(f\"   BMI: {patient1.calculate_bmi()}\")\n    print(f\"   Adult: {patient1.is_adult()}\")\n    \n    # Test 2: Minimal patient record (using defaults)\n    print(\"\\n🧪 Test 2: Minimal patient record\")\n    patient2 = Patient(\n        patient_id='P002',\n        name='Alice Johnson',\n        age=35,\n        weight=58.2\n    )\n    insert_patient_data(patient2)\n    \n    # Test 3: Child patient\n    print(\"\\n🧪 Test 3: Child patient\")\n    patient3 = Patient(\n        patient_id='P003',\n        name='Bobby Smith',\n        age=12,\n        weight=40.0,\n        height=150,\n        allergies=['Peanuts'],\n        contact_info={'phone': '+49-555-123456'},\n        blood_type='A-'\n    )\n    insert_patient_data(patient3)\n    print(f\"   Adult: {patient3.is_adult()}\")\n    \n    # Test 4: Try to insert duplicate\n    print(\"\\n🧪 Test 4: Duplicate insertion attempt\")\n    insert_patient_data(patient1)\n    \n    # Test 5: Update patient\n    print(\"\\n🧪 Test 5: Update patient weight\")\n    patient1_updated = Patient(\n        patient_id='P001',\n        name='Rashed Uzzaman',\n        age=30,  # Birthday!\n        weight=67.0,  # Gained weight\n        height=175,\n        married=True,\n        allergies=['Dust', 'Pollen'],  # No longer allergic to cats!\n        contact_info={'phone': '+49-123-456789', 'email': 'rashed.new@email.com'},\n        blood_type='O+'\n    )\n    update_patient_data(patient1_updated)\n    \nexcept ValidationError as e:\n    print(f\"❌ Validation Error: {e}\")\n\n# Display all patients\nlist_all_patients()\n\n# Query examples\nprint(\"\\n🔍 Query Examples:\")\nprint(\"-\" * 30)\nadults = [p for p in db if p.is_adult()]\nprint(f\"👨‍👩‍👧‍👦 Adult patients: {len(adults)}\")\n\npatients_with_allergies = get_patients_with_allergies()\nprint(f\"🤧 Patients with allergies: {len(patients_with_allergies)}\")\nfor p in patients_with_allergies:\n    print(f\"   - {p.name}: {', '.join(p.allergies)}\")\n\nyoung_adults = get_patients_by_age_range(18, 30)\nprint(f\"🧑 Young adults (18-30): {len(young_adults)}\")\nfor p in young_adults:\n    print(f\"   - {p.name} ({p.age} years old)\")\n\nunterminated string literal (detected at line 19) (&lt;string&gt;, line 19)"
  },
  {
    "objectID": "ch/python/panda.html#advanced-validation-with-custom-validators",
    "href": "ch/python/panda.html#advanced-validation-with-custom-validators",
    "title": "Panda Dataframe, scypy, numpy, data typing, etc.",
    "section": "",
    "text": "Now let’s see what happens when we try to insert invalid data. Pydantic will catch these errors and give us helpful messages:\n\nprint(\"\\n🚨 Testing Validation Errors\")\nprint(\"=\" * 40)\n\n# Test invalid data scenarios\ntest_cases = [\n    {\n        'name': 'Invalid Age Test',\n        'data': {'patient_id': 'P999', 'name': 'Test Patient', 'age': -5, 'weight': 70},\n        'expected_error': 'Age cannot be negative'\n    },\n    {\n        'name': 'Invalid Weight Test', \n        'data': {'patient_id': 'P998', 'name': 'Test Patient', 'age': 25, 'weight': 0},\n        'expected_error': 'Weight must be greater than 0'\n    },\n    {\n        'name': 'Invalid Blood Type Test',\n        'data': {'patient_id': 'P997', 'name': 'Test Patient', 'age': 25, 'weight': 70, 'blood_type': 'XYZ'},\n        'expected_error': 'Invalid blood type format'\n    },\n    {\n        'name': 'Invalid Phone Number Test',\n        'data': {'patient_id': 'P996', 'name': 'Test Patient', 'age': 25, 'weight': 70, 'contact_info': {'phone': 'invalid-phone'}},\n        'expected_error': 'Invalid phone number format'\n    },\n    {\n        'name': 'Empty Name Test',\n        'data': {'patient_id': 'P995', 'name': '   ', 'age': 25, 'weight': 70},\n        'expected_error': 'Name cannot be empty'\n    }\n]\n\nfor test in test_cases:\n    print(f\"\\n🧪 {test['name']}:\")\n    try:\n        invalid_patient = Patient(**test['data'])\n        print(f\"   ⚠️ Unexpectedly succeeded: {invalid_patient.name}\")\n    except ValidationError as e:\n        print(f\"   ✅ Correctly caught error: {str(e).split('\\n')[0]}\")\n    except Exception as e:\n        print(f\"   ❌ Unexpected error type: {type(e).__name__}: {e}\")\n\nf-string expression part cannot include a backslash (&lt;string&gt;, line 39)"
  },
  {
    "objectID": "ch/python/panda.html#real-world-data-processing-with-pydantic",
    "href": "ch/python/panda.html#real-world-data-processing-with-pydantic",
    "title": "Panda Dataframe, scypy, numpy, data typing, etc.",
    "section": "",
    "text": "Let’s simulate reading patient data from a CSV file and using Pydantic to validate and clean it:\n\nimport csv\nfrom io import StringIO\n\n# Simulate CSV data (in real world, you'd read from a file)\ncsv_data = \"\"\"patient_id,name,age,weight,height,married,allergies,phone,email,blood_type\nP101,john doe,25,70.5,180,true,\"Dust,Pollen\",+49-111-222333,john@email.com,A+\nP102,JANE SMITH,35,65.0,,false,Peanuts,+49-444-555666,jane@email.com,O-\nP103,bob wilson,17,55.2,165,false,,+49-777-888999,bob@email.com,\nP104,invalid patient,-5,0,200,maybe,Bad Data,invalid-phone,not-an-email,XYZ\nP105,mary johnson,45,72.3,168,true,\"Shellfish,Latex\",+49-123-987654,mary@email.com,B+\n\"\"\"\n\ndef process_csv_data(csv_content: str):\n    \"\"\"Process CSV data and create Patient objects with validation\"\"\"\n    successful_patients = []\n    failed_records = []\n    \n    csv_reader = csv.DictReader(StringIO(csv_content))\n    \n    for row_num, row in enumerate(csv_reader, 1):\n        try:\n            # Clean and prepare data\n            processed_row = {\n                'patient_id': row['patient_id'].strip(),\n                'name': row['name'].strip(),\n                'age': int(row['age']),\n                'weight': float(row['weight']),\n                'married': row['married'].lower() in ['true', '1', 'yes'],\n            }\n            \n            # Handle optional fields\n            if row['height'].strip():\n                processed_row['height'] = float(row['height'])\n            \n            # Process allergies (split by comma if present)\n            if row['allergies'].strip():\n                processed_row['allergies'] = [a.strip() for a in row['allergies'].split(',')]\n            \n            # Build contact info\n            contact_info = {}\n            if row['phone'].strip():\n                contact_info['phone'] = row['phone'].strip()\n            if row['email'].strip():\n                contact_info['email'] = row['email'].strip()\n            if contact_info:\n                processed_row['contact_info'] = contact_info\n            \n            # Blood type\n            if row['blood_type'].strip():\n                processed_row['blood_type'] = row['blood_type'].strip()\n            \n            # Create Patient object (this will validate everything)\n            patient = Patient(**processed_row)\n            successful_patients.append(patient)\n            print(f\"✅ Row {row_num}: Successfully processed {patient.name}\")\n            \n        except ValidationError as e:\n            error_msg = str(e).split('\\n')[0]  # Get first error line\n            failed_records.append({'row': row_num, 'data': row, 'error': error_msg})\n            print(f\"❌ Row {row_num}: Validation failed - {error_msg}\")\n        except Exception as e:\n            failed_records.append({'row': row_num, 'data': row, 'error': str(e)})\n            print(f\"❌ Row {row_num}: Processing failed - {e}\")\n    \n    return successful_patients, failed_records\n\nprint(\"\\n📊 Processing CSV Data with Pydantic Validation\")\n\n\n📊 Processing CSV Data with Pydantic Validation\n\nprint(\"=\" * 55)\n\n=======================================================\n\nsuccessful, failed = process_csv_data(csv_data)\n\n✅ Row 1: Successfully processed john doe\n✅ Row 2: Successfully processed JANE SMITH\n❌ Row 3: Validation failed - 1 validation error for Patient\n✅ Row 4: Successfully processed invalid patient\n✅ Row 5: Successfully processed mary johnson\n\nprint(f\"\\n📈 Summary:\")\n\n\n📈 Summary:\n\nprint(f\"✅ Successfully processed: {len(successful)} patients\")\n\n✅ Successfully processed: 4 patients\n\nprint(f\"❌ Failed to process: {len(failed)} records\")\n\n❌ Failed to process: 1 records\n\nif successful:\n    print(f\"\\n👥 Successfully Imported Patients:\")\n    for patient in successful:\n        print(f\"   🏥 {patient.get_summary()}\")\n\nAttributeError: 'Patient' object has no attribute 'get_summary'\n\nif failed:\n    print(f\"\\n⚠️ Failed Records (need manual review):\")\n    for failure in failed:\n        print(f\"   Row {failure['row']}: {failure['data']['name']} - {failure['error']}\")\n\n\n⚠️ Failed Records (need manual review):\n   Row 3: bob wilson - 1 validation error for Patient"
  },
  {
    "objectID": "ch/python/panda.html#saving-and-loading-with-json-schema",
    "href": "ch/python/panda.html#saving-and-loading-with-json-schema",
    "title": "Panda Dataframe, scypy, numpy, data typing, etc.",
    "section": "",
    "text": "Pydantic can also generate JSON schemas and work seamlessly with JSON data:\n\nimport json\nfrom datetime import datetime\n\n# Generate JSON schema for our Patient model\npatient_schema = Patient.model_json_schema()\n\nprint(\"📋 Patient Model JSON Schema:\")\n\n📋 Patient Model JSON Schema:\n\nprint(\"=\" * 35)\n\n===================================\n\nprint(json.dumps(patient_schema, indent=2)[:500] + \"...\\n(truncated)\")\n\n{\n  \"properties\": {\n    \"patient_id\": {\n      \"title\": \"Patient Id\",\n      \"type\": \"string\"\n    },\n    \"name\": {\n      \"title\": \"Name\",\n      \"type\": \"string\"\n    },\n    \"age\": {\n      \"title\": \"Age\",\n      \"type\": \"integer\"\n    },\n    \"weight\": {\n      \"title\": \"Weight\",\n      \"type\": \"number\"\n    },\n    \"married\": {\n      \"title\": \"Married\",\n      \"type\": \"boolean\"\n    },\n    \"allergies\": {\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"Allergies\",\n      \"type\": \"array\"\n   ...\n(truncated)\n\n# Save all our patients to JSON with timestamp\ndef save_patients_with_metadata(filename: str = \"patients_database.json\"):\n    data = {\n        'timestamp': datetime.now().isoformat(),\n        'total_patients': len(db),\n        'schema_version': '1.0',\n        'patients': [patient.model_dump() for patient in db]\n    }\n    \n    with open(filename, 'w') as f:\n        json.dump(data, f, indent=2)\n    \n    print(f\"💾 Saved {len(db)} patients to {filename}\")\n    return filename\n\n# Load patients from JSON with validation\ndef load_patients_with_validation(filename: str = \"patients_database.json\"):\n    global db\n    try:\n        with open(filename, 'r') as f:\n            data = json.load(f)\n        \n        print(f\"📖 Loading database from {filename}\")\n        print(f\"   📅 Saved on: {data['timestamp']}\")\n        print(f\"   👥 Expected patients: {data['total_patients']}\")\n        \n        # Validate and load each patient\n        loaded_patients = []\n        for patient_data in data['patients']:\n            try:\n                patient = Patient(**patient_data)\n                loaded_patients.append(patient)\n            except ValidationError as e:\n                print(f\"   ❌ Failed to load patient {patient_data.get('name', 'Unknown')}: {e}\")\n        \n        db = loaded_patients\n        print(f\"   ✅ Successfully loaded {len(db)} patients\")\n        \n    except FileNotFoundError:\n        print(f\"❌ File {filename} not found\")\n    except json.JSONDecodeError as e:\n        print(f\"❌ Invalid JSON in {filename}: {e}\")\n    except Exception as e:\n        print(f\"❌ Error loading database: {e}\")\n\n# Save current database\nfilename = save_patients_with_metadata()\n\n💾 Saved 2 patients to patients_database.json\n\n# Clear database and reload to test\noriginal_db = db.copy()\ndb = []\nprint(f\"\\n🗑️ Cleared database (now has {len(db)} patients)\")\n\n\n🗑️ Cleared database (now has 0 patients)\n\n# Reload\nload_patients_with_validation(filename)\n\n📖 Loading database from patients_database.json\n   📅 Saved on: 2025-09-26T21:19:32.325143\n   👥 Expected patients: 2\n   ✅ Successfully loaded 2 patients\n\nprint(f\"🔄 Reloaded database (now has {len(db)} patients)\")\n\n🔄 Reloaded database (now has 2 patients)\n\n# Verify data integrity\nprint(f\"\\n🔍 Data Integrity Check:\")\n\n\n🔍 Data Integrity Check:\n\nif len(original_db) == len(db):\n    print(\"✅ Patient count matches\")\n    for orig, loaded in zip(original_db, db):\n        if orig.model_dump() == loaded.model_dump():\n            print(f\"   ✅ {orig.name} data matches perfectly\")\n        else:\n            print(f\"   ❌ {orig.name} data mismatch detected\")\nelse:\n    print(f\"❌ Patient count mismatch: original {len(original_db)}, loaded {len(db)}\")\n\n✅ Patient count matches\n   ✅ Rashed data matches perfectly\n   ✅ Rashed data matches perfectly"
  },
  {
    "objectID": "ch/python/panda.html#summary-the-power-of-pydantic",
    "href": "ch/python/panda.html#summary-the-power-of-pydantic",
    "title": "Panda Dataframe, scypy, numpy, data typing, etc.",
    "section": "",
    "text": "Throughout this journey, we’ve seen how Pydantic transforms our approach to data handling:\n\n\n\n🛡️ Automatic Validation: No more manual type checking - Pydantic does it automatically\n\n🔄 Type Coercion: Smart conversion of compatible types (string “55” → float 55.0)\n\n📝 Clear Error Messages: Helpful validation errors that pinpoint exactly what’s wrong\n\n🎨 Clean Code: Models serve as documentation and enforce data contracts\n\n🔧 Flexibility: Optional fields, default values, and custom validators\n\n🌐 JSON Integration: Seamless serialization/deserialization with validation\n\n📊 Real-world Ready: Handles complex data scenarios like CSV imports\n\n\nStarted with basic type hints (limited enforcement)\nAdded manual validation (not scalable)\nIntroduced Pydantic models (automatic validation)\nEnhanced with optional fields and custom validators\nIntegrated with real data processing (CSV, JSON)\nBuilt a complete data management system\n\n\n\nAPI Development: Validate request/response data\n\nData Processing: Clean and validate CSV/JSON imports\n\nConfiguration Management: Validate application settings\n\nDatabase Models: Ensure data integrity before persistence\n\nMicroservices: Validate inter-service communication\n\nPydantic transforms unreliable, error-prone data handling into robust, self-documenting, and maintainable code. It’s not just about validation - it’s about building confidence in your data throughout your entire application! 🎉"
  },
  {
    "objectID": "ch/python/panda.html#real-world-data-processing-with-pydantic-1",
    "href": "ch/python/panda.html#real-world-data-processing-with-pydantic-1",
    "title": "Panda Dataframe, scypy, numpy, data typing, etc.",
    "section": "Real-World Data Processing with Pydantic",
    "text": "Real-World Data Processing with Pydantic\nLet’s simulate reading patient data from a CSV file and using Pydantic to validate and clean it:\n\nimport csv\nfrom io import StringIO\n\n# Simulate CSV data (in real world, you'd read from a file)\ncsv_data = \"\"\"patient_id,name,age,weight,height,married,allergies,phone,email,blood_type\nP101,john doe,25,70.5,180,true,\"Dust,Pollen\",+49-111-222333,john@email.com,A+\nP102,JANE SMITH,35,65.0,,false,Peanuts,+49-444-555666,jane@email.com,O-\nP103,bob wilson,17,55.2,165,false,,+49-777-888999,bob@email.com,\nP104,invalid patient,-5,0,200,maybe,Bad Data,invalid-phone,not-an-email,XYZ\nP105,mary johnson,45,72.3,168,true,\"Shellfish,Latex\",+49-123-987654,mary@email.com,B+\n\"\"\"\n\ndef process_csv_data(csv_content: str):\n    \"\"\"Process CSV data and create Patient objects with validation\"\"\"\n    successful_patients = []\n    failed_records = []\n    \n    csv_reader = csv.DictReader(StringIO(csv_content))\n    \n    for row_num, row in enumerate(csv_reader, 1):\n        try:\n            # Clean and prepare data\n            processed_row = {\n                'patient_id': row['patient_id'].strip(),\n                'name': row['name'].strip(),\n                'age': int(row['age']),\n                'weight': float(row['weight']),\n                'married': row['married'].lower() in ['true', '1', 'yes'],\n            }\n            \n            # Handle optional fields\n            if row['height'].strip():\n                processed_row['height'] = float(row['height'])\n            \n            # Process allergies (split by comma if present)\n            if row['allergies'].strip():\n                processed_row['allergies'] = [a.strip() for a in row['allergies'].split(',')]\n            \n            # Build contact info\n            contact_info = {}\n            if row['phone'].strip():\n                contact_info['phone'] = row['phone'].strip()\n            if row['email'].strip():\n                contact_info['email'] = row['email'].strip()\n            if contact_info:\n                processed_row['contact_info'] = contact_info\n            \n            # Blood type\n            if row['blood_type'].strip():\n                processed_row['blood_type'] = row['blood_type'].strip()\n            \n            # Create Patient object (this will validate everything)\n            patient = Patient(**processed_row)\n            successful_patients.append(patient)\n            print(f\"✅ Row {row_num}: Successfully processed {patient.name}\")\n            \n        except ValidationError as e:\n            error_msg = str(e).split('\\n')[0]  # Get first error line\n            failed_records.append({'row': row_num, 'data': row, 'error': error_msg})\n            print(f\"❌ Row {row_num}: Validation failed - {error_msg}\")\n        except Exception as e:\n            failed_records.append({'row': row_num, 'data': row, 'error': str(e)})\n            print(f\"❌ Row {row_num}: Processing failed - {e}\")\n    \n    return successful_patients, failed_records\n\nprint(\"\\n📊 Processing CSV Data with Pydantic Validation\")\n\n\n📊 Processing CSV Data with Pydantic Validation\n\nprint(\"=\" * 55)\n\n=======================================================\n\nsuccessful, failed = process_csv_data(csv_data)\n\n✅ Row 1: Successfully processed john doe\n✅ Row 2: Successfully processed JANE SMITH\n❌ Row 3: Validation failed - 1 validation error for Patient\n✅ Row 4: Successfully processed invalid patient\n✅ Row 5: Successfully processed mary johnson\n\nprint(f\"\\n📈 Summary:\")\n\n\n📈 Summary:\n\nprint(f\"✅ Successfully processed: {len(successful)} patients\")\n\n✅ Successfully processed: 4 patients\n\nprint(f\"❌ Failed to process: {len(failed)} records\")\n\n❌ Failed to process: 1 records\n\nif successful:\n    print(f\"\\n👥 Successfully Imported Patients:\")\n    for patient in successful:\n        print(f\"   🏥 {patient.get_summary()}\")\n\nAttributeError: 'Patient' object has no attribute 'get_summary'\n\nif failed:\n    print(f\"\\n⚠️ Failed Records (need manual review):\")\n    for failure in failed:\n        print(f\"   Row {failure['row']}: {failure['data']['name']} - {failure['error']}\")\n\n\n⚠️ Failed Records (need manual review):\n   Row 3: bob wilson - 1 validation error for Patient"
  },
  {
    "objectID": "ch/python/panda.html#summary-the-power-of-pydantic-1",
    "href": "ch/python/panda.html#summary-the-power-of-pydantic-1",
    "title": "Panda Dataframe, scypy, numpy, data typing, etc.",
    "section": "Summary: The Power of Pydantic",
    "text": "Summary: The Power of Pydantic\nThroughout this journey, we’ve seen how Pydantic transforms our approach to data handling:\n🎯 Key Benefits We’ve Demonstrated:\n\n\n\n🛡️ Automatic Validation: No more manual type checking - Pydantic does it automatically\n\n🔄 Type Coercion: Smart conversion of compatible types (string “55” → float 55.0)\n\n📝 Clear Error Messages: Helpful validation errors that pinpoint exactly what’s wrong\n\n🎨 Clean Code: Models serve as documentation and enforce data contracts\n\n🔧 Flexibility: Optional fields, default values, and custom validators\n\n🌐 JSON Integration: Seamless serialization/deserialization with validation\n\n📊 Real-world Ready: Handles complex data scenarios like CSV imports\n🚀 From Simple to Sophisticated:\n\n\nStarted with basic type hints (limited enforcement)\nAdded manual validation (not scalable)\nIntroduced Pydantic models (automatic validation)\nEnhanced with optional fields and custom validators\nIntegrated with real data processing (CSV, JSON)\nBuilt a complete data management system\n💡 When to Use Pydantic:\n\n\n\nAPI Development: Validate request/response data\n\nData Processing: Clean and validate CSV/JSON imports\n\nConfiguration Management: Validate application settings\n\nDatabase Models: Ensure data integrity before persistence\n\nMicroservices: Validate inter-service communication\n\nPydantic transforms unreliable, error-prone data handling into robust, self-documenting, and maintainable code. It’s not just about validation - it’s about building confidence in your data throughout your entire application! 🎉"
  },
  {
    "objectID": "ch/prob_stat/stat.html#setting-the-stage-with-a-simple-story",
    "href": "ch/prob_stat/stat.html#setting-the-stage-with-a-simple-story",
    "title": "Statistics Basics",
    "section": "Setting the Stage with a Simple Story",
    "text": "Setting the Stage with a Simple Story\nImagine you own a coffee shop and want to understand what affects customer satisfaction scores (1-10 scale). You consider two factors:\n\n\nCoffee Type: Regular vs Decaf\n\nTime of Day: Morning vs Afternoon\n\nLet’s create this scenario with data:\n\nset.seed(42)\n\n# Create a BALANCED design first (equal sample sizes)\nn_per_cell &lt;- 20  # 20 customers in each combination\n\nbalanced_coffee &lt;- expand.grid(\n  coffee_type = c(\"Regular\", \"Decaf\"),\n  time_of_day = c(\"Morning\", \"Afternoon\"),\n  replicate = 1:n_per_cell\n) %&gt;%\n  mutate(\n    # Create satisfaction scores with main effects and interaction\n    satisfaction = case_when(\n      coffee_type == \"Regular\" & time_of_day == \"Morning\" ~ rnorm(n(), 8, 1),    # High satisfaction\n      coffee_type == \"Regular\" & time_of_day == \"Afternoon\" ~ rnorm(n(), 6, 1),  # Medium\n      coffee_type == \"Decaf\" & time_of_day == \"Morning\" ~ rnorm(n(), 5, 1),      # Low-medium\n      coffee_type == \"Decaf\" & time_of_day == \"Afternoon\" ~ rnorm(n(), 7, 1)     # Medium-high\n    ),\n    coffee_type = factor(coffee_type),\n    time_of_day = factor(time_of_day)\n  ) %&gt;%\n  select(-replicate)\n\n# Show the structure\nprint(\"Balanced Design - Sample Sizes:\")\n\n[1] \"Balanced Design - Sample Sizes:\"\n\ntable(balanced_coffee$coffee_type, balanced_coffee$time_of_day)\n\n         \n          Morning Afternoon\n  Regular      20        20\n  Decaf        20        20\n\n# Calculate means for each cell\ncell_means_balanced &lt;- balanced_coffee %&gt;%\n  group_by(coffee_type, time_of_day) %&gt;%\n  summarise(\n    mean_satisfaction = mean(satisfaction),\n    n = n(),\n    .groups = 'drop'\n  )\n\nkable(cell_means_balanced, digits = 2, \n      caption = \"Mean Satisfaction Scores - Balanced Design\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\nMean Satisfaction Scores - Balanced Design\n\ncoffee_type\ntime_of_day\nmean_satisfaction\nn\n\n\n\nRegular\nMorning\n8.31\n20\n\n\nRegular\nAfternoon\n5.74\n20\n\n\nDecaf\nMorning\n5.00\n20\n\n\nDecaf\nAfternoon\n7.01\n20"
  },
  {
    "objectID": "ch/prob_stat/stat.html#now-lets-create-reality-unbalanced-data",
    "href": "ch/prob_stat/stat.html#now-lets-create-reality-unbalanced-data",
    "title": "Statistics Basics",
    "section": "Now Let’s Create Reality: Unbalanced Data",
    "text": "Now Let’s Create Reality: Unbalanced Data\nIn real life, you don’t get equal numbers of customers in each category. Maybe fewer people order decaf in the morning:\n\n# Create UNBALANCED design (unequal sample sizes)\nset.seed(42)\n\nunbalanced_coffee &lt;- bind_rows(\n  # Regular + Morning: 30 customers (popular!)\n  data.frame(\n    coffee_type = \"Regular\",\n    time_of_day = \"Morning\",\n    satisfaction = rnorm(30, 8, 1)\n  ),\n  # Regular + Afternoon: 25 customers\n  data.frame(\n    coffee_type = \"Regular\",\n    time_of_day = \"Afternoon\",\n    satisfaction = rnorm(25, 6, 1)\n  ),\n  # Decaf + Morning: 10 customers (unpopular combination)\n  data.frame(\n    coffee_type = \"Decaf\",\n    time_of_day = \"Morning\",\n    satisfaction = rnorm(10, 5, 1)\n  ),\n  # Decaf + Afternoon: 20 customers\n  data.frame(\n    coffee_type = \"Decaf\",\n    time_of_day = \"Afternoon\",\n    satisfaction = rnorm(20, 7, 1)\n  )\n) %&gt;%\n  mutate(\n    coffee_type = factor(coffee_type),\n    time_of_day = factor(time_of_day)\n  )\n\nprint(\"Unbalanced Design - Sample Sizes:\")\n\n[1] \"Unbalanced Design - Sample Sizes:\"\n\ntable(unbalanced_coffee$coffee_type, unbalanced_coffee$time_of_day)\n\n         \n          Afternoon Morning\n  Decaf          20      10\n  Regular        25      30\n\ncell_means_unbalanced &lt;- unbalanced_coffee %&gt;%\n  group_by(coffee_type, time_of_day) %&gt;%\n  summarise(\n    mean_satisfaction = mean(satisfaction),\n    n = n(),\n    .groups = 'drop'\n  )\n\nkable(cell_means_unbalanced, digits = 2,\n      caption = \"Mean Satisfaction Scores - Unbalanced Design\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\nMean Satisfaction Scores - Unbalanced Design\n\ncoffee_type\ntime_of_day\nmean_satisfaction\nn\n\n\n\nDecaf\nAfternoon\n7.13\n20\n\n\nDecaf\nMorning\n4.94\n10\n\n\nRegular\nAfternoon\n5.92\n25\n\n\nRegular\nMorning\n8.07\n30"
  },
  {
    "objectID": "ch/prob_stat/stat.html#the-general-linear-model",
    "href": "ch/prob_stat/stat.html#the-general-linear-model",
    "title": "Statistics Basics",
    "section": "The General Linear Model",
    "text": "The General Linear Model\nANOVA is actually a special case of linear regression. Our two-way ANOVA model can be written as:\n\\[Y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\epsilon_{ijk}\\]\nWhere:\n\n\n\\(Y_{ijk}\\) = satisfaction score for the \\(k\\)-th customer with coffee type \\(i\\) and time \\(j\\)\n\n\n\\(\\mu\\) = grand mean (overall average satisfaction)\n\n\\(\\alpha_i\\) = main effect of coffee type \\(i\\) (how much Regular or Decaf changes satisfaction)\n\n\\(\\beta_j\\) = main effect of time of day \\(j\\) (how much Morning or Afternoon changes satisfaction)\n\n\\((\\alpha\\beta)_{ij}\\) = interaction effect (does the coffee type effect depend on time of day?)\n\n\\(\\epsilon_{ijk}\\) = random error (individual differences)"
  },
  {
    "objectID": "ch/prob_stat/stat.html#sum-of-squares-decomposition",
    "href": "ch/prob_stat/stat.html#sum-of-squares-decomposition",
    "title": "Statistics Basics",
    "section": "Sum of Squares Decomposition",
    "text": "Sum of Squares Decomposition\nThe total variation in our data can be decomposed:\n\\[SS_{Total} = SS_{CoffeeType} + SS_{Time} + SS_{Interaction} + SS_{Error}\\]\nIn plain English: Total variation = Variation due to coffee type + Variation due to time + Variation due to their combination + Random variation"
  },
  {
    "objectID": "ch/prob_stat/stat.html#why-do-we-need-different-types",
    "href": "ch/prob_stat/stat.html#why-do-we-need-different-types",
    "title": "Statistics Basics",
    "section": "Why Do We Need Different Types?",
    "text": "Why Do We Need Different Types?\nWith balanced data (equal sample sizes), all three types give the same answer. But with unbalanced data, they differ in how they answer questions:\n\n# Let's see the actual difference with our unbalanced data\ncat(\"Running Three Types of ANOVA on Unbalanced Data:\\n\\n\")\n\nRunning Three Types of ANOVA on Unbalanced Data:\n\n# Type I - Sequential (order matters!)\nmodel_typeI_order1 &lt;- aov(satisfaction ~ coffee_type + time_of_day + coffee_type:time_of_day, \n                          data = unbalanced_coffee)\nmodel_typeI_order2 &lt;- aov(satisfaction ~ time_of_day + coffee_type + coffee_type:time_of_day, \n                          data = unbalanced_coffee)\n\ncat(\"TYPE I ANOVA - Coffee Type First:\\n\")\n\nTYPE I ANOVA - Coffee Type First:\n\nanova(model_typeI_order1)\n\nAnalysis of Variance Table\n\nResponse: satisfaction\n                        Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ncoffee_type              1  9.214   9.214  7.9036  0.006186 ** \ntime_of_day              1 10.607  10.607  9.0985  0.003417 ** \ncoffee_type:time_of_day  1 84.400  84.400 72.3995 7.439e-13 ***\nResiduals               81 94.426   1.166                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncat(\"\\nTYPE I ANOVA - Time of Day First:\\n\")\n\n\nTYPE I ANOVA - Time of Day First:\n\nanova(model_typeI_order2)\n\nAnalysis of Variance Table\n\nResponse: satisfaction\n                        Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ntime_of_day              1 14.481  14.481 12.4218 0.0007011 ***\ncoffee_type              1  5.339   5.339  4.5802 0.0353516 *  \ntime_of_day:coffee_type  1 84.400  84.400 72.3995 7.439e-13 ***\nResiduals               81 94.426   1.166                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncat(\"\\nNotice how the results differ based on order? That's the Type I problem!\\n\")\n\n\nNotice how the results differ based on order? That's the Type I problem!"
  },
  {
    "objectID": "ch/prob_stat/stat.html#type-i-sum-of-squares-sequential",
    "href": "ch/prob_stat/stat.html#type-i-sum-of-squares-sequential",
    "title": "Statistics Basics",
    "section": "Type I Sum of Squares (Sequential)",
    "text": "Type I Sum of Squares (Sequential)\nMathematical Definition:\n\n\n\\(SS(\\alpha | \\mu)\\) = Sum of squares for A after fitting the mean\n\n\\(SS(\\beta | \\mu, \\alpha)\\) = Sum of squares for B after fitting mean and A\n\n\\(SS(\\alpha\\beta | \\mu, \\alpha, \\beta)\\) = Sum of squares for interaction after fitting everything else\n\nPlain English: Type I asks “What does each factor explain that wasn’t already explained by factors entered before it?”"
  },
  {
    "objectID": "ch/prob_stat/stat.html#type-ii-sum-of-squares-hierarchical",
    "href": "ch/prob_stat/stat.html#type-ii-sum-of-squares-hierarchical",
    "title": "Statistics Basics",
    "section": "Type II Sum of Squares (Hierarchical)",
    "text": "Type II Sum of Squares (Hierarchical)\nMathematical Definition:\n\n\n\\(SS(\\alpha | \\mu, \\beta)\\) = Sum of squares for A after fitting mean and B\n\n\\(SS(\\beta | \\mu, \\alpha)\\) = Sum of squares for B after fitting mean and A\n\n\\(SS(\\alpha\\beta | \\mu, \\alpha, \\beta)\\) = Sum of squares for interaction after main effects\n\nPlain English: Type II asks “What does each main effect explain that the other main effect doesn’t, ignoring interactions?”\nWhen to use: When you want to test main effects assuming no interaction (most common in practice)\n\nprint(\"TYPE II - Hierarchical Sum of Squares\")\n\n[1] \"TYPE II - Hierarchical Sum of Squares\"\n\nsep_line(\"=\", 50)\n\n================================================== \n\nAnova(model_type1, type = \"II\")\n\nAnova Table (Type II tests)\n\nResponse: satisfaction\n                        Sum Sq Df F value    Pr(&gt;F)    \ncoffee_type              5.339  1  4.5802  0.035352 *  \ntime_of_day             10.607  1  9.0985  0.003417 ** \ncoffee_type:time_of_day 84.400  1 72.3995 7.439e-13 ***\nResiduals               94.426 81                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "ch/prob_stat/stat.html#type-iii-sum-of-squares-marginal",
    "href": "ch/prob_stat/stat.html#type-iii-sum-of-squares-marginal",
    "title": "Statistics Basics",
    "section": "Type III Sum of Squares (Marginal)",
    "text": "Type III Sum of Squares (Marginal)\nMathematical Definition:\n\n\n\\(SS(\\alpha | \\mu, \\beta, \\alpha\\beta)\\) = Sum of squares for A after fitting everything else\n\n\\(SS(\\beta | \\mu, \\alpha, \\alpha\\beta)\\) = Sum of squares for B after fitting everything else\n\n\\(SS(\\alpha\\beta | \\mu, \\alpha, \\beta)\\) = Sum of squares for interaction after main effects\n\nPlain English: Type III asks “What does each effect explain that isn’t explained by any other effect, including interactions?”\n\n# Type III implementation\n# IMPORTANT: Must use sum-to-zero contrasts for Type III\ncontrasts(unbalanced_coffee$coffee_type) &lt;- contr.sum(2)\ncontrasts(unbalanced_coffee$time_of_day) &lt;- contr.sum(2)\n\nmodel_type3 &lt;- lm(satisfaction ~ coffee_type * time_of_day, data = unbalanced_coffee)\n\nprint(\"TYPE III - Marginal Sum of Squares\")\n\n[1] \"TYPE III - Marginal Sum of Squares\"\n\nsep_line(\"=\", 50)\n\n================================================== \n\nAnova(model_type3, type = \"III\")\n\nAnova Table (Type III tests)\n\nResponse: satisfaction\n                         Sum Sq Df   F value    Pr(&gt;F)    \n(Intercept)             3041.77  1 2609.2756 &lt; 2.2e-16 ***\ncoffee_type               16.40  1   14.0657 0.0003302 ***\ntime_of_day                0.01  1    0.0077 0.9302036    \ncoffee_type:time_of_day   84.40  1   72.3995 7.439e-13 ***\nResiduals                 94.43 81                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "ch/prob_stat/stat.html#what-makes-data-balanced",
    "href": "ch/prob_stat/stat.html#what-makes-data-balanced",
    "title": "Statistics Basics",
    "section": "What Makes Data Balanced?",
    "text": "What Makes Data Balanced?\n\n# Function to check balance\ncheck_balance &lt;- function(data, formula_str) {\n  # Get the design matrix\n  model_matrix &lt;- model.matrix(as.formula(formula_str), data)\n  \n  # Check orthogonality\n  cors &lt;- cor(model_matrix[, -1])  # Exclude intercept\n  \n  # Create visualization\n  library(corrplot)\n  corrplot(cors, method = \"color\", type = \"upper\", \n           title = \"Correlation Between Design Variables\",\n           mar = c(0,0,2,0))\n  \n  return(cors)\n}\n\ncat(\"Balanced Design - Correlations between predictors:\\n\")\n\nBalanced Design - Correlations between predictors:\n\nbalanced_cors &lt;- check_balance(balanced_coffee, \"~ coffee_type * time_of_day\")\n\n\n\ncat(\"\\n\\nUnbalanced Design - Correlations between predictors:\\n\")\n\n\n\nUnbalanced Design - Correlations between predictors:\n\nunbalanced_cors &lt;- check_balance(unbalanced_coffee, \"~ coffee_type * time_of_day\")\n\n\n\ncat(\"\\nIn balanced designs, predictors are orthogonal (correlation ≈ 0)\")\n\n\nIn balanced designs, predictors are orthogonal (correlation ≈ 0)\n\ncat(\"\\nIn unbalanced designs, predictors are correlated (non-zero correlations)\")\n\n\nIn unbalanced designs, predictors are correlated (non-zero correlations)"
  },
  {
    "objectID": "ch/prob_stat/stat.html#why-balance-matters",
    "href": "ch/prob_stat/stat.html#why-balance-matters",
    "title": "Statistics Basics",
    "section": "Why Balance Matters",
    "text": "Why Balance Matters\n\n# Run same analysis on balanced and unbalanced data\nrun_all_types &lt;- function(data, design_name) {\n  cat(paste(\"\\n\", design_name, \"DESIGN RESULTS\\n\"))\n  cat(paste(rep(\"=\", 50), collapse = \"\"), \"\\n\")\n  \n  # Reset contrasts for Type I/II\n  contrasts(data$coffee_type) &lt;- contr.treatment(2)\n  contrasts(data$time_of_day) &lt;- contr.treatment(2)\n  \n  model &lt;- lm(satisfaction ~ coffee_type * time_of_day, data = data)\n  \n  # Type I\n  type1 &lt;- anova(model)\n  cat(\"\\nType I - Coffee SS:\", round(type1$`Sum Sq`[1], 2), \n      \"Time SS:\", round(type1$`Sum Sq`[2], 2), \"\\n\")\n  \n  # Type II\n  type2 &lt;- Anova(model, type = \"II\")\n  cat(\"Type II - Coffee SS:\", round(type2$`Sum Sq`[1], 2),\n      \"Time SS:\", round(type2$`Sum Sq`[2], 2), \"\\n\")\n  \n  # Type III\n  contrasts(data$coffee_type) &lt;- contr.sum(2)\n  contrasts(data$time_of_day) &lt;- contr.sum(2)\n  model3 &lt;- lm(satisfaction ~ coffee_type * time_of_day, data = data)\n  type3 &lt;- Anova(model3, type = \"III\")\n  cat(\"Type III - Coffee SS:\", round(type3$`Sum Sq`[2], 2),\n      \"Time SS:\", round(type3$`Sum Sq`[3], 2), \"\\n\")\n}\n\nrun_all_types(balanced_coffee, \"BALANCED\")\n\n\n BALANCED DESIGN RESULTS\n================================================== \n\nType I - Coffee SS: 21.03 Time SS: 1.6 \nType II - Coffee SS: 21.03 Time SS: 1.6 \nType III - Coffee SS: 21.03 Time SS: 1.6 \n\nrun_all_types(unbalanced_coffee, \"UNBALANCED\")\n\n\n UNBALANCED DESIGN RESULTS\n================================================== \n\nType I - Coffee SS: 9.21 Time SS: 10.61 \nType II - Coffee SS: 5.34 Time SS: 10.61 \nType III - Coffee SS: 16.4 Time SS: 0.01 \n\ncat(\"\\n\\nKEY INSIGHT: With balanced data, all three types give similar results!\")\n\n\n\nKEY INSIGHT: With balanced data, all three types give similar results!\n\ncat(\"\\nWith unbalanced data, results diverge significantly.\")\n\n\nWith unbalanced data, results diverge significantly."
  },
  {
    "objectID": "ch/prob_stat/stat.html#creating-data-with-unequal-variances",
    "href": "ch/prob_stat/stat.html#creating-data-with-unequal-variances",
    "title": "Statistics Basics",
    "section": "Creating Data with Unequal Variances",
    "text": "Creating Data with Unequal Variances\n\nset.seed(123)\n\n# Create data with very different variances\nhetero_data &lt;- bind_rows(\n  data.frame(\n    group = \"A\",\n    value = rnorm(30, mean = 50, sd = 2)  # Small variance\n  ),\n  data.frame(\n    group = \"B\", \n    value = rnorm(30, mean = 52, sd = 8)  # Medium variance\n  ),\n  data.frame(\n    group = \"C\",\n    value = rnorm(30, mean = 54, sd = 15) # Large variance\n  )\n) %&gt;%\n  mutate(group = factor(group))\n\n# Calculate actual variances\nvariance_summary &lt;- hetero_data %&gt;%\n  group_by(group) %&gt;%\n  summarise(\n    Mean = mean(value),\n    Variance = var(value),\n    SD = sd(value),\n    n = n(),\n    .groups = 'drop'\n  )\n\nkable(variance_summary, digits = 2,\n      caption = \"Group Statistics with Heterogeneous Variances\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\nGroup Statistics with Heterogeneous Variances\n\ngroup\nMean\nVariance\nSD\nn\n\n\n\nA\n49.91\n3.85\n1.96\n30\n\n\nB\n53.43\n44.64\n6.68\n30\n\n\nC\n54.37\n170.22\n13.05\n30\n\n\n\n\n# Visualize the different variances\np1 &lt;- ggplot(hetero_data, aes(x = group, y = value, fill = group)) +\n  geom_boxplot(alpha = 0.7) +\n  geom_point(position = position_jitter(width = 0.1), alpha = 0.3) +\n  labs(title = \"Heterogeneous Variances\",\n       subtitle = \"Notice the very different spreads\") +\n  theme_minimal()\n\np2 &lt;- ggplot(hetero_data, aes(x = value, fill = group)) +\n  geom_density(alpha = 0.5) +\n  labs(title = \"Density Plot\", \n       subtitle = \"Different widths = different variances\") +\n  theme_minimal()\n\np1 | p2"
  },
  {
    "objectID": "ch/prob_stat/stat.html#consequences-of-unequal-variances",
    "href": "ch/prob_stat/stat.html#consequences-of-unequal-variances",
    "title": "Statistics Basics",
    "section": "Consequences of Unequal Variances",
    "text": "Consequences of Unequal Variances\n\n# Standard ANOVA (assumes equal variances)\nstandard_anova &lt;- aov(value ~ group, data = hetero_data)\ncat(\"Standard ANOVA (assumes equal variances):\\n\")\n\nStandard ANOVA (assumes equal variances):\n\nsummary(standard_anova)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)\ngroup        2    332   165.9   2.275  0.109\nResiduals   87   6343    72.9               \n\n# Welch's ANOVA (does NOT assume equal variances)\ncat(\"\\n\\nWelch's ANOVA (robust to unequal variances):\\n\")\n\n\n\nWelch's ANOVA (robust to unequal variances):\n\nwelch_test &lt;- oneway.test(value ~ group, data = hetero_data, var.equal = FALSE)\nprint(welch_test)\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  value and group\nF = 5.2616, num df = 2.000, denom df = 42.497, p-value = 0.009086\n\n# Games-Howell post-hoc (for unequal variances)\nlibrary(rstatix)\ncat(\"\\n\\nGames-Howell Post-hoc Test (for unequal variances):\\n\")\n\n\n\nGames-Howell Post-hoc Test (for unequal variances):\n\ngames_howell &lt;- games_howell_test(hetero_data, value ~ group)\nprint(games_howell)\n\n# A tibble: 3 × 8\n  .y.   group1 group2 estimate conf.low conf.high p.adj p.adj.signif\n* &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       \n1 value A      B         3.52     0.406      6.64 0.024 *           \n2 value A      C         4.46    -1.47      10.4  0.17  ns          \n3 value B      C         0.940   -5.56       7.43 0.934 ns"
  },
  {
    "objectID": "ch/prob_stat/stat.html#decision-tree-for-anova-types",
    "href": "ch/prob_stat/stat.html#decision-tree-for-anova-types",
    "title": "Statistics Basics",
    "section": "Decision Tree for ANOVA Types",
    "text": "Decision Tree for ANOVA Types\n\n# Pseudo-code for decision making\ndecision_tree &lt;- \"\n1. Is your design balanced?\n   ├─ YES → All types give same results, use Type I (it's fastest)\n   └─ NO → Continue to step 2\n\n2. Do you have an interaction term?\n   ├─ NO → Use Type II (tests main effects properly)\n   └─ YES → Continue to step 3\n\n3. Is the interaction significant?\n   ├─ NO → Use Type II (cleaner interpretation of main effects)\n   └─ YES → Use Type III (tests main effects in presence of interaction)\n\n4. Special cases:\n   - Nested designs → Type I with proper ordering\n   - Purely exploratory → Type III for most conservative approach\n   - Following specific field conventions → Use what your field uses\n\"\n\ncat(decision_tree)"
  },
  {
    "objectID": "ch/prob_stat/stat.html#complete-analysis-pipeline",
    "href": "ch/prob_stat/stat.html#complete-analysis-pipeline",
    "title": "Statistics Basics",
    "section": "Complete Analysis Pipeline",
    "text": "Complete Analysis Pipeline\n\nanalyze_data_complete &lt;- function(data, dv, factors) {\n  formula_str &lt;- paste(dv, \"~\", paste(factors, collapse = \" * \"))\n  formula_obj &lt;- as.formula(formula_str)\n  \n  cat(paste(rep(\"=\", 60), collapse = \"\"), \"\\n\")\n  cat(\"COMPLETE ANOVA ANALYSIS PIPELINE\\n\")\n  cat(paste(rep(\"=\", 60), collapse = \"\"), \"\\n\")\n  \n  # 1. Check balance\n  cat(\"1. CHECKING BALANCE:\\n\")\n  cat(paste(rep(\"-\", 40), collapse = \"\"), \"\\n\")\n  design_table &lt;- table(data[[factors[1]]], data[[factors[2]]])\n  print(design_table)\n  is_balanced &lt;- length(unique(as.vector(design_table))) == 1\n  cat(\"\\nDesign is\", ifelse(is_balanced, \"BALANCED\", \"UNBALANCED\"), \"\\n\\n\")\n  \n  # 2. Check assumptions\n  cat(\"2. CHECKING ASSUMPTIONS:\\n\")\n  cat(paste(rep(\"-\", 40), collapse = \"\"), \"\\n\")\n  \n  # Normality by group\n  cat(\"Shapiro-Wilk tests by group:\\n\")\n  for(lvl1 in unique(data[[factors[1]]])) {\n    for(lvl2 in unique(data[[factors[2]]])) {\n      subset_data &lt;- data[data[[factors[1]]] == lvl1 & data[[factors[2]]] == lvl2, dv]\n      if(length(subset_data) &gt; 3) {\n        p_val &lt;- shapiro.test(subset_data)$p.value\n        cat(sprintf(\"  %s-%s: p = %.3f %s\\n\", \n                    lvl1, lvl2, p_val,\n                    ifelse(p_val &lt; 0.05, \"⚠️ Non-normal\", \"✓ Normal\")))\n      }\n    }\n  }\n  \n  # Homogeneity of variance\n  cat(\"\\nLevene's Test for Equal Variances:\\n\")\n  levene_p &lt;- leveneTest(formula_obj, data = data)$`Pr(&gt;F)`[1]\n  cat(sprintf(\"  p = %.3f %s\\n\", levene_p,\n              ifelse(levene_p &lt; 0.05, \"⚠️ Unequal variances\", \"✓ Equal variances\")))\n  \n  # 3. Choose and run appropriate ANOVA\n  cat(\"\\n3. ANOVA RESULTS:\\n\")\n  cat(paste(rep(\"-\", 40), collapse = \"\"), \"\\n\")\n  \n  if(is_balanced) {\n    cat(\"Using Type I (all types equivalent for balanced design)\\n\\n\")\n    model &lt;- aov(formula_obj, data = data)\n    print(summary(model))\n  } else {\n    cat(\"Using Type II (unbalanced design, testing main effects)\\n\\n\")\n    model &lt;- lm(formula_obj, data = data)\n    print(Anova(model, type = \"II\"))\n    \n    cat(\"\\n\\nAlso showing Type III for comparison:\\n\")\n    # Set sum-to-zero contrasts for Type III\n    for(f in factors) {\n      contrasts(data[[f]]) &lt;- contr.sum(nlevels(data[[f]]))\n    }\n    model3 &lt;- lm(formula_obj, data = data)\n    print(Anova(model3, type = \"III\"))\n  }\n  \n  # 4. Effect sizes\n  cat(\"\\n4. EFFECT SIZES:\\n\")\n  cat(paste(rep(\"-\", 40), collapse = \"\"), \"\\n\")\n  if(is_balanced) {\n    ss_total &lt;- sum(summary(model)[[1]]$`Sum Sq`)\n    ss_effects &lt;- summary(model)[[1]]$`Sum Sq`[1:(length(factors)+1)]\n    eta_squared &lt;- ss_effects / ss_total\n    \n    effect_names &lt;- c(factors, \"Interaction\")\n    for(i in 1:length(eta_squared)) {\n      cat(sprintf(\"%s: η² = %.3f \", effect_names[i], eta_squared[i]))\n      if(eta_squared[i] &lt; 0.01) cat(\"(negligible)\\n\")\n      else if(eta_squared[i] &lt; 0.06) cat(\"(small)\\n\")\n      else if(eta_squared[i] &lt; 0.14) cat(\"(medium)\\n\")\n      else cat(\"(large)\\n\")\n    }\n  }\n  \n  # 5. Post-hoc tests if needed\n  cat(\"\\n5. POST-HOC COMPARISONS:\\n\")\n  cat(paste(rep(\"-\", 40), collapse = \"\"), \"\\n\")\n  if(levene_p &lt; 0.05) {\n    cat(\"Using Games-Howell (unequal variances)\\n\")\n    # Would implement Games-Howell here\n  } else {\n    cat(\"Using Tukey HSD (equal variances)\\n\")\n    if(is_balanced) {\n      print(TukeyHSD(model))\n    }\n  }\n  \n  return(invisible(model))\n}\n\n# Run the complete pipeline\nfinal_model &lt;- analyze_data_complete(unbalanced_coffee, \"satisfaction\", \n                                    c(\"coffee_type\", \"time_of_day\"))\n\n============================================================ \nCOMPLETE ANOVA ANALYSIS PIPELINE\n============================================================ \n1. CHECKING BALANCE:\n---------------------------------------- \n         \n          Afternoon Morning\n  Decaf          20      10\n  Regular        25      30\n\nDesign is UNBALANCED \n\n2. CHECKING ASSUMPTIONS:\n---------------------------------------- \nShapiro-Wilk tests by group:\n  Regular-Morning: p = 0.350 ✓ Normal\n  Regular-Afternoon: p = 0.602 ✓ Normal\n  Decaf-Morning: p = 0.028 ⚠️ Non-normal\n  Decaf-Afternoon: p = 0.237 ✓ Normal\n\nLevene's Test for Equal Variances:\n  p = 0.581 ✓ Equal variances\n\n3. ANOVA RESULTS:\n---------------------------------------- \nUsing Type II (unbalanced design, testing main effects)\n\nAnova Table (Type II tests)\n\nResponse: satisfaction\n                        Sum Sq Df F value    Pr(&gt;F)    \ncoffee_type              5.339  1  4.5802  0.035352 *  \ntime_of_day             10.607  1  9.0985  0.003417 ** \ncoffee_type:time_of_day 84.400  1 72.3995 7.439e-13 ***\nResiduals               94.426 81                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAlso showing Type III for comparison:\nAnova Table (Type III tests)\n\nResponse: satisfaction\n                         Sum Sq Df   F value    Pr(&gt;F)    \n(Intercept)             3041.77  1 2609.2756 &lt; 2.2e-16 ***\ncoffee_type               16.40  1   14.0657 0.0003302 ***\ntime_of_day                0.01  1    0.0077 0.9302036    \ncoffee_type:time_of_day   84.40  1   72.3995 7.439e-13 ***\nResiduals                 94.43 81                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n4. EFFECT SIZES:\n---------------------------------------- \n\n5. POST-HOC COMPARISONS:\n---------------------------------------- \nUsing Tukey HSD (equal variances)"
  },
  {
    "objectID": "ch/prob_stat/stat.html#the-essential-points",
    "href": "ch/prob_stat/stat.html#the-essential-points",
    "title": "Statistics Basics",
    "section": "The Essential Points",
    "text": "The Essential Points\n\n\n\n\n\n\nKey Takeaways\n\n\n\n1. ANOVA Types exist because of unbalanced designs\n\nBalanced designs: All types give same results\nUnbalanced designs: Results differ, choice matters\n\n2. Type I (Sequential)\n\nTests each factor after those before it\nOrder matters!\nUse when: You have a natural hierarchy\n\n3. Type II (Hierarchical)\n\nTests main effects adjusting for other main effects\nAssumes no interaction\nUse when: Testing main effects, interaction not significant\n\n4. Type III (Marginal)\n\nTests each effect adjusting for all others\nMost conservative\nUse when: Interaction is significant\n\n5. Practical Advice\n\nAlways check assumptions first\nReport which type you used and why\nConsider effect sizes, not just p-values\nBe transparent about unbalanced designs"
  },
  {
    "objectID": "ch/prob_stat/stat.html#mathematical-summary",
    "href": "ch/prob_stat/stat.html#mathematical-summary",
    "title": "Statistics Basics",
    "section": "Mathematical Summary",
    "text": "Mathematical Summary\nThe fundamental difference is in the hypotheses being tested:\n\nType I (Sequential):\\(H_0: \\alpha_i = 0 \\;|\\; \\mu\\)\nType II (No interaction):\\(H_0: \\alpha_i = 0 \\;|\\; \\mu, \\beta_j\\)\nType III (Marginal):\\(H_0: \\alpha_i = 0 \\;|\\; \\mu, \\beta_j, (\\alpha\\beta)_{ij}\\)\n\nHere, the vertical bar “\\(\\;|\\;\\)” means “given that we’ve already accounted for …”.\nVisual Summary of Differences\n\n# Create a visual summary of when each type \"claims\" variance\nlibrary(ggplot2)\nlibrary(tidyr)\n\n# Create conceptual data for visualization\nvariance_allocation &lt;- data.frame(\n  Type = rep(c(\"Type I\", \"Type II\", \"Type III\"), each = 3),\n  Component = rep(c(\"Coffee Unique\", \"Shared\", \"Time Unique\"), 3),\n  Allocation = c(\n    # Type I: Coffee gets unique + shared\n    100, 100, 0,  # Coffee tested first gets all shared\n    # Type II: Each gets only unique\n    100, 50, 100,  # Shared split conceptually\n    # Type III: Most conservative\n    100, 0, 100   # Neither gets shared\n  )\n)\n\nggplot(variance_allocation, aes(x = Component, y = Allocation, fill = Type)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Conceptual Variance Allocation by ANOVA Type\",\n       subtitle = \"How each type 'claims' variance in unbalanced designs\",\n       y = \"Variance Allocated (%)\",\n       x = \"Variance Component\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\nFinal Recommendations Table\n\nfinal_recommendations &lt;- data.frame(\n  `Research Question` = c(\n    \"Do factors A and B affect the outcome?\",\n    \"What is the unique contribution of A?\",\n    \"Does A matter after controlling for everything?\",\n    \"Following a causal chain A→B→C\",\n    \"Interaction is significant\"\n  ),\n  `Best Type` = c(\n    \"Type II\",\n    \"Type II\",\n    \"Type III\",\n    \"Type I\",\n    \"Type III\"\n  ),\n  `Why` = c(\n    \"Tests main effects properly without assuming interaction\",\n    \"Type II isolates unique variance of each factor\",\n    \"Type III is most conservative, controls for all\",\n    \"Type I respects the sequential nature\",\n    \"Type III tests main effects in presence of interaction\"\n  ),\n  check.names = FALSE\n)\n\nkable(final_recommendations,\n      caption = \"Final Recommendations for ANOVA Type Selection\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\nFinal Recommendations for ANOVA Type Selection\n\nResearch Question\nBest Type\nWhy\n\n\n\nDo factors A and B affect the outcome?\nType II\nTests main effects properly without assuming interaction\n\n\nWhat is the unique contribution of A?\nType II\nType II isolates unique variance of each factor\n\n\nDoes A matter after controlling for everything?\nType III\nType III is most conservative, controls for all\n\n\nFollowing a causal chain A→B→C\nType I\nType I respects the sequential nature\n\n\nInteraction is significant\nType III\nType III tests main effects in presence of interaction\n\n\n\n\n\nRemember this Above All\n\n\n\n\n\n\nThe Golden Rule of ANOVA Types\n\n\n\nIf your design is balanced, rejoice! All types give the same answer.\nIf your design is unbalanced:\n\n\nCheck if interaction is significant\n\n\nIf NO interaction → Use Type II\n\n\nIf YES interaction → Use Type III\n\n\nIf natural hierarchy → Consider Type I\n\n\nAlways report: Which type you used and why!"
  },
  {
    "objectID": "ch/prob_stat/stat.html#final-code-quick-reference-function",
    "href": "ch/prob_stat/stat.html#final-code-quick-reference-function",
    "title": "Statistics Basics",
    "section": "Final Code: Quick Reference Function",
    "text": "Final Code: Quick Reference Function\n\n# Quick function to compare all three types\ncompare_anova_types &lt;- function(formula, data) {\n  require(car)\n  \n  # Ensure factors\n  factors &lt;- all.vars(formula)[-1]\n  for(f in factors) {\n    if(f %in% names(data)) {\n      data[[f]] &lt;- factor(data[[f]])\n    }\n  }\n  \n  # Type I\n  model1 &lt;- lm(formula, data = data)\n  \n  # Type II\n  model2 &lt;- model1\n  \n  # Type III (need sum contrasts)\n  data_type3 &lt;- data\n  for(f in factors) {\n    if(f %in% names(data_type3)) {\n      contrasts(data_type3[[f]]) &lt;- contr.sum(nlevels(data_type3[[f]]))\n    }\n  }\n  model3 &lt;- lm(formula, data = data_type3)\n  \n  # Results\n  cat(\"\\n========== TYPE I (Sequential) ==========\\n\")\n  print(anova(model1))\n  \n  cat(\"\\n========== TYPE II (No Interaction) ==========\\n\")\n  print(Anova(model2, type = \"II\"))\n  \n  cat(\"\\n========== TYPE III (Marginal) ==========\\n\")\n  print(Anova(model3, type = \"III\"))\n  \n  cat(\"\\n========== RECOMMENDATION ==========\\n\")\n  design_table &lt;- table(data[[factors[1]]], data[[factors[2]]])\n  is_balanced &lt;- length(unique(as.vector(design_table))) == 1\n  \n  if(is_balanced) {\n    cat(\"✓ Balanced design detected - all types equivalent\\n\")\n    cat(\"→ Use Type I for computational efficiency\\n\")\n  } else {\n    cat(\"⚠️ Unbalanced design detected\\n\")\n    interaction_p &lt;- anova(model1)$`Pr(&gt;F)`[3]\n    if(!is.na(interaction_p) && interaction_p &lt; 0.05) {\n      cat(\"→ Significant interaction (p =\", round(interaction_p, 3), \")\\n\")\n      cat(\"→ RECOMMEND: Type III for main effects interpretation\\n\")\n    } else {\n      cat(\"→ No significant interaction\\n\")\n      cat(\"→ RECOMMEND: Type II for main effects testing\\n\")\n    }\n  }\n}\n\n# Test it\ncompare_anova_types(satisfaction ~ coffee_type * time_of_day, unbalanced_coffee)\n\n\n========== TYPE I (Sequential) ==========\nAnalysis of Variance Table\n\nResponse: satisfaction\n                        Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ncoffee_type              1  9.214   9.214  7.9036  0.006186 ** \ntime_of_day              1 10.607  10.607  9.0985  0.003417 ** \ncoffee_type:time_of_day  1 84.400  84.400 72.3995 7.439e-13 ***\nResiduals               81 94.426   1.166                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n========== TYPE II (No Interaction) ==========\nAnova Table (Type II tests)\n\nResponse: satisfaction\n                        Sum Sq Df F value    Pr(&gt;F)    \ncoffee_type              5.339  1  4.5802  0.035352 *  \ntime_of_day             10.607  1  9.0985  0.003417 ** \ncoffee_type:time_of_day 84.400  1 72.3995 7.439e-13 ***\nResiduals               94.426 81                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n========== TYPE III (Marginal) ==========\nAnova Table (Type III tests)\n\nResponse: satisfaction\n                         Sum Sq Df   F value    Pr(&gt;F)    \n(Intercept)             3041.77  1 2609.2756 &lt; 2.2e-16 ***\ncoffee_type               16.40  1   14.0657 0.0003302 ***\ntime_of_day                0.01  1    0.0077 0.9302036    \ncoffee_type:time_of_day   84.40  1   72.3995 7.439e-13 ***\nResiduals                 94.43 81                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n========== RECOMMENDATION ==========\n⚠️ Unbalanced design detected\n→ Significant interaction (p = 0 )\n→ RECOMMEND: Type III for main effects interpretation"
  },
  {
    "objectID": "ch/prob_stat/stat.html#setting-the-stage-with-a-simple-story-1",
    "href": "ch/prob_stat/stat.html#setting-the-stage-with-a-simple-story-1",
    "title": "Statistics Basics",
    "section": "Setting the Stage with a Simple Story",
    "text": "Setting the Stage with a Simple Story\nImagine you own a coffee shop and want to understand what affects customer satisfaction scores (1-10 scale). You consider two factors:\n\n\nCoffee Type: Regular vs Decaf\n\nTime of Day: Morning vs Afternoon\n\nLet’s create this scenario with data:\nset.seed(42)\n\n# Create a BALANCED design first (equal sample sizes)\nn_per_cell &lt;- 20  # 20 customers in each combination\n\nbalanced_coffee &lt;- expand.grid(\n  coffee_type = c(\"Regular\", \"Decaf\"),\n  time_of_day = c(\"Morning\", \"Afternoon\"),\n  replicate = 1:n_per_cell\n) %&gt;%\n  mutate(\n    # Create satisfaction scores with main effects and interaction\n    satisfaction = case_when(\n      coffee_type == \"Regular\" & time_of_day == \"Morning\" ~ rnorm(n(), 8, 1),    # High satisfaction\n      coffee_type == \"Regular\" & time_of_day == \"Afternoon\" ~ rnorm(n(), 6, 1),  # Medium\n      coffee_type == \"Decaf\" & time_of_day == \"Morning\" ~ rnorm(n(), 5, 1),      # Low-medium\n      coffee_type == \"Decaf\" & time_of_day == \"Afternoon\" ~ rnorm(n(), 7, 1)     # Medium-high\n    ),\n    coffee_type = factor(coffee_type),\n    time_of_day = factor(time_of_day)\n  ) %&gt;%\n  select(-replicate)\n\n# Show the structure\ncat(\"Balanced Design - Sample Sizes:\\n\")\ntable(balanced_coffee$coffee_type, balanced_coffee$time_of_day)\n\n# Calculate means for each cell\ncell_means_balanced &lt;- balanced_coffee %&gt;%\n  group_by(coffee_type, time_of_day) %&gt;%\n  summarise(\n    mean_satisfaction = mean(satisfaction),\n    n = n(),\n    .groups = 'drop'\n  )\n\nkable(cell_means_balanced, digits = 2, \n      caption = \"Mean Satisfaction Scores - Balanced Design\")"
  },
  {
    "objectID": "ch/prob_stat/stat.html#now-lets-create-reality-unbalanced-data-1",
    "href": "ch/prob_stat/stat.html#now-lets-create-reality-unbalanced-data-1",
    "title": "Statistics Basics",
    "section": "Now Let’s Create Reality: Unbalanced Data",
    "text": "Now Let’s Create Reality: Unbalanced Data\nIn real life, you don’t get equal numbers of customers in each category. Maybe fewer people order decaf in the morning:\n# Create UNBALANCED design (unequal sample sizes)\nset.seed(42)\n\nunbalanced_coffee &lt;- bind_rows(\n  # Regular + Morning: 30 customers (popular!)\n  data.frame(\n    coffee_type = \"Regular\",\n    time_of_day = \"Morning\",\n    satisfaction = rnorm(30, 8, 1)\n  ),\n  # Regular + Afternoon: 25 customers\n  data.frame(\n    coffee_type = \"Regular\",\n    time_of_day = \"Afternoon\",\n    satisfaction = rnorm(25, 6, 1)\n  ),\n  # Decaf + Morning: 10 customers (unpopular combination)\n  data.frame(\n    coffee_type = \"Decaf\",\n    time_of_day = \"Morning\",\n    satisfaction = rnorm(10, 5, 1)\n  ),\n  # Decaf + Afternoon: 20 customers\n  data.frame(\n    coffee_type = \"Decaf\",\n    time_of_day = \"Afternoon\",\n    satisfaction = rnorm(20, 7, 1)\n  )\n) %&gt;%\n  mutate(\n    coffee_type = factor(coffee_type),\n    time_of_day = factor(time_of_day)\n  )\n\ncat(\"\\nUnbalanced Design - Sample Sizes:\\n\")\ntable(unbalanced_coffee$coffee_type, unbalanced_coffee$time_of_day)\n\ncell_means_unbalanced &lt;- unbalanced_coffee %&gt;%\n  group_by(coffee_type, time_of_day) %&gt;%\n  summarise(\n    mean_satisfaction = mean(satisfaction),\n    n = n(),\n    .groups = 'drop'\n  )\n\nkable(cell_means_unbalanced, digits = 2,\n      caption = \"Mean Satisfaction Scores - Unbalanced Design\")"
  },
  {
    "objectID": "ch/prob_stat/stat.html#the-general-linear-model-1",
    "href": "ch/prob_stat/stat.html#the-general-linear-model-1",
    "title": "Statistics Basics",
    "section": "The General Linear Model",
    "text": "The General Linear Model\nANOVA is actually a special case of linear regression. Our two-way ANOVA model can be written as:\n\\[Y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\epsilon_{ijk}\\]\nWhere:\n\n\n\\(Y_{ijk}\\) = satisfaction score for the \\(k\\)-th customer with coffee type \\(i\\) and time \\(j\\)\n\n\n\\(\\mu\\) = grand mean (overall average satisfaction)\n\n\\(\\alpha_i\\) = main effect of coffee type \\(i\\) (how much Regular or Decaf changes satisfaction)\n\n\\(\\beta_j\\) = main effect of time of day \\(j\\) (how much Morning or Afternoon changes satisfaction)\n\n\\((\\alpha\\beta)_{ij}\\) = interaction effect (does the coffee type effect depend on time of day?)\n\n\\(\\epsilon_{ijk}\\) = random error (individual differences)"
  },
  {
    "objectID": "ch/prob_stat/stat.html#sum-of-squares-decomposition-1",
    "href": "ch/prob_stat/stat.html#sum-of-squares-decomposition-1",
    "title": "Statistics Basics",
    "section": "Sum of Squares Decomposition",
    "text": "Sum of Squares Decomposition\nThe total variation in our data can be decomposed:\n\\[SS_{Total} = SS_{CoffeeType} + SS_{Time} + SS_{Interaction} + SS_{Error}\\]\nIn plain English:\n\nTotal variation = Variation due to coffee type + Variation due to time + Variation due to their combination + Random variation"
  },
  {
    "objectID": "ch/prob_stat/stat.html#why-do-we-need-different-types-1",
    "href": "ch/prob_stat/stat.html#why-do-we-need-different-types-1",
    "title": "Statistics Basics",
    "section": "Why Do We Need Different Types?",
    "text": "Why Do We Need Different Types?\nWith balanced data (equal sample sizes), all three types give the same answer. But with unbalanced data, they differ in how they answer questions:\n# Let's see the actual difference with our unbalanced data\ncat(\"Running Three Types of ANOVA on Unbalanced Data:\\n\\n\")\n\n# Type I - Sequential (order matters!)\nmodel_typeI_order1 &lt;- aov(satisfaction ~ coffee_type + time_of_day + coffee_type:time_of_day, \n                          data = unbalanced_coffee)\nmodel_typeI_order2 &lt;- aov(satisfaction ~ time_of_day + coffee_type + coffee_type:time_of_day, \n                          data = unbalanced_coffee)\n\ncat(\"TYPE I ANOVA - Coffee Type First:\\n\")\nanova(model_typeI_order1)\n\ncat(\"\\nTYPE I ANOVA - Time of Day First:\\n\")\nanova(model_typeI_order2)\n\ncat(\"\\nNotice how the results differ based on order? That's the Type I problem!\\n\")"
  },
  {
    "objectID": "ch/prob_stat/stat.html#type-i-sum-of-squares-sequential-1",
    "href": "ch/prob_stat/stat.html#type-i-sum-of-squares-sequential-1",
    "title": "Statistics Basics",
    "section": "Type I Sum of Squares (Sequential)",
    "text": "Type I Sum of Squares (Sequential)\nMathematical Definition:\n\n\n\\(SS(\\alpha | \\mu)\\) = Sum of squares for A after fitting the mean\n\n\\(SS(\\beta | \\mu, \\alpha)\\) = Sum of squares for B after fitting mean and A\n\n\\(SS(\\alpha\\beta | \\mu, \\alpha, \\beta)\\) = Sum of squares for interaction after fitting everything else\n\nPlain English: Type I asks “What does each factor explain that wasn’t already explained by factors entered before it?”\nWhen to use: When you have a natural ordering of importance (rare in practice)\n# Type I implementation\ncat(\"TYPE I - Sequential Sum of Squares\\n\")\ncat(\"=====================================\\n\")\nmodel_type1 &lt;- lm(satisfaction ~ coffee_type + time_of_day + coffee_type:time_of_day, \n                  data = unbalanced_coffee)\nanova_type1 &lt;- anova(model_type1)\nprint(anova_type1)\n\ncat(\"\\nInterpretation: Each factor's SS is calculated after removing effects of previous factors\")"
  },
  {
    "objectID": "ch/prob_stat/stat.html#type-ii-sum-of-squares-hierarchical-1",
    "href": "ch/prob_stat/stat.html#type-ii-sum-of-squares-hierarchical-1",
    "title": "Statistics Basics",
    "section": "Type II Sum of Squares (Hierarchical)",
    "text": "Type II Sum of Squares (Hierarchical)\nMathematical Definition:\n\n\n\\(SS(\\alpha | \\mu, \\beta)\\) = Sum of squares for A after fitting mean and B\n\n\\(SS(\\beta | \\mu, \\alpha)\\) = Sum of squares for B after fitting mean and A\n\n\\(SS(\\alpha\\beta | \\mu, \\alpha, \\beta)\\) = Sum of squares for interaction after main effects\n\nPlain English: Type II asks “What does each main effect explain that the other main effect doesn’t, ignoring interactions?”\nWhen to use: When you want to test main effects assuming no interaction (most common in practice)\n# Type II implementation\ncat(\"\\nTYPE II - Hierarchical Sum of Squares\\n\")\ncat(\"======================================\\n\")\nAnova(model_type1, type = \"II\")\n\ncat(\"\\nInterpretation: Each main effect is tested adjusting for other main effects, but NOT interaction\")"
  },
  {
    "objectID": "ch/prob_stat/stat.html#type-iii-sum-of-squares-marginal-1",
    "href": "ch/prob_stat/stat.html#type-iii-sum-of-squares-marginal-1",
    "title": "Statistics Basics",
    "section": "Type III Sum of Squares (Marginal)",
    "text": "Type III Sum of Squares (Marginal)\nMathematical Definition:\n\n\n\\(SS(\\alpha | \\mu, \\beta, \\alpha\\beta)\\) = Sum of squares for A after fitting everything else\n\n\\(SS(\\beta | \\mu, \\alpha, \\alpha\\beta)\\) = Sum of squares for B after fitting everything else\n\n\\(SS(\\alpha\\beta | \\mu, \\alpha, \\beta)\\) = Sum of squares for interaction after main effects\n\nPlain English: Type III asks “What does each effect explain that isn’t explained by any other effect, including interactions?”\nWhen to use: When you have significant interactions and want to test main effects in their presence\n# Type III implementation\n# IMPORTANT: Must use sum-to-zero contrasts for Type III\ncontrasts(unbalanced_coffee$coffee_type) &lt;- contr.sum(2)\ncontrasts(unbalanced_coffee$time_of_day) &lt;- contr.sum(2)\n\nmodel_type3 &lt;- lm(satisfaction ~ coffee_type * time_of_day, data = unbalanced_coffee)\n\ncat(\"\\nTYPE III - Marginal Sum of Squares\\n\")\ncat(\"====================================\\n\")\nAnova(model_type3, type = \"III\")\n\ncat(\"\\nInterpretation: Each effect is tested adjusting for ALL other effects, including interactions\")"
  },
  {
    "objectID": "ch/prob_stat/stat.html#what-makes-data-balanced-1",
    "href": "ch/prob_stat/stat.html#what-makes-data-balanced-1",
    "title": "Statistics Basics",
    "section": "What Makes Data Balanced?",
    "text": "What Makes Data Balanced?\n# Function to check balance\ncheck_balance &lt;- function(data, formula_str) {\n  # Get the design matrix\n  model_matrix &lt;- model.matrix(as.formula(formula_str), data)\n  \n  # Check orthogonality\n  cors &lt;- cor(model_matrix[, -1])  # Exclude intercept\n  \n  # Create visualization\n  library(corrplot)\n  corrplot(cors, method = \"color\", type = \"upper\", \n           title = \"Correlation Between Design Variables\",\n           mar = c(0,0,2,0))\n  \n  return(cors)\n}\n\ncat(\"Balanced Design - Correlations between predictors:\\n\")\nbalanced_cors &lt;- check_balance(balanced_coffee, \"~ coffee_type * time_of_day\")\n\ncat(\"\\n\\nUnbalanced Design - Correlations between predictors:\\n\")\nunbalanced_cors &lt;- check_balance(unbalanced_coffee, \"~ coffee_type * time_of_day\")\n\ncat(\"\\nIn balanced designs, predictors are orthogonal (correlation ≈ 0)\")\ncat(\"\\nIn unbalanced designs, predictors are correlated (non-zero correlations)\")"
  },
  {
    "objectID": "ch/prob_stat/stat.html#why-balance-matters-1",
    "href": "ch/prob_stat/stat.html#why-balance-matters-1",
    "title": "Statistics Basics",
    "section": "Why Balance Matters",
    "text": "Why Balance Matters\n# Run same analysis on balanced and unbalanced data\nrun_all_types &lt;- function(data, design_name) {\n  cat(paste(\"\\n\", design_name, \"DESIGN RESULTS\\n\"))\n  cat(paste(rep(\"=\", 50), collapse = \"\"), \"\\n\")\n  \n  # Reset contrasts for Type I/II\n  contrasts(data$coffee_type) &lt;- contr.treatment(2)\n  contrasts(data$time_of_day) &lt;- contr.treatment(2)\n  \n  model &lt;- lm(satisfaction ~ coffee_type * time_of_day, data = data)\n  \n  # Type I\n  type1 &lt;- anova(model)\n  cat(\"\\nType I - Coffee SS:\", round(type1$`Sum Sq`[1], 2), \n      \"Time SS:\", round(type1$`Sum Sq`[2], 2), \"\\n\")\n  \n  # Type II\n  type2 &lt;- Anova(model, type = \"II\")\n  cat(\"Type II - Coffee SS:\", round(type2$`Sum Sq`[1], 2),\n      \"Time SS:\", round(type2$`Sum Sq`[2], 2), \"\\n\")\n  \n  # Type III\n  contrasts(data$coffee_type) &lt;- contr.sum(2)\n  contrasts(data$time_of_day) &lt;- contr.sum(2)\n  model3 &lt;- lm(satisfaction ~ coffee_type * time_of_day, data = data)\n  type3 &lt;- Anova(model3, type = \"III\")\n  cat(\"Type III - Coffee SS:\", round(type3$`Sum Sq`[2], 2),\n      \"Time SS:\", round(type3$`Sum Sq`[3], 2), \"\\n\")\n}\n\nrun_all_types(balanced_coffee, \"BALANCED\")\nrun_all_types(unbalanced_coffee, \"UNBALANCED\")\n\ncat(\"\\n\\nKEY INSIGHT: With balanced data, all three types give similar results!\")\ncat(\"\\nWith unbalanced data, results diverge significantly.\")"
  },
  {
    "objectID": "ch/prob_stat/stat.html#creating-data-with-unequal-variances-1",
    "href": "ch/prob_stat/stat.html#creating-data-with-unequal-variances-1",
    "title": "Statistics Basics",
    "section": "Creating Data with Unequal Variances",
    "text": "Creating Data with Unequal Variances\nset.seed(123)\n\n# Create data with very different variances\nhetero_data &lt;- bind_rows(\n  data.frame(\n    group = \"A\",\n    value = rnorm(30, mean = 50, sd = 2)  # Small variance\n  ),\n  data.frame(\n    group = \"B\",\n    value = rnorm(30, mean = 52, sd = 8)  # Medium variance\n  ),\n  data.frame(\n    group = \"C\",\n    value = rnorm(30, mean = 54, sd = 15) # Large variance\n  )\n) %&gt;%\n  mutate(group = factor(group))\n\n# Visualize the different variances\np1 &lt;- ggplot(hetero_data, aes(x = group, y = value, fill = group)) +\n  geom_boxplot(alpha = 0.7) +\n  geom_point(position = position_jitter(width = 0.1), alpha = 0.3) +\n  labs(title = \"Heterogeneous Variances\",\n       subtitle = \"Notice the very different spreads\") +\n  theme_minimal()\n\np2 &lt;- ggplot(hetero_data, aes(x = value, fill = group)) +\n  geom_density(alpha = 0.5) +\n  labs(title = \"Density Plot\",\n       subtitle = \"Different widths = different variances\") +\n  theme_minimal()\n\np1 | p2\n\n# Test for homogeneity of variance\ncat(\"\\nLevene's Test for Homogeneity of Variance:\\n\")\nlevene_test &lt;- leveneTest(value ~ group, data = hetero_data)\nprint(levene_test)\n\ncat(\"\\nBartlett's Test (more sensitive to non-normality):\\n\")\nbartlett_test &lt;- bartlett.test(value ~ group, data = hetero_data)\nprint(bartlett_test)"
  },
  {
    "objectID": "ch/prob_stat/stat.html#consequences-of-unequal-variances-1",
    "href": "ch/prob_stat/stat.html#consequences-of-unequal-variances-1",
    "title": "Statistics Basics",
    "section": "Consequences of Unequal Variances",
    "text": "Consequences of Unequal Variances\n# Standard ANOVA (assumes equal variances)\nstandard_anova &lt;- aov(value ~ group, data = hetero_data)\ncat(\"Standard ANOVA (assumes equal variances):\\n\")\nsummary(standard_anova)\n\n# Welch's ANOVA (does NOT assume equal variances)\ncat(\"\\n\\nWelch's ANOVA (robust to unequal variances):\\n\")\nwelch_test &lt;- oneway.test(value ~ group, data = hetero_data, var.equal = FALSE)\nprint(welch_test)\n\n# Games-Howell post-hoc (for unequal variances)\nlibrary(rstatix)\ncat(\"\\n\\nGames-Howell Post-hoc Test (for unequal variances):\\n\")\ngames_howell &lt;- games_howell_test(hetero_data, value ~ group)\nprint(games_howell)"
  },
  {
    "objectID": "ch/prob_stat/stat.html#decision-tree-for-anova-types-1",
    "href": "ch/prob_stat/stat.html#decision-tree-for-anova-types-1",
    "title": "Statistics Basics",
    "section": "Decision Tree for ANOVA Types",
    "text": "Decision Tree for ANOVA Types\n```{r decision-tree, eval=FALSE} # Pseudo-code for decision making decision_tree &lt;- ” 1. Is your design balanced? ├─ YES → All types give same results, use Type I (it’s fastest) └─ NO → Continue to step 2\n\nDo you have an interaction term? ├─ NO → Use Type II (tests main effects properly) └─ YES → Continue to step 3\nIs the interaction significant? ├─ NO → Use Type II (cleaner interpretation of main effects) └─ YES → Use Type III (tests main effects in presence of interaction)\n\nSpecial cases:\n\nNested designs → Type I with proper ordering\nPurely exploratory → Type III for most conservative approach\nFollowing specific field conventions → Use what your field uses ”\n\n\n\ncat(decision_tree)\n\n## Complete Analysis Pipeline\n\n```{r complete-pipeline}\nanalyze_data_complete &lt;- function(data, dv, factors) {\n  formula_str &lt;- paste(dv, \"~\", paste(factors, collapse = \" * \"))\n  formula_obj &lt;- as.formula(formula_str)\n  \n  cat(paste(rep(\"=\", 60), collapse = \"\"), \"\\n\")\n  cat(\"COMPLETE ANOVA ANALYSIS PIPELINE\\n\")\n  cat(paste(rep(\"=\", 60), collapse = \"\"), \"\\n\")\n  \n  # 1. Check balance\n  cat(\"1. CHECKING BALANCE:\\n\")\n  cat(paste(rep(\"-\", 40), collapse = \"\"), \"\\n\")\n  design_table &lt;- table(data[[factors[1]]], data[[factors[2]]])\n  print(design_table)\n  is_balanced &lt;- length(unique(as.vector(design_table))) == 1\n  cat(\"\\nDesign is\", ifelse(is_balanced, \"BALANCED\", \"UNBALANCED\"), \"\\n\\n\")\n  \n  # 2. Check assumptions\n  cat(\"2. CHECKING ASSUMPTIONS:\\n\")\n  cat(paste(rep(\"-\", 40), collapse = \"\"), \"\\n\")\n  \n  # Normality by group\n  cat(\"Shapiro-Wilk tests by group:\\n\")\n  for(lvl1 in unique(data[[factors[1]]])) {\n    for(lvl2 in unique(data[[factors[2]]])) {\n      subset_data &lt;- data[data[[factors[1]]] == lvl1 & data[[factors[2]]] == lvl2, dv]\n      if(length(subset_data) &gt; 3) {\n        p_val &lt;- shapiro.test(subset_data)$p.value\n        cat(sprintf(\"  %s-%s: p = %.3f %s\\n\", \n                    lvl1, lvl2, p_val,\n                    ifelse(p_val &lt; 0.05, \"⚠️ Non-normal\", \"✓ Normal\")))\n      }\n    }\n  }\n  \n  # Homogeneity of variance\n  cat(\"\\nLevene's Test for Equal Variances:\\n\")\n  levene_p &lt;- leveneTest(formula_obj, data = data)$`Pr(&gt;F)`[1]\n  cat(sprintf(\"  p = %.3f %s\\n\", levene_p,\n              ifelse(levene_p &lt; 0.05, \"⚠️ Unequal variances\", \"✓ Equal variances\")))\n  \n  # 3. Choose and run appropriate ANOVA\n  cat(\"\\n3. ANOVA RESULTS:\\n\")\n  cat(paste(rep(\"-\", 40), collapse = \"\"), \"\\n\")\n  \n  if(is_balanced) {\n    cat(\"Using Type I (all types equivalent for balanced design)\\n\\n\")\n    model &lt;- aov(formula_obj, data = data)\n    print(summary(model))\n  } else {\n    cat(\"Using Type II (unbalanced design, testing main effects)\\n\\n\")\n    model &lt;- lm(formula_obj, data = data)\n    print(Anova(model, type = \"II\"))\n    \n    cat(\"\\n\\nAlso showing Type III for comparison:\\n\")\n    # Set sum-to-zero contrasts for Type III\n    for(f in factors) {\n      contrasts(data[[f]]) &lt;- contr.sum(nlevels(data[[f]]))\n    }\n    model3 &lt;- lm(formula_obj, data = data)\n    print(Anova(model3, type = \"III\"))\n  }\n  \n  # 4. Effect sizes\n  cat(\"\\n4. EFFECT SIZES:\\n\")\n  cat(paste(rep(\"-\", 40), collapse = \"\"), \"\\n\")\n  if(is_balanced) {\n    ss_total &lt;- sum(summary(model)[[1]]$`Sum Sq`)\n    ss_effects &lt;- summary(model)[[1]]$`Sum Sq`[1:(length(factors)+1)]\n    eta_squared &lt;- ss_effects / ss_total\n    \n    effect_names &lt;- c(factors, \"Interaction\")\n    for(i in 1:length(eta_squared)) {\n      cat(sprintf(\"%s: η² = %.3f \", effect_names[i], eta_squared[i]))\n      if(eta_squared[i] &lt; 0.01) cat(\"(negligible)\\n\")\n      else if(eta_squared[i] &lt; 0.06) cat(\"(small)\\n\")\n      else if(eta_squared[i] &lt; 0.14) cat(\"(medium)\\n\")\n      else cat(\"(large)\\n\")\n    }\n  }\n  \n  # 5. Post-hoc tests if needed\n  cat(\"\\n5. POST-HOC COMPARISONS:\\n\")\n  cat(paste(rep(\"-\", 40), collapse = \"\"), \"\\n\")\n  if(levene_p &lt; 0.05) {\n    cat(\"Using Games-Howell (unequal variances)\\n\")\n    # Would implement Games-Howell here\n  } else {\n    cat(\"Using Tukey HSD (equal variances)\\n\")\n    if(is_balanced) {\n      print(TukeyHSD(model))\n    }\n  }\n  \n  return(invisible(model))\n}\n\n# Run the complete pipeline\nfinal_model &lt;- analyze_data_complete(unbalanced_coffee, \"satisfaction\", \n                                    c(\"coffee_type\", \"time_of_day\"))"
  },
  {
    "objectID": "ch/prob_stat/stat.html#the-essential-points-1",
    "href": "ch/prob_stat/stat.html#the-essential-points-1",
    "title": "Statistics Basics",
    "section": "The Essential Points",
    "text": "The Essential Points\n\n\nANOVA Types exist because of unbalanced designs\n\nBalanced designs: All types give same results\nUnbalanced designs: Results differ, choice matters\n\n\n\nType I (Sequential)\n\nTests each factor after those before it\nOrder matters!\nUse when: You have a natural hierarchy\n\n\n\nType II (Hierarchical)\n\nTests main effects adjusting for other main effects\nAssumes no interaction\nUse when: Testing main effects, interaction not significant\n\n\n\nType III (Marginal)\n\nTests each effect adjusting for all others\nMost conservative\nUse when: Interaction is significant\n\n\n\nPractical Advice\n\nAlways check assumptions first\nReport which type you used and why\nConsider effect sizes, not just p-values\nBe transparent about unbalanced designs"
  },
  {
    "objectID": "ch/prob_stat/stat.html#mathematical-summary-1",
    "href": "ch/prob_stat/stat.html#mathematical-summary-1",
    "title": "Statistics Basics",
    "section": "Mathematical Summary",
    "text": "Mathematical Summary\nThe fundamental difference is in the hypotheses being tested:\nType I: \\(H_0: \\alpha_i = 0 | \\mu\\) (sequential)\nType II: \\(H_0: \\alpha_i = 0 | \\mu, \\beta_j\\) (no interaction)\nType III: \\(H_0: \\alpha_i = 0 | \\mu, \\beta_j, (\\alpha\\beta)_{ij}\\) (marginal)\nWhere the vertical bar “|” means “given that we’ve already accounted for…”"
  },
  {
    "objectID": "ch/prob_stat/stat.html#final-code-quick-reference-function-1",
    "href": "ch/prob_stat/stat.html#final-code-quick-reference-function-1",
    "title": "Statistics Basics",
    "section": "Final Code: Quick Reference Function",
    "text": "Final Code: Quick Reference Function\n# Quick function to compare all three types\ncompare_anova_types &lt;- function(formula, data) {\n  require(car)\n  \n  # Ensure factors\n  factors &lt;- all.vars(formula)[-1]\n  for(f in factors) {\n    if(f %in% names(data)) {\n      data[[f]] &lt;- factor(data[[f]])\n    }\n  }\n  \n  # Type I\n  model1 &lt;- lm(formula, data = data)\n  \n  # Type II\n  model2 &lt;- model1\n  \n  # Type III (need sum contrasts)\n  data_type3 &lt;- data\n  for(f in factors) {\n    if(f %in% names(data_type3)) {\n      contrasts(data_type3[[f]]) &lt;- contr.sum(nlevels(data_type3[[f]]))\n    }\n  }\n  model3 &lt;- lm(formula, data = data_type3)\n  \n  # Results\n  cat(\"\\n========== TYPE I (Sequential) ==========\\n\")\n  print(anova(model1))\n  \n  cat(\"\\n========== TYPE II (No Interaction) ==========\\n\")\n  print(Anova(model2, type = \"II\"))\n  \n  cat(\"\\n========== TYPE III (Marginal) ==========\\n\")\n  print(Anova(model3, type = \"III\"))\n  \n  cat(\"\\n========== RECOMMENDATION ==========\\n\")\n  design_table &lt;- table(data[[factors[1]]], data[[factors[2]]])\n  is_balanced &lt;- length(unique(as.vector(design_table))) == 1\n  \n  if(is_balanced) {\n    cat(\"✓ Balanced design detected - all types equivalent\\n\")\n    cat(\"→ Use Type I for computational efficiency\\n\")\n  } else {\n    cat(\"⚠️ Unbalanced design detected\\n\")\n    interaction_p &lt;- anova(model1)$`Pr(&gt;F)`[3]\n    if(!is.na(interaction_p) && interaction_p &lt; 0.05) {\n      cat(\"→ Significant interaction (p =\", round(interaction_p, 3), \")\\n\")\n      cat(\"→ RECOMMEND: Type III for main effects interpretation\\n\")\n    } else {\n      cat(\"→ No significant interaction\\n\")\n      cat(\"→ RECOMMEND: Type II for main effects testing\\n\")\n    }\n  }\n}\n\n# Test it\ncompare_anova_types(satisfaction ~ coffee_type * time_of_day, unbalanced_coffee)\n\nThis complete R Markdown document provides a comprehensive tutorial on ANOVA types with mathematical notation, plain English explanations, practical examples, and detailed coverage of balanced vs. unbalanced designs. You can copy and paste this directly into your `.qmd` file and it will run all the analyses step by step.\n:::"
  },
  {
    "objectID": "ch/prob_stat/stat.html#understanding-sequential-vs-simultaneous-testing",
    "href": "ch/prob_stat/stat.html#understanding-sequential-vs-simultaneous-testing",
    "title": "Statistics Basics",
    "section": "Understanding Sequential vs Simultaneous Testing",
    "text": "Understanding Sequential vs Simultaneous Testing\n\n# Create a visual demonstration of why order matters\nset.seed(123)\n\n# Function to calculate partial correlations and visualize\ndemonstrate_order_effects &lt;- function(data) {\n  # Calculate total sum of squares\n  grand_mean &lt;- mean(data$satisfaction)\n  ss_total &lt;- sum((data$satisfaction - grand_mean)^2)\n  \n  # Calculate group means\n  coffee_means &lt;- tapply(data$satisfaction, data$coffee_type, mean)\n  time_means &lt;- tapply(data$satisfaction, data$time_of_day, mean)\n  \n  # Calculate marginal sums of squares (ignoring the other factor)\n  ss_coffee_alone &lt;- sum(table(data$coffee_type) * (coffee_means - grand_mean)^2)\n  ss_time_alone &lt;- sum(table(data$time_of_day) * (time_means - grand_mean)^2)\n  \n  # Create a data frame for visualization\n  results &lt;- data.frame(\n    Approach = c(\"Coffee First\", \"Time First\", \"Coffee Alone\", \"Time Alone\"),\n    Coffee_SS = c(ss_coffee_alone, NA, ss_coffee_alone, NA),\n    Time_SS = c(NA, ss_time_alone, NA, ss_time_alone),\n    Order = c(\"1st\", \"1st\", \"Marginal\", \"Marginal\")\n  )\n  \n  return(list(\n    ss_total = ss_total,\n    ss_coffee_alone = ss_coffee_alone,\n    ss_time_alone = ss_time_alone,\n    coffee_means = coffee_means,\n    time_means = time_means,\n    grand_mean = grand_mean\n  ))\n}\n\n# Apply to both datasets\nbalanced_results &lt;- demonstrate_order_effects(balanced_coffee)\nunbalanced_results &lt;- demonstrate_order_effects(unbalanced_coffee)\n\n# Create comparison table\ncomparison_df &lt;- data.frame(\n  Design = c(\"Balanced\", \"Balanced\", \"Unbalanced\", \"Unbalanced\"),\n  Factor = c(\"Coffee Type\", \"Time of Day\", \"Coffee Type\", \"Time of Day\"),\n  `SS When Tested First` = c(\n    balanced_results$ss_coffee_alone,\n    balanced_results$ss_time_alone,\n    unbalanced_results$ss_coffee_alone,\n    unbalanced_results$ss_time_alone\n  ),\n  check.names = FALSE\n)\n\nkable(comparison_df, digits = 2, \n      caption = \"Sum of Squares When Each Factor is Tested First\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\nSum of Squares When Each Factor is Tested First\n\nDesign\nFactor\nSS When Tested First\n\n\n\nBalanced\nCoffee Type\n21.03\n\n\nBalanced\nTime of Day\n1.60\n\n\nUnbalanced\nCoffee Type\n9.21\n\n\nUnbalanced\nTime of Day\n14.48\n\n\n\n\n\n\n\n\n\n\n\nWhy Does Order Matter in Unbalanced Designs?\n\n\n\nIn unbalanced designs, the factors are correlated. When factors are correlated:\n\nThe effect of Coffee Type partially overlaps with the effect of Time\nTesting Coffee first “claims” the overlapping variance\nTesting Time first would claim that same overlapping variance differently\nThis is why Type I (sequential) ANOVA gives different results based on order"
  },
  {
    "objectID": "ch/prob_stat/stat.html#manual-calculation-tables-for-understanding",
    "href": "ch/prob_stat/stat.html#manual-calculation-tables-for-understanding",
    "title": "Statistics Basics",
    "section": "Manual Calculation Tables for Understanding",
    "text": "Manual Calculation Tables for Understanding\nLet’s manually calculate the different types of sum of squares to truly understand what’s happening:\n\n# Function to manually calculate all types of SS\ncalculate_all_ss_types &lt;- function(data, show_details = TRUE) {\n  # Prepare data\n  n &lt;- nrow(data)\n  grand_mean &lt;- mean(data$satisfaction)\n  \n  # Get cell means and counts\n  cell_summary &lt;- data %&gt;%\n    group_by(coffee_type, time_of_day) %&gt;%\n    summarise(\n      mean = mean(satisfaction),\n      n = n(),\n      sum = sum(satisfaction),\n      .groups = 'drop'\n    )\n  \n  # Marginal means\n  coffee_marginal &lt;- data %&gt;%\n    group_by(coffee_type) %&gt;%\n    summarise(\n      mean = mean(satisfaction),\n      n = n(),\n      .groups = 'drop'\n    )\n  \n  time_marginal &lt;- data %&gt;%\n    group_by(time_of_day) %&gt;%\n    summarise(\n      mean = mean(satisfaction),\n      n = n(),\n      .groups = 'drop'\n    )\n  \n  if(show_details) {\n    print(\"Cell Means and Sample Sizes:\")\n    print(cell_summary)\n    print(\"\")\n    print(\"Marginal Means - Coffee Type:\")\n    print(coffee_marginal)\n    print(\"\")\n    print(\"Marginal Means - Time of Day:\")\n    print(time_marginal)\n    print(\"\")\n    print(paste(\"Grand Mean:\", round(grand_mean, 3)))\n    sep_line(\"-\", 40)\n  }\n  \n  # Calculate Type I SS (Sequential)\n  # Order 1: Coffee -&gt; Time -&gt; Interaction\n  \n  # SS(Coffee | μ)\n  ss_coffee_type1 &lt;- sum(coffee_marginal$n * (coffee_marginal$mean - grand_mean)^2)\n  \n  # For SS(Time | μ, Coffee), we need residuals after fitting Coffee\n  model_coffee_only &lt;- lm(satisfaction ~ coffee_type, data = data)\n  residuals_after_coffee &lt;- residuals(model_coffee_only)\n  \n  # Create pseudo-data with residuals\n  pseudo_data_time &lt;- data.frame(\n    residuals = residuals_after_coffee,\n    time_of_day = data$time_of_day\n  )\n  \n  model_time_on_residuals &lt;- lm(residuals ~ time_of_day, data = pseudo_data_time)\n  ss_time_type1 &lt;- sum((fitted(model_time_on_residuals))^2)\n  \n  # Calculate Type II SS (Marginal, no interaction)\n  model_both_main &lt;- lm(satisfaction ~ coffee_type + time_of_day, data = data)\n  model_coffee_only &lt;- lm(satisfaction ~ coffee_type, data = data)\n  model_time_only &lt;- lm(satisfaction ~ time_of_day, data = data)\n  \n  ss_coffee_type2 &lt;- sum((fitted(model_both_main) - fitted(model_time_only))^2)\n  ss_time_type2 &lt;- sum((fitted(model_both_main) - fitted(model_coffee_only))^2)\n  \n  # Calculate Type III SS (Marginal, with interaction)\n  # This requires more complex calculations with contrast coding\n  \n  # Create results table\n  results &lt;- data.frame(\n    `Type` = c(\"Type I\", \"Type II\", \"Type III*\"),\n    `SS_Coffee` = c(ss_coffee_type1, ss_coffee_type2, NA),\n    `SS_Time` = c(ss_time_type1, ss_time_type2, NA),\n    check.names = FALSE\n  )\n  \n  return(results)\n}\n\n# Calculate for both designs\nprint(\"BALANCED DESIGN:\")\n\n[1] \"BALANCED DESIGN:\"\n\nbalanced_calc &lt;- calculate_all_ss_types(balanced_coffee, show_details = TRUE)\n\n[1] \"Cell Means and Sample Sizes:\"\n# A tibble: 4 × 5\n  coffee_type time_of_day  mean     n   sum\n  &lt;fct&gt;       &lt;fct&gt;       &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n1 Regular     Morning      8.31    20  166.\n2 Regular     Afternoon    5.74    20  115.\n3 Decaf       Morning      5.00    20  100.\n4 Decaf       Afternoon    7.01    20  140.\n[1] \"\"\n[1] \"Marginal Means - Coffee Type:\"\n# A tibble: 2 × 3\n  coffee_type  mean     n\n  &lt;fct&gt;       &lt;dbl&gt; &lt;int&gt;\n1 Regular      7.03    40\n2 Decaf        6.00    40\n[1] \"\"\n[1] \"Marginal Means - Time of Day:\"\n# A tibble: 2 × 3\n  time_of_day  mean     n\n  &lt;fct&gt;       &lt;dbl&gt; &lt;int&gt;\n1 Morning      6.66    40\n2 Afternoon    6.37    40\n[1] \"\"\n[1] \"Grand Mean: 6.516\"\n---------------------------------------- \n\nkable(balanced_calc, digits = 2, \n      caption = \"Manual SS Calculations - Balanced Design\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\nManual SS Calculations - Balanced Design\n\nType\nSS_Coffee\nSS_Time\n\n\n\nType I\n21.03\n1.6\n\n\nType II\n21.03\n1.6\n\n\nType III*\nNA\nNA\n\n\n\n\nprint(\"\")\n\n[1] \"\"\n\nprint(\"UNBALANCED DESIGN:\")\n\n[1] \"UNBALANCED DESIGN:\"\n\nunbalanced_calc &lt;- calculate_all_ss_types(unbalanced_coffee, show_details = TRUE)\n\n[1] \"Cell Means and Sample Sizes:\"\n# A tibble: 4 × 5\n  coffee_type time_of_day  mean     n   sum\n  &lt;fct&gt;       &lt;fct&gt;       &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n1 Decaf       Afternoon    7.13    20 143. \n2 Decaf       Morning      4.94    10  49.4\n3 Regular     Afternoon    5.92    25 148. \n4 Regular     Morning      8.07    30 242. \n[1] \"\"\n[1] \"Marginal Means - Coffee Type:\"\n# A tibble: 2 × 3\n  coffee_type  mean     n\n  &lt;fct&gt;       &lt;dbl&gt; &lt;int&gt;\n1 Decaf        6.40    30\n2 Regular      7.09    55\n[1] \"\"\n[1] \"Marginal Means - Time of Day:\"\n# A tibble: 2 × 3\n  time_of_day  mean     n\n  &lt;fct&gt;       &lt;dbl&gt; &lt;int&gt;\n1 Afternoon    6.46    45\n2 Morning      7.29    40\n[1] \"\"\n[1] \"Grand Mean: 6.849\"\n---------------------------------------- \n\nkable(unbalanced_calc, digits = 2,\n      caption = \"Manual SS Calculations - Unbalanced Design\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\nManual SS Calculations - Unbalanced Design\n\nType\nSS_Coffee\nSS_Time\n\n\n\nType I\n9.21\n10.17\n\n\nType II\n5.34\n10.61\n\n\nType III*\nNA\nNA"
  },
  {
    "objectID": "ch/prob_stat/stat.html#example-when-type-i-makes-sense",
    "href": "ch/prob_stat/stat.html#example-when-type-i-makes-sense",
    "title": "Statistics Basics",
    "section": "Example: When Type I Makes Sense",
    "text": "Example: When Type I Makes Sense\nScenario: Educational Achievement Study\nImagine studying factors affecting student test scores where we have a clear causal hierarchy:\n\n\nSocioeconomic Status (SES) - This is a background variable that exists before schooling\n\nSchool Quality - Students are assigned to schools based partly on where they live (related to SES)\n\nTeaching Method - Applied within schools\n\nHere, it makes sense to use Type I with SES entered first, then School Quality, then Teaching Method. We want to know:\n\nHow much variance does SES explain?\nHow much additional variance does School Quality explain after accounting for SES?\nHow much additional variance does Teaching Method explain after accounting for both?\n\nThis sequential approach respects the causal/temporal ordering of these factors."
  },
  {
    "objectID": "ch/prob_stat/stat.html#what-does-each-row-mean",
    "href": "ch/prob_stat/stat.html#what-does-each-row-mean",
    "title": "Statistics Basics",
    "section": "What Does Each Row Mean?",
    "text": "What Does Each Row Mean?\nClick below to understand each row in the ANOVA table:\nClick to see explanation\nRow 1 - coffee_type: This shows the sum of squares for coffee type when it’s the FIRST factor considered (after the intercept). It answers: “How much variance in satisfaction is explained by coffee type alone?”\nRow 2 - time_of_day: This shows the sum of squares for time of day AFTER removing the effect of coffee type. It answers: “How much additional variance is explained by time of day that wasn’t already explained by coffee type?”\nRow 3 - coffee_type:time_of_day: This is the interaction term. It answers: “Is the effect of coffee type different at different times of day?” A significant interaction means the effect of one factor depends on the level of the other.\nRow 4 - Residuals: This is the unexplained variance - the variation in satisfaction scores that can’t be explained by any of our factors. It represents individual differences and measurement error.\nColumns Explained:\n\n\nDf (Degrees of Freedom): Number of independent pieces of information. For factors: (number of levels - 1)\n\nSum Sq: Total squared deviations explained by that factor\n\nMean Sq: Sum Sq divided by Df (average squared deviation)\n\nF value: Ratio of factor’s Mean Sq to Residual Mean Sq (signal-to-noise ratio)\n\nPr(&gt;F): p-value - probability of seeing this F-value or larger if null hypothesis is true"
  },
  {
    "objectID": "ch/prob_stat/stat.html#what-does-each-row-mean-in-type-ii",
    "href": "ch/prob_stat/stat.html#what-does-each-row-mean-in-type-ii",
    "title": "Statistics Basics",
    "section": "What Does Each Row Mean in Type II?",
    "text": "What Does Each Row Mean in Type II?\nClick to see explanation\nKey Differences from Type I:\nRow 1 - coffee_type: Now shows SS for coffee type AFTER adjusting for time_of_day (but NOT interaction). It answers: “What unique variance does coffee type explain that time doesn’t?”\nRow 2 - time_of_day: Shows SS for time AFTER adjusting for coffee_type (but NOT interaction). It answers: “What unique variance does time explain that coffee type doesn’t?”\nRow 3 - coffee_type:time_of_day: Same as Type I - interaction is always tested last.\nWhy Type II is Often Preferred:\n\nTests each main effect controlling for other main effects\nOrder doesn’t matter (unlike Type I)\nAssumes no interaction when testing main effects\nMore balanced approach for most research questions"
  },
  {
    "objectID": "ch/prob_stat/stat.html#what-does-each-row-mean-in-type-iii",
    "href": "ch/prob_stat/stat.html#what-does-each-row-mean-in-type-iii",
    "title": "Statistics Basics",
    "section": "What Does Each Row Mean in Type III?",
    "text": "What Does Each Row Mean in Type III?\nClick to see explanation\nType III Special Characteristics:\nRow 1 - (Intercept): Type III includes the intercept test, which tests if the grand mean equals zero (usually not interesting).\nRow 2 - coffee_type: Tests coffee type AFTER adjusting for time AND interaction. Answers: “Is there a coffee type effect averaged across all times?”\nRow 3 - time_of_day: Tests time AFTER adjusting for coffee type AND interaction. Answers: “Is there a time effect averaged across all coffee types?”\nRow 4 - coffee_type:time_of_day: Same as other types - interaction effect.\nWhen Type III is Useful:\n\nWhen interaction is significant\nWhen you want the most conservative test\nWhen following certain field conventions (e.g., some areas of psychology)\nTests “average” effects across all levels of other factors"
  },
  {
    "objectID": "ch/prob_stat/stat.html#the-magic-of-orthogonality",
    "href": "ch/prob_stat/stat.html#the-magic-of-orthogonality",
    "title": "Statistics Basics",
    "section": "The Magic of Orthogonality",
    "text": "The Magic of Orthogonality\n\n# Demonstrate orthogonality in balanced vs unbalanced designs\n\ncheck_orthogonality &lt;- function(data, title) {\n  # Create design matrix\n  X &lt;- model.matrix(~ coffee_type * time_of_day, data = data)\n  \n  # Calculate correlation matrix (excluding intercept)\n  cor_matrix &lt;- cor(X[, -1])\n  \n  # Visualize\n  par(mar = c(5, 4, 4, 2))\n  corrplot(cor_matrix, \n           method = \"color\", \n           type = \"upper\",\n           tl.cex = 0.8,\n           tl.col = \"black\",\n           title = title,\n           mar = c(0, 0, 2, 0),\n           addCoef.col = \"black\",\n           number.cex = 0.8)\n  \n  return(cor_matrix)\n}\n\n# Check both designs\npar(mfrow = c(1, 2))\nbalanced_cors &lt;- check_orthogonality(balanced_coffee, \n                                     \"Balanced Design\\n(Orthogonal)\")\nunbalanced_cors &lt;- check_orthogonality(unbalanced_coffee, \n                                       \"Unbalanced Design\\n(Non-orthogonal)\")\n\n\n\npar(mfrow = c(1, 1))\n\n\n\n\n\n\n\nWhy Balanced Designs Make All Types Equal\n\n\n\nIn balanced designs, the design vectors are orthogonal (uncorrelated):\n\n\nZero Correlation: The correlation between coffee_type and time_of_day is 0\n\nNo Overlapping Variance: Each factor explains completely separate portions of variance\n\nOrder Doesn’t Matter: Since factors don’t overlap, the sequence of testing is irrelevant\n\nUnique Contributions: Each factor’s contribution is unique and doesn’t depend on others\n\nThis is why balanced designs are so desirable in experimental research!"
  },
  {
    "objectID": "ch/prob_stat/stat.html#detailed-comparison-table",
    "href": "ch/prob_stat/stat.html#detailed-comparison-table",
    "title": "Statistics Basics",
    "section": "Detailed Comparison Table",
    "text": "Detailed Comparison Table\n\n# Create a comprehensive comparison\ncompare_ss_types &lt;- function(data, design_name) {\n  # Type I (two orders)\n  model_i_order1 &lt;- lm(satisfaction ~ coffee_type + time_of_day, data = data)\n  model_i_order2 &lt;- lm(satisfaction ~ time_of_day + coffee_type, data = data)\n  \n  type_i_order1 &lt;- anova(model_i_order1)\n  type_i_order2 &lt;- anova(model_i_order2)\n  \n  # Type II\n  type_ii &lt;- Anova(model_i_order1, type = \"II\")\n  \n  # Type III (with proper contrasts)\n  data_copy &lt;- data\n  contrasts(data_copy$coffee_type) &lt;- contr.sum(2)\n  contrasts(data_copy$time_of_day) &lt;- contr.sum(2)\n  model_iii &lt;- lm(satisfaction ~ coffee_type + time_of_day, data = data_copy)\n  type_iii &lt;- Anova(model_iii, type = \"III\")\n  \n  # Create comparison table\n  comparison &lt;- data.frame(\n    Design = design_name,\n    Type = c(\"I (Coffee→Time)\", \"I (Time→Coffee)\", \"II\", \"III\"),\n    Coffee_SS = c(\n      type_i_order1$`Sum Sq`[1],\n      type_i_order2$`Sum Sq`[2],\n      type_ii$`Sum Sq`[1],\n      type_iii$`Sum Sq`[2]\n    ),\n    Time_SS = c(\n      type_i_order1$`Sum Sq`[2],\n      type_i_order2$`Sum Sq`[1],\n      type_ii$`Sum Sq`[2],\n      type_iii$`Sum Sq`[3]\n    )\n  )\n  \n  return(comparison)\n}\n\n# Compare both designs\nbalanced_comparison &lt;- compare_ss_types(balanced_coffee, \"Balanced\")\nunbalanced_comparison &lt;- compare_ss_types(unbalanced_coffee, \"Unbalanced\")\n\nfull_comparison &lt;- rbind(balanced_comparison, unbalanced_comparison)\n\nkable(full_comparison, digits = 2,\n      caption = \"Sum of Squares Comparison: All Types, Both Designs\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\")) %&gt;%\n  pack_rows(\"Balanced Design\", 1, 4) %&gt;%\n  pack_rows(\"Unbalanced Design\", 5, 8)\n\n\nSum of Squares Comparison: All Types, Both Designs\n\nDesign\nType\nCoffee_SS\nTime_SS\n\n\n\nBalanced Design\n\n\nBalanced\nI (Coffee→Time)\n21.03\n1.60\n\n\nBalanced\nI (Time→Coffee)\n21.03\n1.60\n\n\nBalanced\nII\n21.03\n1.60\n\n\nBalanced\nIII\n21.03\n1.60\n\n\nUnbalanced Design\n\n\nUnbalanced\nI (Coffee→Time)\n9.21\n10.61\n\n\nUnbalanced\nI (Time→Coffee)\n5.34\n14.48\n\n\nUnbalanced\nII\n5.34\n10.61\n\n\nUnbalanced\nIII\n5.34\n10.61"
  },
  {
    "objectID": "ch/prob_stat/stat.html#testing-for-homogeneity-of-variance",
    "href": "ch/prob_stat/stat.html#testing-for-homogeneity-of-variance",
    "title": "Statistics Basics",
    "section": "Testing for Homogeneity of Variance",
    "text": "Testing for Homogeneity of Variance\n\nprint(\"Testing for Equal Variances:\")\n\n[1] \"Testing for Equal Variances:\"\n\nsep_line(\"=\", 50)\n\n================================================== \n\nprint(\"Levene's Test (robust to non-normality):\")\n\n[1] \"Levene's Test (robust to non-normality):\"\n\nlevene_test &lt;- leveneTest(value ~ group, data = hetero_data)\nprint(levene_test)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value  Pr(&gt;F)    \ngroup  2   19.13 1.3e-07 ***\n      87                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nprint(\"\")\n\n[1] \"\"\n\nprint(\"Bartlett's Test (sensitive to non-normality):\")\n\n[1] \"Bartlett's Test (sensitive to non-normality):\"\n\nbartlett_test &lt;- bartlett.test(value ~ group, data = hetero_data)\nprint(bartlett_test)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  value by group\nBartlett's K-squared = 73.797, df = 2, p-value &lt; 2.2e-16\n\n\n\n\n\n\n\n\nInterpreting Variance Tests\n\n\n\nLevene’s Test: p &lt; 0.05 indicates unequal variances (assumption violated)Bartlett’s Test: More powerful but sensitive to non-normality\nWhat to do with unequal variances:\n\nUse Welch’s ANOVA instead of standard ANOVA\nUse Games-Howell post-hoc test instead of Tukey\nConsider transforming data (log, square root)\nUse robust methods or non-parametric alternatives"
  },
  {
    "objectID": "ch/prob_stat/stat.html#consequences-of-ignoring-unequal-variances",
    "href": "ch/prob_stat/stat.html#consequences-of-ignoring-unequal-variances",
    "title": "Statistics Basics",
    "section": "Consequences of Ignoring Unequal Variances",
    "text": "Consequences of Ignoring Unequal Variances\n\n# Standard ANOVA (assumes equal variances)\nprint(\"Standard ANOVA (assumes equal variances):\")\n\n[1] \"Standard ANOVA (assumes equal variances):\"\n\nstandard_anova &lt;- aov(value ~ group, data = hetero_data)\nsummary(standard_anova)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)\ngroup        2    332   165.9   2.275  0.109\nResiduals   87   6343    72.9               \n\nprint(\"\")\n\n[1] \"\"\n\nprint(\"Welch's ANOVA (robust to unequal variances):\")\n\n[1] \"Welch's ANOVA (robust to unequal variances):\"\n\nwelch_test &lt;- oneway.test(value ~ group, data = hetero_data, var.equal = FALSE)\nprint(welch_test)\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  value and group\nF = 5.2616, num df = 2.000, denom df = 42.497, p-value = 0.009086\n\n# Compare p-values\ncomparison_pvalues &lt;- data.frame(\n  Test = c(\"Standard ANOVA\", \"Welch's ANOVA\"),\n  `Assumes Equal Variances` = c(\"Yes\", \"No\"),\n  `p-value` = c(summary(standard_anova)[[1]]$`Pr(&gt;F)`[1], welch_test$p.value),\n  Decision = c(\n    ifelse(summary(standard_anova)[[1]]$`Pr(&gt;F)`[1] &lt; 0.05, \"Reject H0\", \"Fail to reject H0\"),\n    ifelse(welch_test$p.value &lt; 0.05, \"Reject H0\", \"Fail to reject H0\")\n  ),\n  check.names = FALSE\n)\n\nkable(comparison_pvalues, digits = 4,\n      caption = \"Comparison: Standard vs Welch's ANOVA with Unequal Variances\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\nComparison: Standard vs Welch's ANOVA with Unequal Variances\n\nTest\nAssumes Equal Variances\np-value\nDecision\n\n\n\nStandard ANOVA\nYes\n0.1088\nFail to reject H0\n\n\nWelch's ANOVA\nNo\n0.0091\nReject H0"
  },
  {
    "objectID": "ch/prob_stat/stat.html#step-by-step-analysis-function",
    "href": "ch/prob_stat/stat.html#step-by-step-analysis-function",
    "title": "Statistics Basics",
    "section": "Step-by-Step Analysis Function",
    "text": "Step-by-Step Analysis Function\n\nanalyze_data_complete &lt;- function(data, dv, factors) {\n  formula_str &lt;- paste(dv, \"~\", paste(factors, collapse = \" * \"))\n  formula_obj &lt;- as.formula(formula_str)\n  \n  sep_line(\"=\", 60)\n  print(\"COMPLETE ANOVA ANALYSIS PIPELINE\")\n  sep_line(\"=\", 60)\n  \n  # 1. Check balance\n  print(\"\")\n  print(\"STEP 1: CHECKING BALANCE\")\n  sep_line(\"-\", 40)\n  design_table &lt;- table(data[[factors[1]]], data[[factors[2]]])\n  print(design_table)\n  is_balanced &lt;- all(design_table == design_table[1])\n  print(paste(\"Design is\", ifelse(is_balanced, \"BALANCED ✓\", \"UNBALANCED ⚠️\")))\n  \n  # 2. Fit model\n  model &lt;- aov(formula_obj, data = data)\n  anova_table &lt;- anova(model)\n  \n  # 3. Check assumptions\n  print(\"\")\n  print(\"STEP 2: CHECKING ASSUMPTIONS\")\n  sep_line(\"-\", 40)\n  # Normality of residuals\n  shapiro_p &lt;- shapiro.test(residuals(model))$p.value\n  print(paste(\"Shapiro-Wilk test p =\", signif(shapiro_p, 3)))\n  # Homogeneity of variances\n  levene_p &lt;- car::leveneTest(formula_obj, data = data)$`Pr(&gt;F)`[1]\n  print(paste(\"Levene’s test p =\", signif(levene_p, 3)))\n  \n  # 4. Effect sizes (eta squared)\n  print(\"\")\n  print(\"STEP 3: EFFECT SIZE (η²)\")\n  sep_line(\"-\", 40)\n  ss_total &lt;- sum(anova_table[[\"Sum Sq\"]])\n  ss_effects &lt;- anova_table[[\"Sum Sq\"]][1:(length(factors) + 1)]\n  eta_squared &lt;- ss_effects / ss_total\n  \n  effect_names &lt;- rownames(anova_table)[1:(length(factors) + 1)]\n  for (i in seq_along(eta_squared)) {\n    result_text &lt;- sprintf(\"%s: η² = %.3f \", effect_names[i], eta_squared[i])\n    if (eta_squared[i] &lt; 0.01) {\n      result_text &lt;- paste0(result_text, \"(negligible)\")\n    } else if (eta_squared[i] &lt; 0.06) {\n      result_text &lt;- paste0(result_text, \"(small)\")\n    } else if (eta_squared[i] &lt; 0.14) {\n      result_text &lt;- paste0(result_text, \"(medium)\")\n    } else {\n      result_text &lt;- paste0(result_text, \"(large)\")\n    }\n    print(result_text)\n  }\n  \n  # 5. Post-hoc tests\n  print(\"\")\n  print(\"STEP 5: POST-HOC COMPARISONS\")\n  sep_line(\"-\", 40)\n  if (levene_p &lt; 0.05) {\n    print(\"Using Games-Howell (unequal variances)\")\n    # Implement Games-Howell here if needed\n  } else {\n    print(\"Using Tukey HSD (equal variances)\")\n    if (is_balanced) {\n      print(TukeyHSD(model))\n    }\n  }\n  \n  return(invisible(model))\n}\n# Run the pipeline on unbalanced_coffee\nfinal_model &lt;- analyze_data_complete(\n  unbalanced_coffee,\n  \"satisfaction\",\n  c(\"coffee_type\", \"time_of_day\")\n)\n\n============================================================ \n[1] \"COMPLETE ANOVA ANALYSIS PIPELINE\"\n============================================================ \n[1] \"\"\n[1] \"STEP 1: CHECKING BALANCE\"\n---------------------------------------- \n         \n          Afternoon Morning\n  Decaf          20      10\n  Regular        25      30\n[1] \"Design is UNBALANCED ⚠️\"\n[1] \"\"\n[1] \"STEP 2: CHECKING ASSUMPTIONS\"\n---------------------------------------- \n[1] \"Shapiro-Wilk test p = 0.15\"\n[1] \"Levene’s test p = 0.581\"\n[1] \"\"\n[1] \"STEP 3: EFFECT SIZE (η²)\"\n---------------------------------------- \n[1] \"coffee_type: η² = 0.046 (small)\"\n[1] \"time_of_day: η² = 0.053 (small)\"\n[1] \"coffee_type:time_of_day: η² = 0.425 (large)\"\n[1] \"\"\n[1] \"STEP 5: POST-HOC COMPARISONS\"\n---------------------------------------- \n[1] \"Using Tukey HSD (equal variances)\""
  },
  {
    "objectID": "ch/prob_stat/stat.html#a-simple-story",
    "href": "ch/prob_stat/stat.html#a-simple-story",
    "title": "Statistics Basics",
    "section": "",
    "text": "Imagine you own a coffee shop and want to understand what affects customer satisfaction scores (1-10 scale). You consider two factors:\n\n\nCoffee Type: Regular vs Decaf\n\nTime of Day: Morning vs Afternoon\n\nLet’s create this scenario with data:\n\nset.seed(42)\n\n# Create a BALANCED design first (equal sample sizes)\nn_per_cell &lt;- 20  # 20 customers in each combination\n\nbalanced_coffee &lt;- expand.grid(\n  coffee_type = c(\"Regular\", \"Decaf\"),\n  time_of_day = c(\"Morning\", \"Afternoon\"),\n  replicate = 1:n_per_cell\n) %&gt;%\n  mutate(\n    # Create satisfaction scores with main effects and interaction\n    satisfaction = case_when(\n      coffee_type == \"Regular\" & time_of_day == \"Morning\" ~ rnorm(n(), 8, 1),    # High satisfaction\n      coffee_type == \"Regular\" & time_of_day == \"Afternoon\" ~ rnorm(n(), 6, 1),  # Medium\n      coffee_type == \"Decaf\" & time_of_day == \"Morning\" ~ rnorm(n(), 5, 1),      # Low-medium\n      coffee_type == \"Decaf\" & time_of_day == \"Afternoon\" ~ rnorm(n(), 7, 1)     # Medium-high\n    ),\n    coffee_type = factor(coffee_type),\n    time_of_day = factor(time_of_day)\n  ) %&gt;%\n  select(-replicate)\n\n# Show the structure\nprint(\"Balanced Design - Sample Sizes:\")\n\n[1] \"Balanced Design - Sample Sizes:\"\n\ntable(balanced_coffee$coffee_type, balanced_coffee$time_of_day)\n\n         \n          Morning Afternoon\n  Regular      20        20\n  Decaf        20        20\n\n# Calculate means for each cell\ncell_means_balanced &lt;- balanced_coffee %&gt;%\n  group_by(coffee_type, time_of_day) %&gt;%\n  summarise(\n    mean_satisfaction = mean(satisfaction),\n    n = n(),\n    .groups = 'drop'\n  )\n\nkable(cell_means_balanced, digits = 2, \n      caption = \"Mean Satisfaction Scores - Balanced Design\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\nMean Satisfaction Scores - Balanced Design\n\ncoffee_type\ntime_of_day\nmean_satisfaction\nn\n\n\n\nRegular\nMorning\n8.31\n20\n\n\nRegular\nAfternoon\n5.74\n20\n\n\nDecaf\nMorning\n5.00\n20\n\n\nDecaf\nAfternoon\n7.01\n20"
  },
  {
    "objectID": "ch/prob_stat/stat.html#when-to-use-which-type",
    "href": "ch/prob_stat/stat.html#when-to-use-which-type",
    "title": "Statistics Basics",
    "section": "When to Use Which Type?",
    "text": "When to Use Which Type?\n\n# Create a decision guide\ndecision_guide &lt;- data.frame(\n  Scenario = c(\n    \"Balanced design\",\n    \"Unbalanced + No interaction\",\n    \"Unbalanced + Significant interaction\",\n    \"Natural hierarchy of factors\",\n    \"Exploratory analysis\",\n    \"Following field conventions\"\n  ),\n  `Recommended Type` = c(\n    \"Any (all equal)\",\n    \"Type II\",\n    \"Type III\",\n    \"Type I\",\n    \"Type III\",\n    \"Check literature\"\n  ),\n  Reasoning = c(\n    \"All types give identical results with balanced data\",\n    \"Type II tests main effects properly without interaction assumption\",\n    \"Type III tests main effects in presence of interaction\",\n    \"Type I respects the causal/temporal order\",\n    \"Type III is most conservative\",\n    \"Some fields have established preferences\"\n  ),\n  check.names = FALSE\n)\n\nkable(decision_guide, \n      caption = \"Decision Guide for Choosing ANOVA Type\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\nDecision Guide for Choosing ANOVA Type\n\nScenario\nRecommended Type\nReasoning\n\n\n\nBalanced design\nAny (all equal)\nAll types give identical results with balanced data\n\n\nUnbalanced + No interaction\nType II\nType II tests main effects properly without interaction assumption\n\n\nUnbalanced + Significant interaction\nType III\nType III tests main effects in presence of interaction\n\n\nNatural hierarchy of factors\nType I\nType I respects the causal/temporal order\n\n\nExploratory analysis\nType III\nType III is most conservative\n\n\nFollowing field conventions\nCheck literature\nSome fields have established preferences"
  },
  {
    "objectID": "ch/prob_stat/stat.html#comparison-function-for-all-three-types",
    "href": "ch/prob_stat/stat.html#comparison-function-for-all-three-types",
    "title": "Statistics Basics",
    "section": "Comparison Function for All Three Types",
    "text": "Comparison Function for All Three Types\n\n# Quick function to compare all three types\ncompare_anova_types &lt;- function(formula, data, verbose = TRUE) {\n  require(car)\n  \n  # Ensure factors\n  factors &lt;- all.vars(formula)[-1]\n  for(f in factors) {\n    if(f %in% names(data)) {\n      data[[f]] &lt;- factor(data[[f]])\n    }\n  }\n  \n  # Check balance\n  if(length(factors) == 2) {\n    design_table &lt;- table(data[[factors[1]]], data[[factors[2]]])\n    is_balanced &lt;- length(unique(as.vector(design_table))) == 1\n  } else {\n    is_balanced &lt;- FALSE\n  }\n  \n  # Type I\n  model1 &lt;- lm(formula, data = data)\n  \n  # Type II\n  model2 &lt;- model1\n  \n  # Type III (need sum contrasts)\n  data_type3 &lt;- data\n  for(f in factors) {\n    if(f %in% names(data_type3)) {\n      contrasts(data_type3[[f]]) &lt;- contr.sum(nlevels(data_type3[[f]]))\n    }\n  }\n  model3 &lt;- lm(formula, data = data_type3)\n  \n  # Store results\n  type1_anova &lt;- anova(model1)\n  type2_anova &lt;- Anova(model2, type = \"II\")\n  type3_anova &lt;- Anova(model3, type = \"III\")\n  \n  if(verbose) {\n    print(\"========== TYPE I (Sequential) ==========\")\n    print(type1_anova)\n    \n    print(\"\")\n    print(\"========== TYPE II (No Interaction) ==========\")\n    print(type2_anova)\n    \n    print(\"\")\n    print(\"========== TYPE III (Marginal) ==========\")\n    print(type3_anova)\n    \n    print(\"\")\n    print(\"========== RECOMMENDATION ==========\")\n    \n    if(is_balanced) {\n      print(\"✓ Balanced design detected - all types equivalent\")\n      print(\"→ Use Type I for computational efficiency\")\n    } else {\n      print(\"⚠️ Unbalanced design detected\")\n      \n      # Check for interaction\n      if(length(factors) == 2) {\n        # Get interaction p-value\n        interaction_term &lt;- paste(factors, collapse = \":\")\n        if(interaction_term %in% rownames(type2_anova)) {\n          interaction_p &lt;- type2_anova[interaction_term, \"Pr(&gt;F)\"]\n          if(!is.na(interaction_p) && interaction_p &lt; 0.05) {\n            print(paste(\"→ Significant interaction (p =\", round(interaction_p, 3), \")\"))\n            print(\"→ RECOMMEND: Type III for main effects interpretation\")\n          } else {\n            print(\"→ No significant interaction\")\n            print(\"→ RECOMMEND: Type II for main effects testing\")\n          }\n        }\n      }\n    }\n  }\n  \n  # Return results as a list\n  return(invisible(list(\n    type1 = type1_anova,\n    type2 = type2_anova, \n    type3 = type3_anova,\n    balanced = is_balanced\n  )))\n}\n\n# Test the function\nprint(\"Testing the comparison function with our coffee data:\")\n\n[1] \"Testing the comparison function with our coffee data:\"\n\nresults &lt;- compare_anova_types(satisfaction ~ coffee_type * time_of_day, \n                              unbalanced_coffee, verbose = TRUE)\n\n[1] \"========== TYPE I (Sequential) ==========\"\nAnalysis of Variance Table\n\nResponse: satisfaction\n                        Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ncoffee_type              1  9.214   9.214  7.9036  0.006186 ** \ntime_of_day              1 10.607  10.607  9.0985  0.003417 ** \ncoffee_type:time_of_day  1 84.400  84.400 72.3995 7.439e-13 ***\nResiduals               81 94.426   1.166                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n[1] \"\"\n[1] \"========== TYPE II (No Interaction) ==========\"\nAnova Table (Type II tests)\n\nResponse: satisfaction\n                        Sum Sq Df F value    Pr(&gt;F)    \ncoffee_type              5.339  1  4.5802  0.035352 *  \ntime_of_day             10.607  1  9.0985  0.003417 ** \ncoffee_type:time_of_day 84.400  1 72.3995 7.439e-13 ***\nResiduals               94.426 81                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n[1] \"\"\n[1] \"========== TYPE III (Marginal) ==========\"\nAnova Table (Type III tests)\n\nResponse: satisfaction\n                         Sum Sq Df   F value    Pr(&gt;F)    \n(Intercept)             3041.77  1 2609.2756 &lt; 2.2e-16 ***\ncoffee_type               16.40  1   14.0657 0.0003302 ***\ntime_of_day                0.01  1    0.0077 0.9302036    \ncoffee_type:time_of_day   84.40  1   72.3995 7.439e-13 ***\nResiduals                 94.43 81                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n[1] \"\"\n[1] \"========== RECOMMENDATION ==========\"\n[1] \"⚠️ Unbalanced design detected\"\n[1] \"→ Significant interaction (p = 0 )\"\n[1] \"→ RECOMMEND: Type III for main effects interpretation\""
  },
  {
    "objectID": "ch/prob_stat/stat.html#appendix-r-package-requirements",
    "href": "ch/prob_stat/stat.html#appendix-r-package-requirements",
    "title": "Statistics Basics",
    "section": "Appendix: R Package Requirements",
    "text": "Appendix: R Package Requirements\n\n# Required packages for this tutorial\nrequired_packages &lt;- c(\n  \"tidyverse\",     # Data manipulation and visualization\n  \"car\",           # For Type II and III ANOVA\n  \"emmeans\",       # Estimated marginal means\n  \"knitr\",         # For tables\n  \"kableExtra\",    # Enhanced tables\n  \"patchwork\",     # Combining plots\n  \"corrplot\",      # Correlation plots\n  \"rstatix\"        # Additional statistical tests\n)\n\n# Install if needed\ninstall.packages(required_packages)"
  }
]